{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with NLTK and Scikit-Learn\n",
    "\n",
    "+ Last class, we saw how we'd go about using scikit-learn.\n",
    "+ This class, we'll apply this to something concrete.\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "If you want to follow along, make sure that you have NLTK and Scikit-Learn installed, and that you have downloaded the NLTK corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "\n",
    "pip install nltk scikit-learn\n",
    "python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Pipeline \n",
    "![image](https://user-images.githubusercontent.com/1433964/46770917-53b52380-ccbf-11e8-9195-216d96cd887f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do these mean?\n",
    "+ **The Corpus Reader:** Read files one at a and act as the source of the data. \n",
    "+ **The Tokenizer:** Split raw text into sentences, words and punctuation, then tag their part of speech and lemmatizes them using the WordNet lexicon. \n",
    "+ **The vectorizer:** Encodes the tokens in the document as a feature vector, for example as a TF-IDF vector. \n",
    "+ **The classifier:** Fit to the documents and their labels and to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading a corpus\n",
    "\n",
    "+ A simple corpus that comes pre-installed with ntlk.\n",
    "+ Has 200 movie review text and corresponding lables:\n",
    "    - Positive\n",
    "    - Negative\n",
    "+ Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as reviews\n",
    "import numpy as np\n",
    "\n",
    "X = [reviews.raw(fileid) for fileid in reviews.fileids()]\n",
    "y = np.array([reviews.categories(fileid)[0]!='neg' for fileid in reviews.fileids()]).astype(np.int)\n",
    "\n",
    "print(X[1], y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/1433964/46770902-413aea00-ccbf-11e8-97b1-3d382194018c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing: Tokenize, Lemmatize, etc.\n",
    "\n",
    "+ Now that we have read the corpus, we need to preprocess the string to tokenize it, and lemmatize it.\n",
    "\n",
    "+ Basically, \n",
    "    - We want to convert \"A sentence of words\" to `[\"A\", \"sentence\", \"of\", \"words\"]`\n",
    "    - Next, we want to combine redundant features into a single token.\n",
    "        - E.g, `bunny`, `bunnies`, `bunny's`, `Bunny`, and `bunny!` all become `bunny`\n",
    "    - We also want to remove stopwords like \"A\", \"of\", etc.\n",
    "    - We don't much care for punctuations, uppercase letters, numbers, etc.\n",
    "\n",
    "\n",
    "+ Let's see how we'd do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# ----- TOKENIZE A DOCUMENT --------\n",
    "# ----------------------------------\n",
    "\n",
    "def tokenize(document):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    \"Break the document into sentences\"\n",
    "    for sent in sent_tokenize(document):\n",
    "\n",
    "        \"Break the sentence into part of speech tagged tokens\"\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            \n",
    "            \"Apply preprocessing to the token\"\n",
    "            token = token.lower()  # Convert to lower case\n",
    "            token = token.strip()  # Strip whitespace and other punctuations\n",
    "            token = token.strip('_')  # remove _ if any\n",
    "            token = token.strip('*')  # remove * if any\n",
    "\n",
    "            \"If stopword, ignore.\"\n",
    "            if token in stopwords.words('english'):\n",
    "                continue\n",
    "\n",
    "            \"If punctuation, ignore.\"\n",
    "            if all(char in string.punctuation for char in token):\n",
    "                continue\n",
    "\n",
    "            \"If number, ignore.\"\n",
    "            if token.isdigit():\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and yield\n",
    "            # Note: Lemmatization is the process of looking up a single word form \n",
    "            # from the variety of morphologic affixes that can be applied to \n",
    "            # indicate tense, plurality, gender, etc.\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            yield lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Original String:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([tok for tok in tokenize(X[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/1433964/46770872-1cdf0d80-ccbf-11e8-89ea-ca7529ecb19b.png)\n",
    "\n",
    "## 3. Scikit-Learn Vectorization\n",
    "\n",
    "+ Just a list of words is of not much use, we need to vectorize these words to be able to do classification.\n",
    "+ TF-IDF is a popular vectorizer\n",
    "\n",
    "### What is TF-IDF?\n",
    "+ Convert a word into a number based on the following:\n",
    "\n",
    "###### Term Frequency\n",
    "```\n",
    "TermFrequency(word)\n",
    "  No. of times the word appears in one document\n",
    "```\n",
    "###### Document Frequency\n",
    "```\n",
    "DocumentFrequecy(word)\n",
    "\n",
    "    No. times the word appears in all the documents \n",
    "    -----------------------------------------------\n",
    "                  No. of Documents              \n",
    "```\n",
    "\n",
    "+ Intuition,\n",
    "  - Words that appear more often in a document are useful.\n",
    "  - Words that appear least often in all documents are useful.\n",
    "\n",
    "\n",
    "##### Example\n",
    "\n",
    "+ Take the following two sentences\n",
    "    1. *\"Something about a cat.\"*\n",
    "    2. *\"Something about a dog.\"*\n",
    "+ `TF(\"cat\")` = 1\n",
    "+ `DF(\"cat\")` = 1/2 = 0.5\n",
    "+ `TFIDF(\"cat\") = TF(\"cat\") / DF(\"cat\")`\n",
    "    `TFIDF(\"cat\") = 1 / 0.5 = 2`\n",
    "\n",
    "+ `TF(\"something\")` = 1\n",
    "+ `DF(\"something\")` = 2/2 = 1\n",
    "+ `TFIDF(\"cat\") = TF(\"cat\") / DF(\"cat\")`\n",
    "    `TFIDF(\"cat\") = 1 / 1 = 1`\n",
    "\n",
    "\n",
    "**`TFIDF(\"cat\")=2` and `TFIDF(\"something\")=1`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Tokenize all the documents in the corpus\n",
    "# ----------------------------------------\n",
    "tokenize_corpus = [tokenize(x) for x in X]\n",
    "\n",
    "# Vectorize with TFIDF\n",
    "# ====================\n",
    "\n",
    "# 1. Instantiate TfidfVectorizer\n",
    "# ------------------------------\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, preprocessor=None, lowercase=False)\n",
    "                                    #---------#\n",
    "                # We to pass our tokenizer that we wrote above.#\n",
    "    \n",
    "\n",
    "# 2. Generate TFIDF Matrix\n",
    "# ------------------------\n",
    "# + **Remember: Use fit_transform NOT fit.**\n",
    "# + vectorizer.fit(X) will only compute vocubulary. \n",
    "# + vectorizer.fit_transform(X) will compute vocubulary, compute TFIDF, \n",
    "# and return a transformed matrix. \n",
    "\n",
    "# NOTE: This will take a while......\n",
    "X_tfidf = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/1433964/46770839-00db6c00-ccbf-11e8-93f1-b6219556d22a.png)\n",
    "\n",
    "## 4. Scikit Learn Classification \n",
    "\n",
    "We've seen this before. So let's build a classifier.\n",
    "\n",
    "### First, let's do a `10-Fold` cross validation with with SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.base import clone  \n",
    "\n",
    "# Stratification attempts to maintain the realtive ratios\n",
    "# of positive and negative classes in each of folds.\n",
    "# This is very helpful for skewed datasets (read: SE Data)\n",
    "skfolds = StratifiedKFold(n_splits=10, random_state=0)\n",
    "\n",
    "# Create and instance of your classifier\n",
    "clf = LinearSVC(C=1, loss='hinge')  \n",
    "\n",
    "# Store F1, precision, and recall for svm\n",
    "svm_f1   = []\n",
    "svm_prec = []\n",
    "svm_recl = []\n",
    "\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_tfidf, y):\n",
    "    # Deep copy here, when you do cross-validation, it's always a good idead\n",
    "    # not to mess with the original classifier. \n",
    "\n",
    "    # We need a \"clean\" classifier for every fold. Otherwise your mixing the\n",
    "    # training and testing data form different folds. That's a no-no.\n",
    "    cloned_clf = clone(clf)\n",
    "\n",
    "    # Training data\n",
    "    # -------------\n",
    "    X_train_folds = X_tfidf[train_index]\n",
    "    y_train_folds = y[train_index]\n",
    "\n",
    "    # Testing data\n",
    "    # ------------\n",
    "    X_test_folds = X_tfidf[test_index]\n",
    "    y_test_folds = y[test_index]\n",
    "\n",
    "    # Fit a classifier on the training data\n",
    "    # -------------------------------------\n",
    "    cloned_clf.fit(X_train_folds, y_train_folds)\n",
    "\n",
    "    # Make predictions on a test set\n",
    "    # ------------------------------\n",
    "    y_hat = cloned_clf.predict(X_test_folds)\n",
    "\n",
    "    # Compute some metrics here. Like Precision, Recall, False Alarm, or what have you.\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    cmat = confusion_matrix(y_test_folds, y_hat)\n",
    "\n",
    "    # Precision\n",
    "    # ---------\n",
    "    prec = cmat[1,1]/(cmat[1,1]+cmat[0,1])\n",
    "\n",
    "    # Recall\n",
    "    # ------\n",
    "    recall = cmat[1,1]/(cmat[1,1]+cmat[1,0])\n",
    "    \n",
    "    # F1 Score\n",
    "    # --------\n",
    "    f1 = 2*(prec*recall)/(prec+recall)\n",
    "    \n",
    "    # Record results\n",
    "    # --------------\n",
    "    \n",
    "    svm_f1.append(f1)\n",
    "    svm_prec.append(prec)\n",
    "    svm_recl.append(recall)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's do a 10-Fold cross validation with with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.base import clone  \n",
    "\n",
    "# Stratification attempts to maintain the realtive ratios\n",
    "# of positive and negative classes in each of folds.\n",
    "# This is very helpful for skewed datasets (read: SE Data)\n",
    "skfolds = StratifiedKFold(n_splits=10, random_state=0)\n",
    "\n",
    "# Create and instance of your classifier\n",
    "clf = SGDClassifier(random_state=0, max_iter=10, tol=1e3)  \n",
    "\n",
    "# Store F1, precision, and recall for svm\n",
    "sgd_f1   = []\n",
    "sgd_prec = []\n",
    "sgd_recl = []\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_tfidf, y):\n",
    "    # Deep copy here, when you do cross-validation, it's always a good idead\n",
    "    # not to mess with the original classifier. \n",
    "\n",
    "    # We need a \"clean\" classifier for every fold. Otherwise your mixing the\n",
    "    # training and testing data form different folds. That's a no-no.\n",
    "    cloned_clf = clone(clf)\n",
    "\n",
    "    # Training data\n",
    "    # -------------\n",
    "    X_train_folds = X_tfidf[train_index]\n",
    "    y_train_folds = y[train_index]\n",
    "\n",
    "    # Testing data\n",
    "    # ------------\n",
    "    X_test_folds = X_tfidf[test_index]\n",
    "    y_test_folds = y[test_index]\n",
    "\n",
    "    # Fit a classifier on the training data\n",
    "    # -------------------------------------\n",
    "    cloned_clf.fit(X_train_folds, y_train_folds)\n",
    "\n",
    "    # Make predictions on a test set\n",
    "    # ------------------------------\n",
    "    y_hat = cloned_clf.predict(X_test_folds)\n",
    "\n",
    "    # Compute some metrics here. Like Precision, Recall, False Alarm, or what have you.\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    cmat = confusion_matrix(y_test_folds, y_hat)\n",
    "\n",
    "    # Precision\n",
    "    # ---------\n",
    "    prec = cmat[1,1]/(cmat[1,1]+cmat[0,1])\n",
    "\n",
    "    # Recall\n",
    "    # ------\n",
    "    recall = cmat[1,1]/(cmat[1,1]+cmat[1,0])\n",
    "    \n",
    "    # F1 score\n",
    "    # --------\n",
    "    f1 = 2*(prec*recall)/(prec+recall)\n",
    "    \n",
    "    # Record results\n",
    "    # --------------\n",
    "    \n",
    "    sgd_f1.append(f1)\n",
    "    sgd_prec.append(prec)\n",
    "    sgd_recl.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup Matplotlib\n",
    "# ----------------\n",
    "\n",
    "# Font sizes\n",
    "# ----------\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 36\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([svm_prec, sgd_prec], labels=['Linear SVM', 'Stochastic GD'])\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([svm_recl, sgd_recl], labels=['Linear SVM', 'Stochastic GD'])\n",
    "plt.ylabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([svm_f1, sgd_f1], labels=['Linear SVM', 'Stochastic GD'])\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/1433964/46771960-c9bb8980-ccc3-11e8-9eef-95a4c621f2a1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
