<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Tables - Found.Soft.Sci</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="../../css/extra.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">Found.Soft.Sci</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../..">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Notes <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#">Admin</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../syllabus/">Syllabus &#10004;</a>
</li>
            
<li >
    <a href="http://tiny.cc/fss18give">Submit site &#10004;</a>
</li>
            
<li >
    <a href="http://found18.slack.com">Chat &#10004;</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Starting</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../inspiration/">Inspiration &#10004;</a>
</li>
            
<li >
    <a href="../../history/">SE+AI,  then and now &#10004;</a>
</li>
            
<li >
    <a href="../baselines/">Baselines for Adequate AI &#10004;</a>
</li>
            
<li >
    <a href="../simple/">Simplicity ;</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Lectures</a>
    <ul class="dropdown-menu">
            
  <li class="dropdown-submenu">
    <a href="#">Stats</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../stats">Stats lecture   &#10004</a>
</li>
            
<li >
    <a href="http://menzies.us/lean/knn.html">Stats example   &#10004</a>
</li>
    </ul>
  </li>
            
<li >
    <a href="../domination">Domination &#10004</a>
</li>
            
<li >
    <a href="./">Tables and Ranges &#10004</a>
</li>
            
<li >
    <a href="../dt101/">Decision trees 101 &#10004</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">H/W <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../proj/ProjectIdeas">Project Ideas</a>
</li>
                                    
<li >
    <a href="../../proj/w5/">Five &#10004;</a>
</li>
                                    
<li >
    <a href="../../proj/w4/">Four &#10004;</a>
</li>
                                    
<li >
    <a href="../../proj/w3/">Three &#10004;</a>
</li>
                                    
<li >
    <a href="../../proj/w1/">One &#10004;</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Review</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../review/three">Week 3</a>
</li>
            
<li >
    <a href="../../review/two">Week 2 &#10004;</a>
</li>
            
<li >
    <a href="../../review/one">Week 1 &#10004;</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">A-Z <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#">A</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../axe/">Axe</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">D</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../deeplearning/">Deep learning &#10004;</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">E</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../effectsize.md">Effect size</a>
</li>
            
<li >
    <a href="../explain/">Explanation</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">B</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../bootstrap.md">Bootstrap</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">S</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../significance.md">Significance test</a>
</li>
            
<li >
    <a href="../sk.md">Scott-Knot</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">T</a>
    <ul class="dropdown-menu">
            
<li class="active">
    <a href="./">Tables</a>
</li>
            
<li >
    <a href="../things.md">Things</a>
</li>
            
  <li class="dropdown-submenu">
    <a href="#">Tools</a>
    <ul class="dropdown-menu">
            
  <li class="dropdown-submenu">
    <a href="#">Data mining</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../aa">Matlab</a>
</li>
            
<li >
    <a href="../../bb">R</a>
</li>
            
<li >
    <a href="../../cc">Weka</a>
</li>
            
<li >
    <a href="../../dd">scikit-learn</a>
</li>
    </ul>
  </li>
            
  <li class="dropdown-submenu">
    <a href="#">Optimizers</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../dd">JMetal</a>
</li>
            
<li >
    <a href="../../dd">Deep</a>
</li>
            
<li >
    <a href="../../ee">EvoSuite</a>
</li>
    </ul>
  </li>
    </ul>
  </li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li >
                                <a href="../../license/">(c) 2018</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../explain/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../../license/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#different-learners-for-different-data">Different Learners for Different Data</a></li>
            <li><a href="#y-fx">Y = F(X)</a></li>
            <li><a href="#feature-section">Feature Section</a></li>
            <li><a href="#instance-selection">Instance Selection</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="different-learners-for-different-data">Different Learners for Different Data</h1>
<p>Let us start at the very beginning (a very good place to start). When you read you begin with A-B-C. When you mine, you begin with data.</p>
<p>Different kinds of data miners work best of different kinds of data. Such data may be viewed as tables of examples:</p>
<ul>
<li>Tables have one column per feature and one row per example.</li>
<li>The columns may be numeric (have numbers) or symbolic (contain discrete
  values).</li>
<li>Also, some columns are goals (things we want to predict using the other
columns).</li>
<li>Finally, columns may contain missing values.</li>
</ul>
<p>For example, in text mining, where there is one column per word and one row per document, the columns contain many missing values (since not all words appear in all documents) and there may be hundreds of thousands of columns.</p>
<ul>
<li>While text mining applications can have many columns, Big Data applications can have any number of columns and millions to billions of rows. </li>
<li>For such very large datasets, a complete analysis may be impossible. Hence, these might be sampled probabilistically (e.g., using the naive Bayesian algorithm discussed below).</li>
</ul>
<p>On the other hand, </p>
<ul>
<li>when there are very few rows, data mining may fail since there are too few examples to support summarization. </li>
<li>For such sparse tables, k- nearest neighbors (kNN) may be best. kNN makes conclusions about new examples by looking at their neighborhood in the space of old examples. Hence, kNN only needs a few (or even only one) similar examples to make conclusions.</li>
</ul>
<p>If a table has no goal columns, then this is an unsupervised learning problem that might be addressed by (say) finding clusters of similar rows using, say, K- means or expectation maximization. </p>
<p>An alternate approach, taken by the Apriori
association rule learner, is to assume that every column is a goal and to look for what combinations of any values predict for any combination of any other.</p>
<p>If a table has one goal, then this is a supervised learning problem where the task is to find combinations of values from the other columns that predict for the goal values. Note that for datasets with one discrete goal feature, it is common to call that goal the class of the dataset.</p>
<p>For example, here is a table of data for a simple data mining problem: </p>
<pre><code>outlook, temp,humid,wind,play
-----------------------------
sunny,    85,   85, FALSE,  no
sunny,    80,   90, TRUE,     no
overcast,   83, 86, FALSE,  yes
rainy,    70,   96, FALSE,  yes
rainy,    68,   80, FALSE,  yes
rainy,    65,   70, TRUE,     no
overcast,   64, 65, TRUE,     yes
sunny,    72,   95, FALSE,  no
sunny,    69,   70, FALSE,  yes
rainy,    75,   80, FALSE,  yes
sunny,    75,   70, TRUE,     yes
overcast,   72, 90, TRUE,     yes
overcast,   81, 75, FALSE,  yes
rainy,    71,   91, TRUE,     no
</code></pre>

<p>In this table, we are trying to predict for the goal of play?, given a record of the weather.</p>
<p>Each row is one example where we did or did not play golf (and the goal of data mining is to find what weather predicts for playing golf).</p>
<p>Note that temp and humidity are numeric columns and there are no missing values.</p>
<p>Such simple tables are characterized by just a few columns and not many rows (say, dozens to thousands). </p>
<p>Traditionally, such simple data mining problems have been explored by C4.5 and CART. </p>
<p>However, with some clever sampling of the data, it is possible to scale these traditional learners to Big Data problems.</p>
<h2 id="y-fx">Y = F(X)</h2>
<p>One way to look at a table of data is an example of some function that computes
columns "<em>Y</em>" from input columns "<em>X</em>".</p>
<h3 id="splits">Splits</h3>
<p>Another way to look at a table of data is as a source of <code>Split</code>s.</p>
<ul>
<li>Columns have ranges</li>
<li>Most ranges are not interesting (not useful for decision making)</li>
<li>So most columns are not interesting <ul>
<li>Standard lesson: need only <code>sqrt(col)</code> of the columns (and for text mining data, even fewer)</li>
</ul>
</li>
</ul>
<p>Sym columns:</p>
<ul>
<li>Splits are solo simples or disjunctions</li>
</ul>
<p>Num columns:</p>
<ul>
<li>Splits can be found oh so many ways. In one file called auto, I can set a <code>min</code>imum size (say sqrt(size(rows))
  and combine adjacent bins when the combination is no different to the parts. This is the expected value calculation<ul>
<li>let <code>X</code> be a bin of size n and standard deviation <code>s</code></li>
<li>if <code>X&lt;=2*min</code> return <code>X</code></li>
<li>else
        - Split <code>X</code> into <code>Y,Z</code> of size <code>n1,n2</code> and standard deviation <code>s1,s2</code>
        -  If <code>n1/n+s1 + n2/n*s2 &lt; s</code>, then recurse to return <code>split(Y)</code>, <code>split(Z)</code>
        -  Else return <code>X</code></li>
</ul>
</li>
</ul>
<h3 id="why-split">Why Split?</h3>
<p>Timm's rule: the best thing to do with data is to throw most of it away.</p>
<ul>
<li>Simpler models,</li>
<li>Quicker to explain, audit</li>
<li>Less work for anything down stream that has to watch or act on any
  variable</li>
</ul>
<p>Occam's Razor</p>
<ul>
<li>English philosopher, William of
    Occam (1300-1349) propounded Occam's Razor:<ul>
<li>Entia non sunt multiplicanda praeter necessitatem.</li>
<li>(Latin for "Entities should not be multiplied more
    than necessary"). That is, the fewer assumptions an explanation
    of a phenomenon depends on, the better it is.</li>
</ul>
</li>
<li>(BTW, Occam's razor did not survive into the 21st century.<ul>
<li>The data mining community modified it to the <em>Minimum
    Description Length</em> (MDL) principle.</li>
<li>MDL: the best theory is the smallest BOTH is size AND number
    of errors).</li>
</ul>
</li>
<li>Many ways to throw away data<ul>
<li>feature selection</li>
<li>range selection</li>
<li>instance selection (prototype generation)</li>
</ul>
</li>
</ul>
<h2 id="feature-section">Feature Section</h2>
<p>The case for FSS</p>
<p>Repeated result: throwing out features rarely damages a theory</p>
<p><img alt="" src="https://raw.githubusercontent.com/txt/fss16/master/img/fsses.gif" /></p>
<p>And, sometimes, feature removal is very useful:</p>
<ul>
<li>
<p>E.g. linear regression on
    <a href="https://raw.githubusercontent.com/abutcher/toe/master/lisp/data/arff/bn.arff">bn.arff</a>
    yielded:</p>
<pre><code>Defects =
    82.2602 * S1=L,M,VH +
    158.6082 * S1=M,VH +
    249.407  * S1=VH +
     41.0281 * S2=L,H +
     68.9153 * S2=H +
    151.9207 * S3=M,H +
    125.4786 * S3=H +
    257.8698 * S4=H,M,VL +
    108.1679 * S4=VL +
    134.9064 * S5=L,M +
   -385.7142 * S6=H,M,VH +
    115.5933 * S6=VH +
   -178.9595 * S7=H,L,M,VL +
   ...
   [ 50 lines deleted ]
</code></pre>
</li>
<li>
<p>On a 10-way cross-validation, this correlates 0.45 from predicted
    to actuals.</p>
</li>
<li>
<p>10 times, take 90% of the date and run a <em>WRAPPER</em>- a best first
    search through combinations of attributes. At each step, linear
    regression was called to asses a particular combination
    of attributes. In those ten experiments, WRAPPER found that adding
    feature X to features A,B,C,... improved correlation the following
    number of times:</p>
<pre><code>number of folds (%)  attribute
           2( 20 %)     1 S1
           0(  0 %)     2 S2
           2( 20 %)     3 S3
           1( 10 %)     4 S4
           0(  0 %)     5 S5
           1( 10 %)     6 S6
           6( 60 %)     7 S7     &lt;==
           1( 10 %)     8 F1
           1( 10 %)     9 F2
           2( 20 %)    10 F3
           2( 20 %)    11 D1
           0(  0 %)    12 D2
           5( 50 %)    13 D3     &lt;==
           0(  0 %)    14 D4
           0(  0 %)    15 T1
           1( 10 %)    16 T2
           1( 10 %)    17 T3
           1( 10 %)    18 T4
           0(  0 %)    19 P1
           1( 10 %)    20 P2
           0(  0 %)    21 P3
           1( 10 %)    22 P4
           6( 60 %)    23 P5     &lt;==
           1( 10 %)    24 P6
           2( 20 %)    25 P7
           1( 10 %)    26 P8
           0(  0 %)    27 P9
           2( 20 %)    28 Hours
           8( 80 %)    29 KLoC   &lt;==
           4( 40 %)    30 Language
           3( 30 %)    32 log(hours)
</code></pre>
</li>
<li>
<p>Four variables appeared in the majority of folds. A second run did a
    10-way using just those variables to yield a smaller model
    with (much) larger correlation (98\%):</p>
<pre><code>Defects =
    876.3379 * S7=VL +
   -292.9474 * D3=L,M +
    483.6206 * P5=M +
      5.5113 * KLoC +
     95.4278
</code></pre>
</li>
</ul>
<h3 id="excess-attributes">Excess attributes</h3>
<ul>
<li>Confuse decision tree learners<ul>
<li>Too much early splitting of data</li>
<li>Less data available for each sub-tree</li>
</ul>
</li>
<li>Too many things correlated to class?<ul>
<li>Dump some of them!</li>
</ul>
</li>
</ul>
<h3 id="why-fss">Why FSS?</h3>
<ul>
<li>throw away noisy attributes</li>
<li>throw away redundant attributes</li>
<li>smaller model= better accuracies (often)</li>
<li>smaller model= simpler explanation</li>
<li>smaller model= less variance</li>
<li>smaller model= any downstream processing will thank you</li>
</ul>
<h3 id="problem">Problem</h3>
<ul>
<li>Exploring all subsets exponential</li>
<li>Need heuristic methods to cull search;<ul>
<li>e.g. forward/back select</li>
</ul>
</li>
<li>
<p>Forward select:</p>
<ul>
<li>start with empty set</li>
<li>grow via hill climbing:</li>
<li>repeat<ul>
<li>try adding one thing and if that improves things</li>
<li>try again using the remaining attributes</li>
</ul>
</li>
<li>until no improvement after N additions OR nothing to add</li>
<li>
<p>Back select</p>
</li>
<li>
<p>as above but start with all attributes and discard, don't add</p>
</li>
<li>
<p>Usually, we throw away most attributes:</p>
</li>
<li>
<p>so forward select often better</p>
</li>
<li>exception: J48 exploits interactions more than,say, NB.</li>
<li>so, possibly, back select is better when wrapping j48</li>
<li>so, possibly, forward select is as good as it gets for NB</li>
</ul>
</li>
</ul>
<h3 id="supervised-vs-unsupervised">Supervised vs Unsupervised</h3>
<ul>
<li>Supervised, use the class variable in column2 to discretize column1.<ul>
<li>E.g. split column1 such that the etropy of the column2 symbols are minimized.</li>
<li>see below</li>
</ul>
</li>
<li>Unsupervised, just refect on column1.<ul>
<li>E.g. find splits that minimize the variance afer the spits.</li>
</ul>
</li>
</ul>
<p>E.g. here unsupervised discretization running on the <code>horsepower</code> column of <code>auto.csv</code>. Note these
numbers run 46 to 230 and this code](http://menzies.us/lean/unsuper.html) decides that this should be divided into:</p>
<ul>
<li>less that 65</li>
<li>66 to 69</li>
<li>69 to 72</li>
<li>74 to 82</li>
<li>83 to 86</li>
<li>87 to 89</li>
<li>90 to 91</li>
<li>92 to 97</li>
<li>98 to 105</li>
<li>107 to 116</li>
<li>120 to 140</li>
<li>142 to 160</li>
<li>over 165</li>
</ul>
<pre><code>46.. 230
|.. 46.. 116
|.. |.. 46.. 82
|.. |.. |.. 46.. 65 (..65)
|.. |.. |.. 66.. 82
|.. |.. |.. |.. 66.. 72
|.. |.. |.. |.. |.. 66.. 69 (66..69)
|.. |.. |.. |.. |.. 69.. 72 (69..72)
|.. |.. |.. |.. 74.. 82 (74..82)
|.. |.. 83.. 116
|.. |.. |.. 83.. 97
|.. |.. |.. |.. 83.. 91
|.. |.. |.. |.. |.. 83.. 86 (83..86)
|.. |.. |.. |.. |.. 87.. 91
|.. |.. |.. |.. |.. |.. 87.. 89 (87..89)
|.. |.. |.. |.. |.. |.. 90.. 91 (90..91)
|.. |.. |.. |.. 92.. 97 (92..97)
|.. |.. |.. 98.. 116
|.. |.. |.. |.. 98.. 105 (98..105)
|.. |.. |.. |.. 107.. 116 (107..116)
|.. 120.. 230
|.. |.. 120.. 160
|.. |.. |.. 120.. 140 (120..140)
|.. |.. |.. 142.. 160 (142..160)
|.. |.. 165.. 230 (165..)
</code></pre>

<p><img alt="" src="../../img/unsuper.png" /></p>
<h3 id="fss-types">FSS types:</h3>
<p><img alt="" src="https://raw.githubusercontent.com/txt/fss16/master/img/filter-img.jpg" />
<img alt="" src="https://raw.githubusercontent.com/txt/fss16/master/img/wrapper-img.jpg" /></p>
<ul>
<li>
<p>filters vs wrappers:</p>
<ul>
<li>wrappers: use an actual target learners e.g. WRAPPER</li>
<li>filters: study aspects of the data e.g. the rest</li>
<li>filters are faster!</li>
<li>wrappers exploit bias of target learner so often perform better,
    when they terminate<ul>
<li>don't terminate on large data sets</li>
</ul>
</li>
<li>
<p>solo vs combinations:</p>
</li>
<li>
<p>evaluate solo attributes: e.g. INFO GAIN, RELIEF</p>
</li>
<li>evaluate combinations: e.g. PCA, SVD, CFS, CBS, WRAPPER</li>
<li>solos can be faster than combinations</li>
<li>
<p>supervised vs unsupervised:</p>
</li>
<li>
<p>use/ignores class values e.g. PCA/SVD is unsupervised, reset
    supervised</p>
</li>
<li>
<p>numeric vs discrete search methods</p>
</li>
<li>
<p>ranker: for schemes that numerically score attributes e.g.
    RELIEF, INFO GAIN,</p>
</li>
<li>
<p>best first: for schemes that do heuristic search e.g. CBS, CFS,
    WRAPPER</p>
</li>
</ul>
</li>
</ul>
<h3 id="hall-and-holmes">Hall and Holmes:</h3>
<p>This paper: pre-discretize numerics using entropy.</p>
<p><a href="http://www.cs.waikato.ac.nz/~mhall/HallHolmesTKDE.pdf">Hall &amp; Holmes</a></p>
<h3 id="info-gain">INFO GAIN</h3>
<ul>
<li>often useful in high-dimensional problems<ul>
<li>real simple to calculate</li>
</ul>
</li>
<li>attributes scored based on info gain: H(C) - H(C|A)</li>
<li>Sort of like doing decision tree learning, just to one level.</li>
</ul>
<h3 id="relief">RELIEF</h3>
<ul>
<li><a href="http://menzies.us/iccle/?refs#Kononenko97">Kononenko97</a></li>
<li>useful attributes differentiate between instances from other class</li>
<li>randomly pick some instances (here, 250)</li>
<li>find something similar, in an another class</li>
<li>compute distance this one to the other one</li>
<li>Stochastic sampler: scales to large data sets.</li>
<li>
<p>Binary RELIEF (two class system) for "n" instances for weights on
    features "F"</p>
<pre><code>set all weights W[f]=0
for i = 1 to n; do
   randomly select instance R with class C
   find nearest hit H      // closest thing of same class
   find nearest miss M     // closest thing of difference class
   for f = 1 to #features; do
       W[f] = W[f] - diff(f,R,H)/n + diff(f,R,M)/n
   done
done
</code></pre>
</li>
<li>
<p>diff:</p>
<ul>
<li>discrete differences: 0 if same 1 if not.</li>
<li>continuous: differences absolute differences</li>
<li>normalized to 0:1</li>
<li>When values are missing, see
    <a href="http://menzies.us/iccle/?refs#Kononenko97">Kononenko97</a>, p4.</li>
</ul>
</li>
<li>
<p>N-class RELIEF: not 1 near hit/miss, but k nearest misses for each
    class C</p>
<pre><code>W[f]= W[f] - ∑i=1..k diff(f,R, Hi) / (n*k)
           + ∑C ≠ class(R) ∑i=1..k (
                                P(C) / ( 1 - P(class(R)))
                                * diff(f,R, Mi(C)) / (n*k)
                               )
</code></pre>
<p>The <em>P(C) / (1 - P(class(R))</em> expression is a normalization function
that</p>
<ul>
<li>demotes the effect of R from rare classes</li>
<li>and rewards the effect of near hits from common classes.</li>
</ul>
</li>
</ul>
<h3 id="cbs-consistency-based-evaluation">CBS (consistency-based evaluation)</h3>
<ul>
<li>Seek combinations of attributes that divide data containing a strong
    single class majority.<ul>
<li>Kind of like info gain, but emphasis of single winner</li>
</ul>
</li>
<li>Discrete attributes</li>
<li>Forward select to find subsets of attributes</li>
</ul>
<h3 id="wrapper">WRAPPER</h3>
<ul>
<li>Forward select attributes<ul>
<li>score each combination using a 5-way cross val</li>
</ul>
</li>
<li>When wrapping, best to try different target learners<ul>
<li>Check that we aren't over exploiting the learner's bias</li>
<li>e.g. J48 and NB</li>
</ul>
</li>
</ul>
<p><img alt="" src="https://raw.githubusercontent.com/txt/fss16/master/img/wrapper1-img.jpg" /></p>
<h3 id="principal-components-analysis-pca">PRINCIPAL COMPONENTS ANALYSIS (PCA)</h3>
<p>(The traditional way to do FSS.)</p>
<ul>
<li>Only unsupervised method studied here</li>
<li>Transform dimensions</li>
<li>Find covariance matrix C[i,j] is the correlation i to j;<ul>
<li>C[i,i]=1;</li>
<li>C[i,j]=C[j,i]</li>
</ul>
</li>
<li>Find eigenvectors</li>
<li>Transform the original space to the eigenvectors</li>
<li>Rank them by the variance in their predictions</li>
<li>Report the top ranked vectors</li>
</ul>
<p><img alt="" src="https://raw.githubusercontent.com/txt/fss16/master/img/pca-img.jpg" /></p>
<ul>
<li>Makes things easier, right? Well...<pre><code>if   domain1  &lt;= 0.180
then NoDefects
else if domain1 &gt; 0.180
     then if domain1 &lt;= 0.371 then NoDefects
     else if domain1 &gt; 0.371 then Defects

domain1 = 0.241 * loc     + 0.236 * v(g)
        + 0.222 * ev(g)   + 0.236 * iv(g)     + 0.241 *  n
        + 0.238 * v       - 0.086 * l         + 0.199  * d
        + 0.216 * i       + 0.225 * e + 0.236 * b + 0.221  * t
        + 0.241 * lOCode  + 0.179 * lOComment
        + 0.221 * lOBlank + 0.158 * lOCodeAndComment
        + 0.163 * uniqO p + 0.234 * uniqOpnd
        + 0.241 * totalOp + 0.241 * totalOpnd
        + 0.236 * branchCount
</code></pre>
</li>
</ul>
<h3 id="cfs-correlation-based-feature-selection">CFS (correlation-based feature selection)</h3>
<ul>
<li>Scores high subsets with strong correlation to class and weak
    correlation to each other.</li>
<li>Numerator: how predictive</li>
<li>Denominator: how redundant</li>
<li>FIRST ranks correlation of solo attributes</li>
<li>THEN heuristic search to explore subsets</li>
</ul>
<h3 id="and-the-winner-is">And the winner is:</h3>
<ul>
<li>Wrapper! and it that is too slow...</li>
<li>CFS, Relief are best all round performers<ul>
<li>CFS selects fewer features</li>
</ul>
</li>
<li>Phew. Hall invented CFS</li>
</ul>
<h2 id="instance-selection">Instance Selection</h2>
<h3 id="prototype-selection-with-clusters">Prototype Selection with Clusters</h3>
<ul>
<li>Step1: Feature selection: sort columns by their Infogain score. Delete bottom half.</li>
<li>Step2: Cluster: return one example pre centroid.</li>
</ul>
<p><img alt="" src="https://raw.githubusercontent.com/txt/fss16/master/img/clusters.png" /></p>
<p>Reduction of 800 rows by 24 attributes to 5 attributes by 22 rows</p>
<p><img alt="" src="https://raw.githubusercontent.com/txt/fss16/master/img/fssvasil.png" /></p>
<p>For many data sets:</p>
<p><img alt="" src="https://raw.githubusercontent.com/txt/fss16/master/img/reductions.png" /></p>
<p>Note that for classification by weighted scores from 2 nearest neighbors,
the reduced data as accurate as the full data.</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"search": 83, "next": 78, "help": 191, "previous": 80};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
