{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Foundations of Software Science Fall 2018 NC State Computer Science Tuesdays, Thursdays, 4:30pm Prof. Tim Menzies Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. Graduates of this class will becomes producers , not mere consumers , of AI software. This subject will explore methods for generating and using models built from data (i.e. how to collectthat data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects). For more admin details, see the syllabus . Why? Facetiously, I'll say this subject asks: \"If deep learning is the answer, what was the question?\". In this subject, we ask \"what kind of learners do we need?\" then work backwards from there to design new AI methods, or improve existing ones. This is an important task since the following is a very pressing issue: The future of SE is more and more AI . But AI software is still software; So as society uses more AI, SE folks will be required to use and maintain and extend that software; So how should SE people look at AI software? What should they expect from that software? What would be an AI software \"bad smells\" that prompts a code reorganization? Enter this subject. What? How to teach software how to be a scientist In short, we will teach software how to a scientist; i.e. how to automatically build, critique, and revise models. How to build and modify and refactor and remix and repurpose: Software for data miners; Software for optimizers; Software for theorem provers. To help society, and our customers, achieve their dreams better, faster, and cheaper. How to achieve maximum AI benefits at minimal AI costs: By recognize and modify bias in learners. By reducing the CPU and memory footprints of AI; By respecting the privacy of citizens in a society shared with AI. How? Monthly: Can you do better than last year's high water mark ? A 500 to 1000-fold speed up of a text miner? August = Start-up September = Data mining October = Optimizers and theorem provers November = Your do your own big project that utilizes the tools and perspectivs of this sibject. Weekly (till mid-October): Small, simple programming assignments Regular poster presentations by students on some tiny aspect of AI+SE For full details, see the syllabus .","title":"Home"},{"location":"#foundations-of-software-science","text":"Fall 2018 NC State Computer Science Tuesdays, Thursdays, 4:30pm Prof. Tim Menzies Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. Graduates of this class will becomes producers , not mere consumers , of AI software. This subject will explore methods for generating and using models built from data (i.e. how to collectthat data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects). For more admin details, see the syllabus .","title":"Foundations of Software Science"},{"location":"#why","text":"Facetiously, I'll say this subject asks: \"If deep learning is the answer, what was the question?\". In this subject, we ask \"what kind of learners do we need?\" then work backwards from there to design new AI methods, or improve existing ones. This is an important task since the following is a very pressing issue: The future of SE is more and more AI . But AI software is still software; So as society uses more AI, SE folks will be required to use and maintain and extend that software; So how should SE people look at AI software? What should they expect from that software? What would be an AI software \"bad smells\" that prompts a code reorganization? Enter this subject.","title":"Why?"},{"location":"#what","text":"How to teach software how to be a scientist In short, we will teach software how to a scientist; i.e. how to automatically build, critique, and revise models. How to build and modify and refactor and remix and repurpose: Software for data miners; Software for optimizers; Software for theorem provers. To help society, and our customers, achieve their dreams better, faster, and cheaper. How to achieve maximum AI benefits at minimal AI costs: By recognize and modify bias in learners. By reducing the CPU and memory footprints of AI; By respecting the privacy of citizens in a society shared with AI.","title":"What?"},{"location":"#how","text":"Monthly: Can you do better than last year's high water mark ? A 500 to 1000-fold speed up of a text miner? August = Start-up September = Data mining October = Optimizers and theorem provers November = Your do your own big project that utilizes the tools and perspectivs of this sibject. Weekly (till mid-October): Small, simple programming assignments Regular poster presentations by students on some tiny aspect of AI+SE For full details, see the syllabus .","title":"How?"},{"location":"about/","text":"Neque iam est venti flamma corpora aderam Consulit cunctae cruore inposuit Quis superi Lorem markdownum; si muta , iam vidi. Gravet crevit, afuerunt aetherias datus, gravitate utque; amnemque breve et excutiant . Scit vineta vincas dentes tympana. Imagine me mediis sequentis nati coloni illa distabat? Que sed creavit terreris flammaeque ipsos cornuaque ponto iugulo formaeque Seditioque naves sors temptanti. Servaberis Cererem alis, est sua his divam euntem, opemque. Debes vectus , ait, ordine , pallentemque valeant respicere, et ego. Hanc illo favilla promissis ut habenis Tritoniaca: sex ad matres habet , distinxit. Dum mens populos intrarant at genusque multaque Non fuit ipse, vim sed Erectheus sequitur: contra et corymbis. Cadit pars vitulus , erat anno inmiti momordit mihi, sparserat sacravere haut concussit miles. Puro una , est figura celer. Neque virum poterat Sunt studiumque audit iam Dubitat natus caret poteram suspicere Dea templis sede alimenta obscenique feroces vidit Eosdem mutata A pudori nemorum loquentem, si sic relicta, flammas violentaque matrem. Tamen omnia nec cernitis mugitus popularia paterno formas flava sed? Inferius haec et Alce sacerdos, imago unde dea ab tacitus dixi . redundancy_ccd_menu.fileWysiwygWpa(url, directory_floppy_meta - toggle, led( tiffToggleScrolling, printer)); tableTroubleshootingAlu(syntax, server_aiff); leopard_bios += c_jpeg(computing_unfriend_led(fileRosettaUp, winsock)); var dvdStationBluetooth = font(wiredDriver.hot_json_dithering(secondary, key, pim_bluetooth_bit) / 3); Visa fecit, esse Circes omnibus forma : Minervae expulit cum colat quiescere. Ignotis status mutatum, Haemus , videntur, femineo intrarunt de absunt iubentque turba inritamina suae. Quae neci non has aurigam plangorem quae, sensit frustra molitur laceri, alendi madidus quattuor. Romulus tum , qui non ab movent conligit Coronida, nostra; et.","title":"Neque iam est venti flamma corpora aderam"},{"location":"about/#neque-iam-est-venti-flamma-corpora-aderam","text":"","title":"Neque iam est venti flamma corpora aderam"},{"location":"about/#consulit-cunctae-cruore-inposuit-quis-superi","text":"Lorem markdownum; si muta , iam vidi. Gravet crevit, afuerunt aetherias datus, gravitate utque; amnemque breve et excutiant . Scit vineta vincas dentes tympana. Imagine me mediis sequentis nati coloni illa distabat? Que sed creavit terreris flammaeque ipsos cornuaque ponto iugulo formaeque Seditioque naves sors temptanti. Servaberis Cererem alis, est sua his divam euntem, opemque. Debes vectus , ait, ordine , pallentemque valeant respicere, et ego. Hanc illo favilla promissis ut habenis Tritoniaca: sex ad matres habet , distinxit.","title":"Consulit cunctae cruore inposuit Quis superi"},{"location":"about/#dum-mens-populos-intrarant-at-genusque-multaque","text":"Non fuit ipse, vim sed Erectheus sequitur: contra et corymbis. Cadit pars vitulus , erat anno inmiti momordit mihi, sparserat sacravere haut concussit miles. Puro una , est figura celer. Neque virum poterat Sunt studiumque audit iam Dubitat natus caret poteram suspicere Dea templis sede alimenta obscenique feroces vidit","title":"Dum mens populos intrarant at genusque multaque"},{"location":"about/#eosdem-mutata","text":"A pudori nemorum loquentem, si sic relicta, flammas violentaque matrem. Tamen omnia nec cernitis mugitus popularia paterno formas flava sed? Inferius haec et Alce sacerdos, imago unde dea ab tacitus dixi . redundancy_ccd_menu.fileWysiwygWpa(url, directory_floppy_meta - toggle, led( tiffToggleScrolling, printer)); tableTroubleshootingAlu(syntax, server_aiff); leopard_bios += c_jpeg(computing_unfriend_led(fileRosettaUp, winsock)); var dvdStationBluetooth = font(wiredDriver.hot_json_dithering(secondary, key, pim_bluetooth_bit) / 3); Visa fecit, esse Circes omnibus forma : Minervae expulit cum colat quiescere. Ignotis status mutatum, Haemus , videntur, femineo intrarunt de absunt iubentque turba inritamina suae. Quae neci non has aurigam plangorem quae, sensit frustra molitur laceri, alendi madidus quattuor. Romulus tum , qui non ab movent conligit Coronida, nostra; et.","title":"Eosdem mutata"},{"location":"history/","text":"SE + AI: then and now SE's past is full of cases where someone declared \"X\" was not part of SE then we ignored them and added \"X\" to SE and lots of things got lots better So the question I pose to you is this: Q: What is currently \"not\" SE, but soon must be? A: AI Enter this subject SE: the past e.g. \"SE is not about requirements engineering\" (which is wrong) e.g. From Boehm, Keynote, 2004 , slide 8: \"The notion of 'user' cannot be precisely defined, and therefore has no place in CS or SE.\" Edsger Dijkstra, ICSE 4, 1979 \"Analysis and allocation of the system requirements is not the responsibility of the SE group but is a prerequisite for their work.\" Mark Paulk at al., SEI Software CMM* v.1.1, 1993 e.g. \"Programmning is not about testing\" (wrong again) e.g. Harlin Mills, 1984 : software engineers should write, but not run or test, their own software \"Cleanroom software engineering\" No unit testing (instead, mathematical verification) so before we run anything, we write perfect code And there is a seperate testing team to the programming team e.g. \"Programming is not about deploying software\" (so very, very wrong) Before devops, the coding team used to hand off the system to the production team Now we do much less of that.. achieving must faster change cycles Q: So What's next? A: AI SE: the present Software now mediates what we see and how we act Chemists win Nobel Prize for software sims Engineers use software to design optical tweezers, radiation therapy, remote sensing, chip design Web analysts use software to analyze clickstreams to improve sales and marketing strategies Stock traders write software to simulate trading strategies Analysts write software to mine labor statistics data to review proposed gov policies Journalists use software to analyze economic data, make visualizations of their news stories Etc etc etc In short, now more than ever, software really really matters In London or New York, The time for ambulance to reach patient is controlled by models If you cross the border Arizona to Mexico, Models determine if you are taken away for extra security measures If you default on a car loans, Models determine when (or if) someone repossesses your car Autonomous cars Software is essential to international financial and transport systems; our energy generation and distribution systems; and even the pacemakers that control the beat of our hearts. Looking forward, to the forthcoming age of autonomous cars and flying drones, it is clear that software models (written in traditional programming languages or in some next-generation interpretation) will be key in determining what we can do, when, where, and how. So how can we help our AI systems reason better about our data, and our models? Using data mining, we might learn a model from data that predicts for (say) a single target class; Using optionzers, we might a multi-objective optimizer to find what solutions score best on multiple target variables. Also, data miners can be used to to summarize the data, after which optimizers can leap to better solutions, faster ; Also, optimizers can be used to select intelligent settings for data mining algorithms e.g. such as how many trees should be included in a random forest. SE: the future Software enginenering isn't just about software any more Olde SE: just polish up the lens of the telescope New SE (with AI): use the telescope to look and understand and change \"things\" After \"continuous integration\" (where we automated everything) Comes \"AI everywhere\" (where we automate automation). From Software Analytics: What\u2019s Next? , IEEE Software, Sept/Oct 2018: \"Consider the rise of the data scientist in industry. Many organizations now pay handsomely to hire large teams of data scientists. For example, at the time of this writing, there are more than 1,000 Microsoft employees exploring project data using software analytics. These teams are performing tasks that a decade ago would have been called cutting-edge research. But now we call that work standard operating procedure .\" \"Every innovation also offers new opportunities. There is a flow-on effect from software analytics to other AI tasks outside of software engineering. Software analytics lets software engineers learn about AI techniques, all the while practicing on domains they understand (i.e., their own development practices). Once developers can apply data-mining algorithms to their data, they can build and ship innovative AI tools. While sometimes those tools solve software engineering problems (e.g., recommending what to change in source code), they can also be used on a wider range of problems. That is, we see software analytics as the training ground for the next generation of AI-literate software engineers working on applications such as image recognition, large-scale text mining, autonomous cars, drones, etc.\" \"What is the most important technology newcomers should learn to make themselves better at data science (in general) and software analytics (in particular)? \"To answer this question, we need a workable definition of \u201cscience,\u201d which we take to mean a community of people collecting, curating, and critiquing a set of ideas. In this community, everyone does each other the courtesy to try to prove this shared pool of ideas. By this definition, most data science (and much software analytics) is not science. Many developers use software analytics tools to produce conclusions, and that\u2019s the end of the story. Those conclusions are not registered and monitored. There is nothing that checks whether old conclusions are now out of date (e.g., using anomaly detectors). There are no incremental revisions that seek minimal changes when updating old ideas. \"If software analytics really wants to be called a science, then it needs to be more than just a way to make conclusions about the present. Any scientist will tell you that all ideas should be checked, rechecked, and incrementally revised. Data science methods such as software analytics should be a tool for assisting in complex discussions about ongoing issues. Which is a long-winded way of saying that the technology we most need to better understand software analytics and data science is ... science.\"","title":"SE+AI,  then and now &#10004;"},{"location":"history/#se-ai-then-and-now","text":"SE's past is full of cases where someone declared \"X\" was not part of SE then we ignored them and added \"X\" to SE and lots of things got lots better So the question I pose to you is this: Q: What is currently \"not\" SE, but soon must be? A: AI Enter this subject","title":"SE + AI: then and now"},{"location":"history/#se-the-past","text":"","title":"SE: the past"},{"location":"history/#eg-se-is-not-about-requirements-engineering-which-is-wrong","text":"e.g. From Boehm, Keynote, 2004 , slide 8: \"The notion of 'user' cannot be precisely defined, and therefore has no place in CS or SE.\" Edsger Dijkstra, ICSE 4, 1979 \"Analysis and allocation of the system requirements is not the responsibility of the SE group but is a prerequisite for their work.\" Mark Paulk at al., SEI Software CMM* v.1.1, 1993","title":"e.g. \"SE is not about requirements engineering\" (which is wrong)"},{"location":"history/#eg-programmning-is-not-about-testing-wrong-again","text":"e.g. Harlin Mills, 1984 : software engineers should write, but not run or test, their own software \"Cleanroom software engineering\" No unit testing (instead, mathematical verification) so before we run anything, we write perfect code And there is a seperate testing team to the programming team","title":"e.g. \"Programmning  is not about testing\" (wrong again)"},{"location":"history/#eg-programming-is-not-about-deploying-software-so-very-very-wrong","text":"Before devops, the coding team used to hand off the system to the production team Now we do much less of that.. achieving must faster change cycles","title":"e.g. \"Programming  is not about deploying software\"  (so very, very wrong)"},{"location":"history/#q-so-whats-next","text":"A: AI","title":"Q: So What's next?"},{"location":"history/#se-the-present","text":"Software now mediates what we see and how we act Chemists win Nobel Prize for software sims Engineers use software to design optical tweezers, radiation therapy, remote sensing, chip design Web analysts use software to analyze clickstreams to improve sales and marketing strategies Stock traders write software to simulate trading strategies Analysts write software to mine labor statistics data to review proposed gov policies Journalists use software to analyze economic data, make visualizations of their news stories Etc etc etc In short, now more than ever, software really really matters In London or New York, The time for ambulance to reach patient is controlled by models If you cross the border Arizona to Mexico, Models determine if you are taken away for extra security measures If you default on a car loans, Models determine when (or if) someone repossesses your car Autonomous cars Software is essential to international financial and transport systems; our energy generation and distribution systems; and even the pacemakers that control the beat of our hearts. Looking forward, to the forthcoming age of autonomous cars and flying drones, it is clear that software models (written in traditional programming languages or in some next-generation interpretation) will be key in determining what we can do, when, where, and how. So how can we help our AI systems reason better about our data, and our models? Using data mining, we might learn a model from data that predicts for (say) a single target class; Using optionzers, we might a multi-objective optimizer to find what solutions score best on multiple target variables. Also, data miners can be used to to summarize the data, after which optimizers can leap to better solutions, faster ; Also, optimizers can be used to select intelligent settings for data mining algorithms e.g. such as how many trees should be included in a random forest.","title":"SE: the present"},{"location":"history/#se-the-future","text":"Software enginenering isn't just about software any more Olde SE: just polish up the lens of the telescope New SE (with AI): use the telescope to look and understand and change \"things\" After \"continuous integration\" (where we automated everything) Comes \"AI everywhere\" (where we automate automation). From Software Analytics: What\u2019s Next? , IEEE Software, Sept/Oct 2018: \"Consider the rise of the data scientist in industry. Many organizations now pay handsomely to hire large teams of data scientists. For example, at the time of this writing, there are more than 1,000 Microsoft employees exploring project data using software analytics. These teams are performing tasks that a decade ago would have been called cutting-edge research. But now we call that work standard operating procedure .\" \"Every innovation also offers new opportunities. There is a flow-on effect from software analytics to other AI tasks outside of software engineering. Software analytics lets software engineers learn about AI techniques, all the while practicing on domains they understand (i.e., their own development practices). Once developers can apply data-mining algorithms to their data, they can build and ship innovative AI tools. While sometimes those tools solve software engineering problems (e.g., recommending what to change in source code), they can also be used on a wider range of problems. That is, we see software analytics as the training ground for the next generation of AI-literate software engineers working on applications such as image recognition, large-scale text mining, autonomous cars, drones, etc.\" \"What is the most important technology newcomers should learn to make themselves better at data science (in general) and software analytics (in particular)? \"To answer this question, we need a workable definition of \u201cscience,\u201d which we take to mean a community of people collecting, curating, and critiquing a set of ideas. In this community, everyone does each other the courtesy to try to prove this shared pool of ideas. By this definition, most data science (and much software analytics) is not science. Many developers use software analytics tools to produce conclusions, and that\u2019s the end of the story. Those conclusions are not registered and monitored. There is nothing that checks whether old conclusions are now out of date (e.g., using anomaly detectors). There are no incremental revisions that seek minimal changes when updating old ideas. \"If software analytics really wants to be called a science, then it needs to be more than just a way to make conclusions about the present. Any scientist will tell you that all ideas should be checked, rechecked, and incrementally revised. Data science methods such as software analytics should be a tool for assisting in complex discussions about ongoing issues. Which is a long-winded way of saying that the technology we most need to better understand software analytics and data science is ... science.\"","title":"SE: the future"},{"location":"inspiration/","text":"Inspiration Light the fire Learn why the world wags and what wags it Live for the surprise \u201cIf the world merely lived up to our wildest dreams, what a dull place it would be. Happily\u2026\u201d \u2013 Tim Menzies","title":"Inspiration &#10004;"},{"location":"inspiration/#inspiration","text":"","title":"Inspiration"},{"location":"inspiration/#light-the-fire","text":"","title":"Light the fire"},{"location":"inspiration/#learn-why-the-world-wags-and-what-wags-it","text":"","title":"Learn why the world wags and what wags it"},{"location":"inspiration/#live-for-the-surprise","text":"\u201cIf the world merely lived up to our wildest dreams, what a dull place it would be. Happily\u2026\u201d \u2013 Tim Menzies","title":"Live for the surprise"},{"location":"license/","text":"License Tim Menzies 2018 This work is licensed under a Creative Commons Attribution 4.0 International License You are free to: Share: copy and redistribute the material in any medium or format Adapt: remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions: You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: Notice: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"(c) 2018"},{"location":"license/#license","text":"Tim Menzies 2018 This work is licensed under a Creative Commons Attribution 4.0 International License You are free to: Share: copy and redistribute the material in any medium or format Adapt: remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions: You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: Notice: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"License"},{"location":"syllabus/","text":"Syllabus Foundations of software science NCSU, CS, Fall 2018 CSC 591-023 (16403) CSC 791-023 (17051) Tues/Thurs 4:30 to 5:45 EE III, Room 2232 Synopsis Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. This subject will explore methods for designing data collection experiments; collecting that data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects. Objectives By the end of the course, students should be able to Build and modify and refactor and remix and repurpose AI software (data miners, optimizers, theorem provers); Report on complex technical issues (spoken talk) Report on complex technical issues (written talk) Discuss issues associated with applying the above to help society, and customers, achieve their goals better, faster, and cheaper. Staff Lecturer Tim Menzies (Prof) Office Hours: Tuesday, 2:00-4:00 and by request Location of Office Hours: EE II room 3298 Slack name: timm E-Mail: timm@ieee.org Only use this email for private matters. All other class communication should be via the class Slack group http://found18.slack.com . Phone: 304-376-2859 Do not use this number, except in the most dire of circumstances (best way to contact me is via email). Teaching assistant TBD Office Hours: TBD Location of Office Hours: TBD Slack name: TBD Group mailing list During term time, all communication will be via the Slack group https://found18.slack.com. . Students are strongly encouraged to contribute their questions and answers to that shared resource. + Note that, for communication of a more private nature, contact the lecturer on the email shown above. Prerequisite Note that this is a programming-intensive subject. A programming background is required in a contemporary language such as Java or C/C++ or Python. Hence,he prerequisite for this class is 510, Software Engineering. Significant software industry experience may be substituted, at the instructor\u2019s discretion. Students in this class will work in Python, but no background knowledge of that language will be assumed. Suggested texts none Expected Workload Sometimes, the lecturer/tutor will require you to attend a review session during their consultation time. There, students may be asked to review code, concepts, or comment on the structure of the course. Those sessions are mandatory and failure to attend will result in marks being deducted. Also, this is tools-based subject and it is required that students learn and use those tools (Python, repositories, etc). Students MUST be prepared to dedicate AT LEAST 5-8 working hours a week to this class (excluding the time spent in the classroom). Laboratory instruction is not included in this subject (but the first three weeks will be spent on some in-depth programming tutorials). Note that the workload for masters and Ph.D. students will be different (see above). Grades The following grade scale will be used: A+ (97-100), A (93-96), A-(90-92) B+ (87-89), B (83-86), B-(80-82) C+ (77-79), C (73-76), C-(70-72) D+ (67-69), D (63-66), D-(60-62) F (below 60). Grades will be added together using: Weekly solo homeworks (till October): 8 marks (8 homeworks 1 mark each) Homeworks can be resubmitted till you get full marks on them. But: No homeworks will be accepted after October 15. Once you submit homework i+1, your marks for homework 1..i will freeze. Only 2 homeworks (max) will be marked per student per week (this includes resubmissions) You lose 0.5 marks for any week where you do not submit a homework (including resubmissions) Mid-term Oct 23 (on terminology): 22 marks Solo poster (on current directions in foundations of software science): 10 marks 2 page paper Posters are 2 pages pdf in ACM format Sample posters can be found here Topic: Posters will explain some interesting aspect on some paper on some work NOT authored by Menzies. Something on data mining and/or search-based SE and/or thorem proving as applied to some SE task To find examples of that work, see papers after 2012 from top SE-venues Google scholar, top SE-venues In particular, ICSE, TSE, JSS, IST, MSR, ESE, ASE, TOSEM, ICSM (now ICSME) Poster may burrow definitions and graphics from the paper they are reviewing. BUT. Anything the student must be able to explain in further detail anything they put into their poster. Presentation (5 mins talk, 5 mins questions) 10 marks No slides Instead, place your 2 page paper under the podium camera, then talk to the content. IMPORTANT : Prior to class, post your paper to the submit site . Otherwise, lose 1 mark. Big project (due end November) on automated SE): 25 (essay) 15 (presentation) 10 (code review) Project must present two implementations An initial implementation which you will critize using our baseline criteria A second implementation where the you strive to improve that implementation, according to any of the baseline criteria. Note that merely doing some existing standard data mining project will NOT be sufficcient Paper are 8+ pages pdf in ACM format The code review will be the lecturer reading the code checking that the implementation is above a minimum level of effort. Lecturer may call students/ teams at any time to his office for such reviews. Presentations will start mid-November Paper is not due till Dec 1. Note that: Master students will do the big project in groups of 3. All other work is solo. Submissions will be posted to the submit site . Take long urls and shorten them with (e.g.) tiny.cc before psiting. Attendance Attendance is extremely important for your learning experience in this class. Once you reach three unexcused absences, each additional absence will reduce your attendance grade by 10%. Except for officially allowed reasons, your presence in the class if required from day one. Late-comers will have to work in their own solo groups (to avoid disruptions to existing groups). Note that absences for weddings (your own, or someone else's, is not an offically allowed reason). Exceptions: this subject will support students who are absent for any of the following officially allowed reasons: Anticipated Absences (cleared with the instructor before the absence). Examples of anticipated situations include representing an official university function, e.g., participating in a professional meeting, as part of a judging team, or athletic team; required court attendance as certified by the Clerk of Court; religious observances as verified by the Division of Academic and Student Affairs (DASA). Required military duty as certified by the student's commanding officer. Unanticipated Absences. Excuses must be reported to the instructor not more than one week after the return to class. Examples of unanticipated absences are: Short-term illness or injury affecting the ability to attend or to be productive academically while in class, or that could jeopardize the health of the individual or the health of the classmates attending. Students must notify instructors prior to the class absence, if possible, that they are temporarily unable to attend class or complete assignments on time. Death or serious illnesses in the family when documented appropriately. An attempt to verify deaths or serious illness will be made by the Division of Academic and Student Affairs. That support will include changing the schedule of deliverables and/or (in extreme case) different grading arrangements. Academic Integrity Cheating will be punished to the full extent permitted. Cheating includes plagerism of other people's work. All students will be working on public code repositories and informed reuse is encouraged where someone else's product is: Imported and clearly acknowledged (as to where it came from); The imported project is understood, and The imported project is significantly extended. Students are encouraged to read each others code and repor uninformed reuse to the lecturer. The issue will be explored and, if uncovered, cheating will be reported to the university and marks will be deducted if the person who is doing the reuse: Does not acknowledge the source of the product; Does not exhibit comprehension of the product when asked about it; Does not significantly extend the product. All students are expected to maintain traditional standards of academic integrity by giving proper credit for all work. All suspected cases of academic dishonesty will be aggressively pursued. You should be aware of the University policy on academic integrity found in the Code of Student Conduct. The exams will be done individually. Academic integrity is important. Do not work together on the exams: cheating on either will be punished to the full extent permitted. Disabilities Reasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with Disability Services for Students at 1900 Student Health Center, Campus Box 7509, 919-515-7653. For more information on NC State's policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation(REG 02.20.01). Students are responsible for reviewing the PRRs which pertain to their course rights and responsibilities. These include: http://policies.ncsu.edu/policy/pol-04-25-05 (Equal Opportunity and Non-Discrimination Policy Statement), http://oied.ncsu.edu/oied/policies.php (Office for Institutional Equity and Diversity),http://policies.ncsu.edu/policy/pol-11-35-01 (Code of Student Conduct), and http://policies.ncsu.edu/regulation/reg-02-50-03 (Grades and Grade Point Average). Non-Discrimination Policy NC State University provides equality of opportunity in education and employment for all students and employees. Accordingly, NC State affirms its commitment to maintain a work environment for all employees and an academic environment for all students that is free from all forms of discrimination. Discrimination based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation is a violation of state and federal law and/or NC State University policy and will not be tolerated. Harassment of any person (either in the form of quid pro quo or creation of a hostile environment) based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation also is a violation of state and federal law and/or NC State University policy and will not be tolerated. Note that, as a lecturer, I am legally required to report all such acts to the campus policy. Retaliation against any person who complains about discrimination is also prohibited. NC State's policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or http://www.ncsu.edu/equal_op/. Any person who feels that he or she has been the subject of prohibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148. Other Information Non-scheduled class time for field trips or out-of-class activities are NOT required for this class. No such trips are currently planned. However, if they do happen then students are required to purchase liability insurance. For more information, see http://www2.acs.ncsu.edu/insurance/","title":"Syllabus &#10004;"},{"location":"syllabus/#syllabus","text":"Foundations of software science NCSU, CS, Fall 2018 CSC 591-023 (16403) CSC 791-023 (17051) Tues/Thurs 4:30 to 5:45 EE III, Room 2232","title":"Syllabus"},{"location":"syllabus/#synopsis","text":"Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. This subject will explore methods for designing data collection experiments; collecting that data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects.","title":"Synopsis"},{"location":"syllabus/#objectives","text":"By the end of the course, students should be able to Build and modify and refactor and remix and repurpose AI software (data miners, optimizers, theorem provers); Report on complex technical issues (spoken talk) Report on complex technical issues (written talk) Discuss issues associated with applying the above to help society, and customers, achieve their goals better, faster, and cheaper.","title":"Objectives"},{"location":"syllabus/#staff","text":"","title":"Staff"},{"location":"syllabus/#lecturer","text":"Tim Menzies (Prof) Office Hours: Tuesday, 2:00-4:00 and by request Location of Office Hours: EE II room 3298 Slack name: timm E-Mail: timm@ieee.org Only use this email for private matters. All other class communication should be via the class Slack group http://found18.slack.com . Phone: 304-376-2859 Do not use this number, except in the most dire of circumstances (best way to contact me is via email).","title":"Lecturer"},{"location":"syllabus/#teaching-assistant","text":"TBD Office Hours: TBD Location of Office Hours: TBD Slack name: TBD","title":"Teaching assistant"},{"location":"syllabus/#group-mailing-list","text":"During term time, all communication will be via the Slack group https://found18.slack.com. . Students are strongly encouraged to contribute their questions and answers to that shared resource. + Note that, for communication of a more private nature, contact the lecturer on the email shown above.","title":"Group mailing list"},{"location":"syllabus/#prerequisite","text":"Note that this is a programming-intensive subject. A programming background is required in a contemporary language such as Java or C/C++ or Python. Hence,he prerequisite for this class is 510, Software Engineering. Significant software industry experience may be substituted, at the instructor\u2019s discretion. Students in this class will work in Python, but no background knowledge of that language will be assumed.","title":"Prerequisite"},{"location":"syllabus/#suggested-texts","text":"none","title":"Suggested texts"},{"location":"syllabus/#expected-workload","text":"Sometimes, the lecturer/tutor will require you to attend a review session during their consultation time. There, students may be asked to review code, concepts, or comment on the structure of the course. Those sessions are mandatory and failure to attend will result in marks being deducted. Also, this is tools-based subject and it is required that students learn and use those tools (Python, repositories, etc). Students MUST be prepared to dedicate AT LEAST 5-8 working hours a week to this class (excluding the time spent in the classroom). Laboratory instruction is not included in this subject (but the first three weeks will be spent on some in-depth programming tutorials). Note that the workload for masters and Ph.D. students will be different (see above).","title":"Expected Workload"},{"location":"syllabus/#grades","text":"The following grade scale will be used: A+ (97-100), A (93-96), A-(90-92) B+ (87-89), B (83-86), B-(80-82) C+ (77-79), C (73-76), C-(70-72) D+ (67-69), D (63-66), D-(60-62) F (below 60). Grades will be added together using: Weekly solo homeworks (till October): 8 marks (8 homeworks 1 mark each) Homeworks can be resubmitted till you get full marks on them. But: No homeworks will be accepted after October 15. Once you submit homework i+1, your marks for homework 1..i will freeze. Only 2 homeworks (max) will be marked per student per week (this includes resubmissions) You lose 0.5 marks for any week where you do not submit a homework (including resubmissions) Mid-term Oct 23 (on terminology): 22 marks Solo poster (on current directions in foundations of software science): 10 marks 2 page paper Posters are 2 pages pdf in ACM format Sample posters can be found here Topic: Posters will explain some interesting aspect on some paper on some work NOT authored by Menzies. Something on data mining and/or search-based SE and/or thorem proving as applied to some SE task To find examples of that work, see papers after 2012 from top SE-venues Google scholar, top SE-venues In particular, ICSE, TSE, JSS, IST, MSR, ESE, ASE, TOSEM, ICSM (now ICSME) Poster may burrow definitions and graphics from the paper they are reviewing. BUT. Anything the student must be able to explain in further detail anything they put into their poster. Presentation (5 mins talk, 5 mins questions) 10 marks No slides Instead, place your 2 page paper under the podium camera, then talk to the content. IMPORTANT : Prior to class, post your paper to the submit site . Otherwise, lose 1 mark. Big project (due end November) on automated SE): 25 (essay) 15 (presentation) 10 (code review) Project must present two implementations An initial implementation which you will critize using our baseline criteria A second implementation where the you strive to improve that implementation, according to any of the baseline criteria. Note that merely doing some existing standard data mining project will NOT be sufficcient Paper are 8+ pages pdf in ACM format The code review will be the lecturer reading the code checking that the implementation is above a minimum level of effort. Lecturer may call students/ teams at any time to his office for such reviews. Presentations will start mid-November Paper is not due till Dec 1. Note that: Master students will do the big project in groups of 3. All other work is solo. Submissions will be posted to the submit site . Take long urls and shorten them with (e.g.) tiny.cc before psiting.","title":"Grades"},{"location":"syllabus/#attendance","text":"Attendance is extremely important for your learning experience in this class. Once you reach three unexcused absences, each additional absence will reduce your attendance grade by 10%. Except for officially allowed reasons, your presence in the class if required from day one. Late-comers will have to work in their own solo groups (to avoid disruptions to existing groups). Note that absences for weddings (your own, or someone else's, is not an offically allowed reason). Exceptions: this subject will support students who are absent for any of the following officially allowed reasons: Anticipated Absences (cleared with the instructor before the absence). Examples of anticipated situations include representing an official university function, e.g., participating in a professional meeting, as part of a judging team, or athletic team; required court attendance as certified by the Clerk of Court; religious observances as verified by the Division of Academic and Student Affairs (DASA). Required military duty as certified by the student's commanding officer. Unanticipated Absences. Excuses must be reported to the instructor not more than one week after the return to class. Examples of unanticipated absences are: Short-term illness or injury affecting the ability to attend or to be productive academically while in class, or that could jeopardize the health of the individual or the health of the classmates attending. Students must notify instructors prior to the class absence, if possible, that they are temporarily unable to attend class or complete assignments on time. Death or serious illnesses in the family when documented appropriately. An attempt to verify deaths or serious illness will be made by the Division of Academic and Student Affairs. That support will include changing the schedule of deliverables and/or (in extreme case) different grading arrangements.","title":"Attendance"},{"location":"syllabus/#academic-integrity","text":"Cheating will be punished to the full extent permitted. Cheating includes plagerism of other people's work. All students will be working on public code repositories and informed reuse is encouraged where someone else's product is: Imported and clearly acknowledged (as to where it came from); The imported project is understood, and The imported project is significantly extended. Students are encouraged to read each others code and repor uninformed reuse to the lecturer. The issue will be explored and, if uncovered, cheating will be reported to the university and marks will be deducted if the person who is doing the reuse: Does not acknowledge the source of the product; Does not exhibit comprehension of the product when asked about it; Does not significantly extend the product. All students are expected to maintain traditional standards of academic integrity by giving proper credit for all work. All suspected cases of academic dishonesty will be aggressively pursued. You should be aware of the University policy on academic integrity found in the Code of Student Conduct. The exams will be done individually. Academic integrity is important. Do not work together on the exams: cheating on either will be punished to the full extent permitted.","title":"Academic Integrity"},{"location":"syllabus/#disabilities","text":"Reasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with Disability Services for Students at 1900 Student Health Center, Campus Box 7509, 919-515-7653. For more information on NC State's policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation(REG 02.20.01). Students are responsible for reviewing the PRRs which pertain to their course rights and responsibilities. These include: http://policies.ncsu.edu/policy/pol-04-25-05 (Equal Opportunity and Non-Discrimination Policy Statement), http://oied.ncsu.edu/oied/policies.php (Office for Institutional Equity and Diversity),http://policies.ncsu.edu/policy/pol-11-35-01 (Code of Student Conduct), and http://policies.ncsu.edu/regulation/reg-02-50-03 (Grades and Grade Point Average).","title":"Disabilities"},{"location":"syllabus/#non-discrimination-policy","text":"NC State University provides equality of opportunity in education and employment for all students and employees. Accordingly, NC State affirms its commitment to maintain a work environment for all employees and an academic environment for all students that is free from all forms of discrimination. Discrimination based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation is a violation of state and federal law and/or NC State University policy and will not be tolerated. Harassment of any person (either in the form of quid pro quo or creation of a hostile environment) based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation also is a violation of state and federal law and/or NC State University policy and will not be tolerated. Note that, as a lecturer, I am legally required to report all such acts to the campus policy. Retaliation against any person who complains about discrimination is also prohibited. NC State's policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or http://www.ncsu.edu/equal_op/. Any person who feels that he or she has been the subject of prohibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148.","title":"Non-Discrimination Policy"},{"location":"syllabus/#other-information","text":"Non-scheduled class time for field trips or out-of-class activities are NOT required for this class. No such trips are currently planned. However, if they do happen then students are required to purchase liability insurance. For more information, see http://www2.acs.ncsu.edu/insurance/","title":"Other Information"},{"location":"lectures/","text":"","title":"Home"},{"location":"lectures/axe/","text":"Axe = argmin, argmax of X ArgMin(d,x,list) and ArgMax(d,x,list) are functions that seek the value of element a=list[i] that minimizes/maximizes the expected value of the function x(a) above and below that element. This process is then repeated recursviely d times on each part of the data. A large number of learners, clusterers, evaluation methods can be characeterised in this way. Example1 For example, given the numbers 100,10,90,20,110,30,90,40,120 we might what to know how to best split the numbers so as to minimize standard deviation. Step1: sort the numbers: all= 10,20,30,40,90,100,110,120 i= 1 2 3 4 5 6 7 8 Step2: what is f of the whole space? all.n = 8 all.mu = 65 all.sd = sqrt( sum( (all[i] - all.mu)^2 ) / (n-1) ) = 44.4 Step3: find any split (say,i=2) and calculate the standard deviation above and below that slot left = all[ i = 2 ] right = all[ i 2 ] left.sd = 7.07 right.sd = 37.64 Step 4: find the expected value of the standard deviation afer the split. xpect(n, n1, v1, n2, v2) = n1/n*v1 + n2/n * v2 left.n = 2 right.n = 6 xpect(8, left.n, left.sd, right.n, right.sd) = 30.00 From this we can conclude that splitting at x=2 will reduce the uncertainty in this space from 44.4 to 30. Great! Step5: now repeat the above, but for all values of i . For sanity sake, we might demand that each left and right split not be too small; e.g. enough=2 items. We'll also assume there exists some Num class that can incrementally update and report n and sd . Here's ArgMin(1,sd, list) ; i.e. only one level of split and we want to minimize standard deviation: def sd(x, y): return xpect(x.n + y.n, x.n, x.sd, y.n, y.sd) def lt(x, y): return x y def gt(x, y): return x y def xSplit(lst, lo = 1, hi = length(lst), enough= 2, better= lt, f = sd, best = 10000000): cut = nil if hi-lo 2*enough: # otherwise, why bother? left = Num() right = Num() for i = lo, hi: right + lst[i] # give eveything to right for i = lo, hi: a = lst[i] left + a # add one to the right right - a # remove one from the left if left.n = enough and right.n = enough: tmp = f(left,right) if better( tmp , best ): cut, best = i, tmp return cut,best Note that: This function returns cut=nil if no cut is found. This function assumes the list is sorted before calling this function. For Python use lo,hi = 0 to length-1 ; If you want to generate more than one split, then call it recursively with lo=lo,hi=cut and lo=cut+1,hi=hi and best (in the recursive call) equal to the best return from the above). To minimze on other things then use another xpect function. This can actually serve as a argmax as well. Change better to gt Initialize best to a large negative number. If we called this in the above data, then it would recommend we cut at 4, i.e. the list all= 10,20,30,40,90,100,110,120 i= 1 2 3 4 5 6 7 8 should split into left = 10,20,30,40 right= 90,100,110,120 So what we have right now is a very simple 1-dimensional clustering algorithm for a list of numeric numbers For lists of more complex data types, we need to pass in a selector, which we will call x that extracts the number we want to use from each item in the list. E.g. if clustering on an Employee's age, we might use x=employee.age . Further, given tables of independent and dependent variables, sometimes we want to clsuter on the independent variable in order to have most impact on the dependent variable. e.g. age alive sanity checks Sanit.g.y checks. sd cohen. fayyad iranni. is it valid to split (are the splits actually different?) scott knott FFTtrees which would have an expected value of standard deviation of 12.9 (much less than the origianl 44.4) def gotEnd(x, y) return xpect(x.n + y.n, x.n, x.ent, y.n, y.ent)","title":"Axe"},{"location":"lectures/axe/#axe-argmin-argmax-of-x","text":"ArgMin(d,x,list) and ArgMax(d,x,list) are functions that seek the value of element a=list[i] that minimizes/maximizes the expected value of the function x(a) above and below that element. This process is then repeated recursviely d times on each part of the data. A large number of learners, clusterers, evaluation methods can be characeterised in this way.","title":"Axe = argmin, argmax of X"},{"location":"lectures/axe/#example1","text":"For example, given the numbers 100,10,90,20,110,30,90,40,120 we might what to know how to best split the numbers so as to minimize standard deviation. Step1: sort the numbers: all= 10,20,30,40,90,100,110,120 i= 1 2 3 4 5 6 7 8 Step2: what is f of the whole space? all.n = 8 all.mu = 65 all.sd = sqrt( sum( (all[i] - all.mu)^2 ) / (n-1) ) = 44.4 Step3: find any split (say,i=2) and calculate the standard deviation above and below that slot left = all[ i = 2 ] right = all[ i 2 ] left.sd = 7.07 right.sd = 37.64 Step 4: find the expected value of the standard deviation afer the split. xpect(n, n1, v1, n2, v2) = n1/n*v1 + n2/n * v2 left.n = 2 right.n = 6 xpect(8, left.n, left.sd, right.n, right.sd) = 30.00 From this we can conclude that splitting at x=2 will reduce the uncertainty in this space from 44.4 to 30. Great! Step5: now repeat the above, but for all values of i . For sanity sake, we might demand that each left and right split not be too small; e.g. enough=2 items. We'll also assume there exists some Num class that can incrementally update and report n and sd . Here's ArgMin(1,sd, list) ; i.e. only one level of split and we want to minimize standard deviation: def sd(x, y): return xpect(x.n + y.n, x.n, x.sd, y.n, y.sd) def lt(x, y): return x y def gt(x, y): return x y def xSplit(lst, lo = 1, hi = length(lst), enough= 2, better= lt, f = sd, best = 10000000): cut = nil if hi-lo 2*enough: # otherwise, why bother? left = Num() right = Num() for i = lo, hi: right + lst[i] # give eveything to right for i = lo, hi: a = lst[i] left + a # add one to the right right - a # remove one from the left if left.n = enough and right.n = enough: tmp = f(left,right) if better( tmp , best ): cut, best = i, tmp return cut,best Note that: This function returns cut=nil if no cut is found. This function assumes the list is sorted before calling this function. For Python use lo,hi = 0 to length-1 ; If you want to generate more than one split, then call it recursively with lo=lo,hi=cut and lo=cut+1,hi=hi and best (in the recursive call) equal to the best return from the above). To minimze on other things then use another xpect function. This can actually serve as a argmax as well. Change better to gt Initialize best to a large negative number. If we called this in the above data, then it would recommend we cut at 4, i.e. the list all= 10,20,30,40,90,100,110,120 i= 1 2 3 4 5 6 7 8 should split into left = 10,20,30,40 right= 90,100,110,120 So what we have right now is a very simple 1-dimensional clustering algorithm for a list of numeric numbers For lists of more complex data types, we need to pass in a selector, which we will call x that extracts the number we want to use from each item in the list. E.g. if clustering on an Employee's age, we might use x=employee.age . Further, given tables of independent and dependent variables, sometimes we want to clsuter on the independent variable in order to have most impact on the dependent variable. e.g. age alive sanity checks Sanit.g.y checks. sd cohen. fayyad iranni. is it valid to split (are the splits actually different?) scott knott FFTtrees which would have an expected value of standard deviation of 12.9 (much less than the origianl 44.4) def gotEnd(x, y) return xpect(x.n + y.n, x.n, x.ent, y.n, y.ent)","title":"Example1"},{"location":"lectures/baselines/","text":"Baseline for an \"Adequate\" AI Software engineering is about engineering and engineering is about generate a produce of adequate quality, given the available constraints. What does that mean for AI-enhanced software? Within any optimizer or data mining toolkit we can find hundreds of classifiers, regression tools, neural nets, support vector machines, evolutionary algorithms, ant-colony optimizers, etc etc, etc. These primitives can be combined in millions of ways, then tuned in quadrillions of ways (see the very active research literature on all these methods). So given a new problem, which learner/optimizer should we apply? This is a very hard problem. Wolpert reports in his famous No Free Lunch Theorems that if some optimizer/learner works best for some data, then some other optimizer/learner will work best for other data . This means that when new data arrives, you need commissioning experiments ; i.e. try a variety of techniques before you can find what words best for the local data. (Aside: it turns out that the NFL has some good news for us: the greater the performance gain desired, the fewer the learners exist that produce at least such a performance gain . See the Hyperband optimizer for an adaptive approach to pruning away less-than-great methods. Also, for many learners/optimizers, their performance is indistinguishable for anything less than some value. So if we divide the output space into bins of width means we can stop looking once we find a few methods that falls into the best bins .) When conducting such commissioning experiments, it is methodologically useful to have a baseline method ; i.e. an algorithm which can generate floor performance values. Such baselines let a developer quickly rule out any method that falls \u201cbelow the floor\u201d. With this, researchers and industrial practitioners can achieve fast early results, while also gaining some guidance in all their subsequent experimentation (specifically: \u201ctry to beat the baseline\u201d). Using baselines for analyzing algorithms has been endorsed by several experienced researchers: In his textbook on empirical methods for artificial intelligence , Cohen strongly recommends comparing supposedly sophisticated systems against simpler alternatives. In the machine learning community, Holte uses the OneR baseline algorithm as a scout that runs ahead of a more complicated learners as a way to judge the complexity of up-coming tasks. In the software engineering community, Sarro et al et al. recently proposed baseline methods for effort estimation . Shepperd and Macdonnel argue convincingly that measurements are best viewed as ratios compared to measurements taken from some minimal baseline system . Work on cross-versus within-company cost estimation has also recommended the use of some very simple baseline I've offered several good baseline AI tools for SE tasks. In both the following, my graduate students were able to replace widely used very complex solutions with much simple alternative. For example: Search-based SE: A Baseline Method For Search-Based Software Engineering Data mining: Bellwethers: A Baseline Method For Transfer Learning Optimizing: \"Sampling\"' as a Baseline Optimizer for Search-based Software Engineering So for this subject, we propose replacing the question of \"what AI tool is best\" with two other questions that make more sense to engineers racing to deliver products with limited time and resources: How can we quickly commission an initial, adequate, AI baseline system? What can we do to test and improve on that baseline? But what is a \"good\" baseline? Here's one list of what a \"baseline\" means. Items 1..10 are adapted from Sarro, TOSEM'18 . (which we extend with our own notes). The other items come from my experience. Note also that the following list offers a road map for future research in SE+AI. Find cases where some of these points do not matter. Find ways to enhance existing systems such that they perform better on the following criteria. Etc. The key thing to note about the following is that one system may not satisfy all these criteria (in fact, no known system satisfied all of them). That said, each of the following points is important. And by reflecting on the value of each point for a particular AI application, we naturally consider and review (and possibly discard) important design alternatives. The Checklist Now stay calm citizens of FSS'18. The following is not as complex as it looks. while there are many complex ways to support the following, there are also very simple ways that can work very well for each. And for your project, you only have to understand a one of the following. 1. SIMPLE: Be simple to describe, implement, and interpret (i.e. interpret the output for business uses). My current \"simplest\" methods is Fast-and-Frugal decision trees (or see also here ); which is available in a nice R-package . 2. REASONABLE: Offer comparable performance to standard methods. So note the great paradox of simplicity research Curious fact: evaluation can get so hard that we usually try to milk it for everything that can. So \"cross val\" experiments lead us to ensemble learning then to boosting ; \"round robin\" lead to transfer learning ; \"jiggle a little\" lead to evolutionary methods ; evolutionary methods. It is very hard to be simple Cause the simplest thing has to be compared against other, more complex, things. 3. STABLE: Be deterministic stable in its outcomes I've replaced deterministic with stable, since I think that is more important. Instability is very unsettling for software project managers struggling to find general policies. Project managers lose faith in the results of software analytics if those results keep changing. Also, such instability prevents project managers from offering clear guidelines on what to change, or what to avoid, in a project. And instability plagues SE data . For example: Here , Fig1, are the coefficients learned by regression on 20 67% samples of some training data. Note their WILD instability. One trick for increasing stability is not to focus on all the smaller details e.g. when learning regression equations, do not use all the variables; e.g. when learning rules, avoid wide conditions e.g. when learning trees, do not learn deep trees). Of course, if you optimize for simplicity, you may pay a performance penalty. 4a. INTUITIVE: Be applicable to mixed qualitative and quantitative data. It is good to use numeric and symbolic data. It is useful to be able to initial a systems with qualitative intuitions. Then, at least, you can compare the output to what folks already believe. But be warned, in SE, the beliefs of many developers are... dubious . If for no other reasons that humans have numerous cognitive biases For a really long list of those biases, see Wikipedia or this chart It is useful to be able to guide model construction via high-level qualitative goals. One useful technology here are Bayes nets which can be either initially drawn by people, then revised by data miners, or vice versa. Another trick is to use some incremental rule learning algorithm that updates many possible new rules, then scores them by their distance to old rules (and the best new rules are those that are closest to old and score highest). In that rig, user background beliefs would become the first generation rules. Yet another method is to use multi-objective optimizers that fit rule learner to human biases. 4b. COMPREHENSIBLE: This is connected to 4a. Essential for communities critiquing ideas. If the only person reading a model is a carburetor, then we can expect little push back. But if your models are about policies that humans have to implement, then I take it as axiomatic that humans will want to read and critique the models. 5. GENERAL: Offer some explanatory information regarding the prediction by representing generalized properties of the underlying data. Many systems offer only \"point\" solutions; i.e. examples of what might be useful. Given N attributes, a point solution offers exact values for all attributes. E.g. the output of most evolutionary programs E.g. all instance-based (nearest neighbor) methods E.g. A happy author might be editing this particular file and this particular time and place. Some systems offer solutions that hold over a volume; I.e. they ignore some values while saying things like x 10 for others. E.g. Happy authors might be editing html files on many computers (and when they do it does not matter). One way to generalize a instance-based method is to cluster the solutions, then only report ranges that are different in different clusters. 6. NO MAGIC: Have no magic parameters within the modeling process that require tuning. E.g. for random forests, engineers have to decide on how many trees are included in the forest. Alternatively, if such tunings exist, then the must be some automatic method for selecting what tunings are best for particular data sets. 7. AVAILABLE: Be publicly available via a reference implementation and associated environment for execution. In this day and age of Docker images and package managers and Github-like environments where everyone can load up each other's code at the drop of a hat, it makes no sense for some baseline tool to be inaccessible. 8. USEFUL: Generally be more accurate than a random guess or (e.g. an estimate based purely on the distribution of the response variable). E.g. evaluate the output via \"standardarized error\"; i.e. compare the prediction to some some prediction generated from (say) the median value of the response variable. 9. CHEAP: Do not be expensive to apply. Here we mean that the CPU, Ram, and disk space required to make something work is not crazy high. \"CHEAP is important since ~Reproducing and improving an old ideas means that you can reproduce that old result. Also, certifying that new ideas often means multiple runs over many sub-samples of the data. Such reproducibility and certification is impractical when such reproduction is impractically slow 10. ROBUST: I.e. does not change much over different data splits and validation methods? And if it does vary wildly, can it find ways to find regions in the data where the data conclusions are stable. 11. GOAL-AWARE: Different goals means different models. AND multiple goals = no problem! This is important since most data miners build models that optimizer for a single goal (e.g. minimize error or least-square error) yet business users often want their data miners to achieve many goals. For example, if we want to ask \"what to do\" rather than \"what is\", then we need a planner, not a classifier. Of course, in that case, the classifier can be used as a what-if guide to assess different plans. 12. CONTEXT-AWARE: context-aware: Easy path context awareness: first cluster the data, then build different models for different clusters: see NbTrees . Knows that local parts of data generate different models. E.g. hierarchically clusters the data and builds one model per cluster. While general principles are good, so too is how to handle particular contexts. For example, in general, exercise is good for maintaining healthy. However, in the particular context of patients who have just had cardiac surgery, then that general principle has to be carefully tailored to particular patients. ideas need to be updated. 13. HUMBLE: Easy path to certification envelopes: cluster data, report k items per cluster. Can publish succinct certification envelope that can report when new data is out-of-scope to what was seen before.(so we know when not to trust) This is important since the delivered data mined models should be able to recognize when new data is out-of-scope of anything they\u2019ve seen before. This means, at runtime, having access to the data used to build that model. Note that phrase succinct here: certification envelopes cannot include all the data relating to a model, otherwise every hard drive in the world will soon fill up. Another form of humility is knowing when the baseline should be replaced with something else. Holte uses the OneR baseline algorithm as a scout that runs ahead of a more complicated learners as a way to judge the complexity of up-coming tasks. 14. STREAMING: Can run over an infinite stream of data, updating itself (or knows when to go back to old versions of itself). Easy path to anomaly detection: cluster data, report items that fall far from each cluster. Can detect anomalies (when new inputs differ from old training data). This is the trigger for re-learning. 15. SHARABLE: Knows how to transfer models, data, between contexts. Easy path to lightweight sharing: just share reduced data from context awareness. Need some way to keep the volume of shared data down (otherwise \"sharing\" would clog the Internet). Such transfer may requires some transformation of the source data to the target data. 16. PRIVACY-AWARE: Easy path to privacy: within the clusters of the certification envelope, just share k items per cluster, each slightly mutated. See LACE2 . Can hide an individual's data This is essential when sharing a certification envelope Project The project of this class is to apply the above to AI tools applied to SE problems. Even trying to apply the above and not getting anywhere, would also be fine (just as you long as you document your comprehension of the ideas of baselines, along the way). So go seek, or build, good baselines: Take any SE problem and ask are the current methods \"baselines?\". Would simpler alternatives suffice? Can you make the method simpler to use; e.g. replace it with something much simpler to implement and explain e.g. apply an optimizer to a data mining to find better settings from that data miner? If you replace the complex with the simpler, what (if any) is the performance penalty? Can you make the method use less RAM or be faster to use; e.g. see what happens if you learn on just X% of the data (randomly selected) for X {50,25,10,5,1}%? If you apply a prototype generator, can you select/build a very small subset of the data from which learning is faster and just as effective? Finding prototypes can be as easy as \"cluster and take just a few from each cluster\" But there are many other methods eg. apply a data miner to an optimizer to divide up the data to make the whole process much faster? See 500+ faster than deep learning . Does that method need additional support to enable explanation of their output? Do their models fail the stability test? How does that method respond if you run it N times on 90% of the data? And if they do, can you find regions of the data where the performance is stable? How to reduce the CPU and RAM and runtime requirements of that method by large amounts e.g. see 500+ faster than deep learning . If we stream over the data, how soon does this model stabilize? If we inject mutations into the data, can this method be used to recognize that strange data? Once the weird data arrives, how long (if ever) before the model recovers? If a model is update, can be it done some minimally ; i.e. with least change to the existing model? etc etc etc All Connected The more we compress the smaller the memory and the faster we learn and the less we need to share (so more privacy). The more we understand the data's prototypes the more we know what is usual/ unusual so we more we know what is anomalous so the easier it is to offer a certification envelope Note that if our compression method is somehow hierarchical and if we track the errors seen by our learners in different subtrees then the more we know which parts of the model need revising (and which can stay the same). Which means we only make revisions to the parts that matter, leaving the rest stable. Other Requirements No eval tools Tests conclusion stability across multiple data sets (if available) or across multiple subsets of know scenarios See Evaluation for many examples of that kind of evaluation. Note that these can significantly increase the computational cost of using learners. Hence, the need to faster , lighter AI algorithms. No support for stats tests Check if this treatment has same effect as that treatment. Need at least two tests: significance and effect size I also think you need a third test; Something that clusters the treatments before the other tests are applied Reduces the number of other statistical tests. E.g the Scott-Knot test.","title":"Baselines for Adequate AI &#10004;"},{"location":"lectures/baselines/#baseline-for-an-adequate-ai","text":"Software engineering is about engineering and engineering is about generate a produce of adequate quality, given the available constraints. What does that mean for AI-enhanced software? Within any optimizer or data mining toolkit we can find hundreds of classifiers, regression tools, neural nets, support vector machines, evolutionary algorithms, ant-colony optimizers, etc etc, etc. These primitives can be combined in millions of ways, then tuned in quadrillions of ways (see the very active research literature on all these methods). So given a new problem, which learner/optimizer should we apply? This is a very hard problem. Wolpert reports in his famous No Free Lunch Theorems that if some optimizer/learner works best for some data, then some other optimizer/learner will work best for other data . This means that when new data arrives, you need commissioning experiments ; i.e. try a variety of techniques before you can find what words best for the local data. (Aside: it turns out that the NFL has some good news for us: the greater the performance gain desired, the fewer the learners exist that produce at least such a performance gain . See the Hyperband optimizer for an adaptive approach to pruning away less-than-great methods. Also, for many learners/optimizers, their performance is indistinguishable for anything less than some value. So if we divide the output space into bins of width means we can stop looking once we find a few methods that falls into the best bins .) When conducting such commissioning experiments, it is methodologically useful to have a baseline method ; i.e. an algorithm which can generate floor performance values. Such baselines let a developer quickly rule out any method that falls \u201cbelow the floor\u201d. With this, researchers and industrial practitioners can achieve fast early results, while also gaining some guidance in all their subsequent experimentation (specifically: \u201ctry to beat the baseline\u201d). Using baselines for analyzing algorithms has been endorsed by several experienced researchers: In his textbook on empirical methods for artificial intelligence , Cohen strongly recommends comparing supposedly sophisticated systems against simpler alternatives. In the machine learning community, Holte uses the OneR baseline algorithm as a scout that runs ahead of a more complicated learners as a way to judge the complexity of up-coming tasks. In the software engineering community, Sarro et al et al. recently proposed baseline methods for effort estimation . Shepperd and Macdonnel argue convincingly that measurements are best viewed as ratios compared to measurements taken from some minimal baseline system . Work on cross-versus within-company cost estimation has also recommended the use of some very simple baseline I've offered several good baseline AI tools for SE tasks. In both the following, my graduate students were able to replace widely used very complex solutions with much simple alternative. For example: Search-based SE: A Baseline Method For Search-Based Software Engineering Data mining: Bellwethers: A Baseline Method For Transfer Learning Optimizing: \"Sampling\"' as a Baseline Optimizer for Search-based Software Engineering So for this subject, we propose replacing the question of \"what AI tool is best\" with two other questions that make more sense to engineers racing to deliver products with limited time and resources: How can we quickly commission an initial, adequate, AI baseline system? What can we do to test and improve on that baseline?","title":"Baseline for an \"Adequate\" AI"},{"location":"lectures/baselines/#but-what-is-a-good-baseline","text":"Here's one list of what a \"baseline\" means. Items 1..10 are adapted from Sarro, TOSEM'18 . (which we extend with our own notes). The other items come from my experience. Note also that the following list offers a road map for future research in SE+AI. Find cases where some of these points do not matter. Find ways to enhance existing systems such that they perform better on the following criteria. Etc. The key thing to note about the following is that one system may not satisfy all these criteria (in fact, no known system satisfied all of them). That said, each of the following points is important. And by reflecting on the value of each point for a particular AI application, we naturally consider and review (and possibly discard) important design alternatives.","title":"But what is a \"good\" baseline?"},{"location":"lectures/baselines/#the-checklist","text":"Now stay calm citizens of FSS'18. The following is not as complex as it looks. while there are many complex ways to support the following, there are also very simple ways that can work very well for each. And for your project, you only have to understand a one of the following. 1. SIMPLE: Be simple to describe, implement, and interpret (i.e. interpret the output for business uses). My current \"simplest\" methods is Fast-and-Frugal decision trees (or see also here ); which is available in a nice R-package . 2. REASONABLE: Offer comparable performance to standard methods. So note the great paradox of simplicity research Curious fact: evaluation can get so hard that we usually try to milk it for everything that can. So \"cross val\" experiments lead us to ensemble learning then to boosting ; \"round robin\" lead to transfer learning ; \"jiggle a little\" lead to evolutionary methods ; evolutionary methods. It is very hard to be simple Cause the simplest thing has to be compared against other, more complex, things. 3. STABLE: Be deterministic stable in its outcomes I've replaced deterministic with stable, since I think that is more important. Instability is very unsettling for software project managers struggling to find general policies. Project managers lose faith in the results of software analytics if those results keep changing. Also, such instability prevents project managers from offering clear guidelines on what to change, or what to avoid, in a project. And instability plagues SE data . For example: Here , Fig1, are the coefficients learned by regression on 20 67% samples of some training data. Note their WILD instability. One trick for increasing stability is not to focus on all the smaller details e.g. when learning regression equations, do not use all the variables; e.g. when learning rules, avoid wide conditions e.g. when learning trees, do not learn deep trees). Of course, if you optimize for simplicity, you may pay a performance penalty. 4a. INTUITIVE: Be applicable to mixed qualitative and quantitative data. It is good to use numeric and symbolic data. It is useful to be able to initial a systems with qualitative intuitions. Then, at least, you can compare the output to what folks already believe. But be warned, in SE, the beliefs of many developers are... dubious . If for no other reasons that humans have numerous cognitive biases For a really long list of those biases, see Wikipedia or this chart It is useful to be able to guide model construction via high-level qualitative goals. One useful technology here are Bayes nets which can be either initially drawn by people, then revised by data miners, or vice versa. Another trick is to use some incremental rule learning algorithm that updates many possible new rules, then scores them by their distance to old rules (and the best new rules are those that are closest to old and score highest). In that rig, user background beliefs would become the first generation rules. Yet another method is to use multi-objective optimizers that fit rule learner to human biases. 4b. COMPREHENSIBLE: This is connected to 4a. Essential for communities critiquing ideas. If the only person reading a model is a carburetor, then we can expect little push back. But if your models are about policies that humans have to implement, then I take it as axiomatic that humans will want to read and critique the models. 5. GENERAL: Offer some explanatory information regarding the prediction by representing generalized properties of the underlying data. Many systems offer only \"point\" solutions; i.e. examples of what might be useful. Given N attributes, a point solution offers exact values for all attributes. E.g. the output of most evolutionary programs E.g. all instance-based (nearest neighbor) methods E.g. A happy author might be editing this particular file and this particular time and place. Some systems offer solutions that hold over a volume; I.e. they ignore some values while saying things like x 10 for others. E.g. Happy authors might be editing html files on many computers (and when they do it does not matter). One way to generalize a instance-based method is to cluster the solutions, then only report ranges that are different in different clusters. 6. NO MAGIC: Have no magic parameters within the modeling process that require tuning. E.g. for random forests, engineers have to decide on how many trees are included in the forest. Alternatively, if such tunings exist, then the must be some automatic method for selecting what tunings are best for particular data sets. 7. AVAILABLE: Be publicly available via a reference implementation and associated environment for execution. In this day and age of Docker images and package managers and Github-like environments where everyone can load up each other's code at the drop of a hat, it makes no sense for some baseline tool to be inaccessible. 8. USEFUL: Generally be more accurate than a random guess or (e.g. an estimate based purely on the distribution of the response variable). E.g. evaluate the output via \"standardarized error\"; i.e. compare the prediction to some some prediction generated from (say) the median value of the response variable. 9. CHEAP: Do not be expensive to apply. Here we mean that the CPU, Ram, and disk space required to make something work is not crazy high. \"CHEAP is important since ~Reproducing and improving an old ideas means that you can reproduce that old result. Also, certifying that new ideas often means multiple runs over many sub-samples of the data. Such reproducibility and certification is impractical when such reproduction is impractically slow 10. ROBUST: I.e. does not change much over different data splits and validation methods? And if it does vary wildly, can it find ways to find regions in the data where the data conclusions are stable. 11. GOAL-AWARE: Different goals means different models. AND multiple goals = no problem! This is important since most data miners build models that optimizer for a single goal (e.g. minimize error or least-square error) yet business users often want their data miners to achieve many goals. For example, if we want to ask \"what to do\" rather than \"what is\", then we need a planner, not a classifier. Of course, in that case, the classifier can be used as a what-if guide to assess different plans. 12. CONTEXT-AWARE: context-aware: Easy path context awareness: first cluster the data, then build different models for different clusters: see NbTrees . Knows that local parts of data generate different models. E.g. hierarchically clusters the data and builds one model per cluster. While general principles are good, so too is how to handle particular contexts. For example, in general, exercise is good for maintaining healthy. However, in the particular context of patients who have just had cardiac surgery, then that general principle has to be carefully tailored to particular patients. ideas need to be updated. 13. HUMBLE: Easy path to certification envelopes: cluster data, report k items per cluster. Can publish succinct certification envelope that can report when new data is out-of-scope to what was seen before.(so we know when not to trust) This is important since the delivered data mined models should be able to recognize when new data is out-of-scope of anything they\u2019ve seen before. This means, at runtime, having access to the data used to build that model. Note that phrase succinct here: certification envelopes cannot include all the data relating to a model, otherwise every hard drive in the world will soon fill up. Another form of humility is knowing when the baseline should be replaced with something else. Holte uses the OneR baseline algorithm as a scout that runs ahead of a more complicated learners as a way to judge the complexity of up-coming tasks. 14. STREAMING: Can run over an infinite stream of data, updating itself (or knows when to go back to old versions of itself). Easy path to anomaly detection: cluster data, report items that fall far from each cluster. Can detect anomalies (when new inputs differ from old training data). This is the trigger for re-learning. 15. SHARABLE: Knows how to transfer models, data, between contexts. Easy path to lightweight sharing: just share reduced data from context awareness. Need some way to keep the volume of shared data down (otherwise \"sharing\" would clog the Internet). Such transfer may requires some transformation of the source data to the target data. 16. PRIVACY-AWARE: Easy path to privacy: within the clusters of the certification envelope, just share k items per cluster, each slightly mutated. See LACE2 . Can hide an individual's data This is essential when sharing a certification envelope","title":"The Checklist"},{"location":"lectures/baselines/#project","text":"The project of this class is to apply the above to AI tools applied to SE problems. Even trying to apply the above and not getting anywhere, would also be fine (just as you long as you document your comprehension of the ideas of baselines, along the way). So go seek, or build, good baselines: Take any SE problem and ask are the current methods \"baselines?\". Would simpler alternatives suffice? Can you make the method simpler to use; e.g. replace it with something much simpler to implement and explain e.g. apply an optimizer to a data mining to find better settings from that data miner? If you replace the complex with the simpler, what (if any) is the performance penalty? Can you make the method use less RAM or be faster to use; e.g. see what happens if you learn on just X% of the data (randomly selected) for X {50,25,10,5,1}%? If you apply a prototype generator, can you select/build a very small subset of the data from which learning is faster and just as effective? Finding prototypes can be as easy as \"cluster and take just a few from each cluster\" But there are many other methods eg. apply a data miner to an optimizer to divide up the data to make the whole process much faster? See 500+ faster than deep learning . Does that method need additional support to enable explanation of their output? Do their models fail the stability test? How does that method respond if you run it N times on 90% of the data? And if they do, can you find regions of the data where the performance is stable? How to reduce the CPU and RAM and runtime requirements of that method by large amounts e.g. see 500+ faster than deep learning . If we stream over the data, how soon does this model stabilize? If we inject mutations into the data, can this method be used to recognize that strange data? Once the weird data arrives, how long (if ever) before the model recovers? If a model is update, can be it done some minimally ; i.e. with least change to the existing model? etc etc etc","title":"Project"},{"location":"lectures/baselines/#all-connected","text":"The more we compress the smaller the memory and the faster we learn and the less we need to share (so more privacy). The more we understand the data's prototypes the more we know what is usual/ unusual so we more we know what is anomalous so the easier it is to offer a certification envelope Note that if our compression method is somehow hierarchical and if we track the errors seen by our learners in different subtrees then the more we know which parts of the model need revising (and which can stay the same). Which means we only make revisions to the parts that matter, leaving the rest stable.","title":"All Connected"},{"location":"lectures/baselines/#other-requirements","text":"","title":"Other Requirements"},{"location":"lectures/baselines/#no-eval-tools","text":"Tests conclusion stability across multiple data sets (if available) or across multiple subsets of know scenarios See Evaluation for many examples of that kind of evaluation. Note that these can significantly increase the computational cost of using learners. Hence, the need to faster , lighter AI algorithms.","title":"No eval tools"},{"location":"lectures/baselines/#no-support-for-stats-tests","text":"Check if this treatment has same effect as that treatment. Need at least two tests: significance and effect size I also think you need a third test; Something that clusters the treatments before the other tests are applied Reduces the number of other statistical tests. E.g the Scott-Knot test.","title":"No support for stats tests"},{"location":"lectures/deeplearning/","text":"Deep learning DP is a kind of neural net architecture where the outputs on N i become the inputs to net i+1. Deep learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transforms the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. Compared to the conventional machine learning algorithms, deep learning methods are very good at exploring high-dimensional data. DL has received a lot of press lately. A. Lot. Of. Press. The computational requirements for this method are challenging. One NCSU grad student working at Google this summer was running 3 years of CPU per day (no joke) for his DL experiments. While that that is certainly an extreme example, it does illustrate how costly it can be to tune DL networks. Literally, costly. We costed out one project where 4 students would use commercial GPU clusters to run 20 experiments per week for 3 years. That work would add $1M to the cost of that grant. So it seem DL is the answer, what was the question? If we ask what kind of learners do we need, then work backwards from there, do we get to DL? Perhaps not. Wolpert reports in his famous No Free Lunch Theorems that if some learner works best for some data, then some other earner will work best for other data . Which means that when DL is not the one solution to all problems. Rather, it is one amongst many that we need to try. For example, suppose we don't need a classifier, but a planner. In that case, DL might become part of a larger system than uses some classifier as a what-if query device to test different plans. And if we want an explanation system, then we 'd have to use some symbolic method to run over the same examples as DL to build a second model (and its that model we can show to users). Of course, sometimes DL is necessary. Some domains have such frighteningly complex data that layers of neural networks are required to tease out the complexities of the data. But sometimes, DL is not necessary. And in that case, it seems silly to incur the computational cost of DL. So far, in SE, the case for the superiority of DL other other methods has not been made: Are Deep Neural Networks the Best Choice for Modeling Source Code? Easy over Hard: A Case Study on Deep Learning 500+ Faster than Deep Learning","title":"Deep learning &#10004;"},{"location":"lectures/deeplearning/#deep-learning","text":"DP is a kind of neural net architecture where the outputs on N i become the inputs to net i+1. Deep learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transforms the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. Compared to the conventional machine learning algorithms, deep learning methods are very good at exploring high-dimensional data. DL has received a lot of press lately. A. Lot. Of. Press. The computational requirements for this method are challenging. One NCSU grad student working at Google this summer was running 3 years of CPU per day (no joke) for his DL experiments. While that that is certainly an extreme example, it does illustrate how costly it can be to tune DL networks. Literally, costly. We costed out one project where 4 students would use commercial GPU clusters to run 20 experiments per week for 3 years. That work would add $1M to the cost of that grant. So it seem DL is the answer, what was the question? If we ask what kind of learners do we need, then work backwards from there, do we get to DL? Perhaps not. Wolpert reports in his famous No Free Lunch Theorems that if some learner works best for some data, then some other earner will work best for other data . Which means that when DL is not the one solution to all problems. Rather, it is one amongst many that we need to try. For example, suppose we don't need a classifier, but a planner. In that case, DL might become part of a larger system than uses some classifier as a what-if query device to test different plans. And if we want an explanation system, then we 'd have to use some symbolic method to run over the same examples as DL to build a second model (and its that model we can show to users). Of course, sometimes DL is necessary. Some domains have such frighteningly complex data that layers of neural networks are required to tease out the complexities of the data. But sometimes, DL is not necessary. And in that case, it seems silly to incur the computational cost of DL. So far, in SE, the case for the superiority of DL other other methods has not been made: Are Deep Neural Networks the Best Choice for Modeling Source Code? Easy over Hard: A Case Study on Deep Learning 500+ Faster than Deep Learning","title":"Deep learning"},{"location":"lectures/domination/","text":"Domination Which house do you want: Lets do it again. Given Fig4 of this paper what options would you reject? Congraulations, you've just worked out how to do multi-objective optimization Build Popluation[ i=0 ] by making lots of guesses Experiment: (mutate, cross over) Select the best results (a.k.a know as find the Pareto frontier ) Set Population[ i+1 ] to the frontier. Goto step1 \"Give me the fruitful error any time, full of seeds, bursting with its own corrections. You can keep your sterile truth for yourself.\" \u2015 Vilfredo Pareto \"For many events, roughly 80% of the effects come from 20% of the causes.\" \u2015 Vilfredo Pareto Domination measures: Binary Domination X domiantes Y if better on at least one, and worse on none. Returns a boolean. Widely used. Often stops distinguishing things after 3+ goals. Insenstive to the size of the domnation Indicator Domiantion From Zitler 2004 : X dominates Y if moving X to Y loses less than moving Y to X. To make the signal stronger, we shout the difference (raise it to some expendial power). To gauge the overall signal, we average across all goals To handle goal we want to minimize, maximize, we add a \" w \" constant (-1,1). Returns a number. Better at distinguishing higher problems with 3+ goals , see Table8. To rank rowX , I N=100 times (say), pick rowY at random. Q: Why not comapre to all rows? If indicatorDomiantes( X,Y ) then +1/N to rowX Then I add a new dom column to the data, wheich we want to maximize. see dom.html For example, here are some rows of auto.csv : %cylinders displacement horsepower weight acceltn model origin mpg 8 340:360 150:160 3609 8 70:70 1 10 6 198:225 83:86 2587 16 70:70 1 20 8 383:455 165:230 4425 10 70:70 1 10 8 302:305 120:140 3449 10.5 70:70 1 20 6 198:225 92:97 2833 15.5 70:70 1 20 8 383:455 165:230 3850 8.5 70:70 1 20 4 104:114 92:97 2372 15 70:70 3 20 4 104:114 92:97 2375 17.5 70:70 2 30 8 340:360 165:230 3693 11.5 70:70 1 20 .... From the first row, we see we want to minimize weight and maximize acceleration and mpg. From the other rows, we see that some discretizer has gotten to the displacement and horsepower values are replaced them with some string describing a range e.g. lo:hi = 340:360 . Here's the same data, with dom score added. Shown here are the 5 best and worst rows. %cylinders displacement horsepower weight acceltn model origin mpg dom 8 383:455 165:230 4746 12 71:71 1 10 0 8 383:455 165:230 4951 11 72:73 1 10 0 8 383:455 165:230 4952 11.5 72:73 1 10 0 8 383:455 165:230 4955 11.5 71:71 1 10 0 8 383:455 165:230 5140 12 71:71 1 10 0 8 383:455 165:230 4354 9 70:70 1 10 0.01 8 383:455 165:230 4425 10 70:70 1 10 0.01 8 383:455 165:230 4464 11.5 71:71 1 10 0.01 8 383:455 165:230 4735 11 72:73 1 10 0.01 8 383:455 165:230 4906 12.5 72:73 1 10 0.01 .. ... ... ... .. ... ... ... ... 4 85:91 69:72 2070 18.6 78:78 3 40 0.98 4 85:91 ? 1835 17.3 80:80 2 40 0.98 4 68:85 46:65 1825 18.6 77:77 2 40 0.99 4 68:85 69:72 1613 18 71:71 3 40 0.99 4 85:91 46:65 2335 23.7 80:80 2 40 0.99 4 85:91 46:65 1968 18.8 80:80 3 40 1.0 4 85:91 46:65 1975 19.4 81:81 3 40 1.0 4 85:91 46:65 1985 21.5 78:78 2 40 1.0 4 85:91 46:65 2085 21.7 80:80 2 40 1.0 4 96:97 46:65 2130 24.6 82:82 2 40 1.0 Observe that the highest dom scores are assocaited wiht rows with least weight, most acceleration and most mpg (and the lowest dom scores are associated with the reverse). The Lay of the Land The real story is that underneath surface features of all these problems is a common mathematical structure called, you guessed it, the landscape. To know the landscape is to know how to optimize, how to avoid getting stuck on being over-adapted (hence overspecialized) on some local peak, when as Stewart Brand so aptly puts it... \"Whole mountain ranges of opportunity could be glimpsed in the distance, but getting to them involved venturing 'downhill' into regions of lower fitness\". Studying such landscapes made Brand distrust claims for \"optimality\" since what you call \"optimum\" may actually be just a stepping zone to a better place. Brand's favorite landscape comes from a 1932 genetics paper that discusses how different breeding strategies respond well (or badly) to environmental pressures. In the following, the x-y axis might be \"length of hair\" and \"body weight\" and the z-axis might \"probability of winning a fight\". Says Brand: \"The first two illustrate how low selection pressure or a high rate of mutation (which comes with small populations) can broaden the range of a species whereas intense selection pressure or a low mutation rate can severely limit a species to the very peak of local fitness. The third diagram shows what happens when the landscape itself shifts, and the population has to evolve to shift with it.\" \"The bottom row explores how small populations respond to inbreeding by wandering ineffectively. The best mode of exploration Wright deemed the final diagram, showing how a species can divide into an array of races that interact with one another. That jostling crowd explores well, and it can respond to opportunity.\" Holes, poles, saddles, local minima, flat, brittle So to understand search, understand the landscape. If you know the landscape, you can see where it can trap and where it can help us out. One such trap is the saddle which, in the above diagram is the flat space between the mountain (called a pole ) and the hole next to it. Note that if walk along the saddle, you might think you are in a stable space of solutions. But be warned, one slip to the left or right and the landscape changes dramatically. In the above space you might fall into a hole or a pole. Another trap is the local minima that seems like a good idea but if you get sucked into it, you may never find the much better place: Another bad landscape is one that is completely flat. Try as you like, you walk along way around this one before finding anything better or worse: The opposite of flat is bumpy : Bumpy landscapes are common so Harman comments that understanding the neighborhood of our solutions is an open and pressing issue in search-based software engineering (SBSE): \"In some software engineering applications, solution robustness may be as im-portant as solution functionality. For example, it may be better to locate an area of the search space that is rich in fit solutions, rather than identifying an even better solution that is surrounded by a set of far less fit solutions.\" \"Hitherto, research on SBSE has tended to focus on the production of the fittest possible results. However, many application areas require solutions in a search space that may be subject to change. This makes robustness a natural second order property to which the research community could and should turn its attention.\" Bumpy landscapes mean that, sometimes, to achieve better goals you may have to first give up some of your current achievements. In the history of A.I. this is also called Sussmann's anomaly - that sometimes the way to \"better\" is via \"worse\". There are many ways to \"jump over\" these anomalies. Sussman (and his supervisor, Marvin Minsky) believed that intelligence requires an exolicit list of exceptions or tricks and that any plan for making things better had better be auditted by a \"debugging\" system. That is a knowledge-full approach that requires some analyst to supply descriptions of \"when to jump around\". Alternate knowledge-less approaches are: Stochastic jumps using, say, simulated annealing ; The retries mechanism discussed below; Momentum constants added to the mutators of stocahstic search. Such momentum constants resist sudden stops to the search: if a local maximum is reached, the momentum constant would push the inference a little further just to see if anything better lies beyond the current position. For example, in the following landscape, momentum would nudge the ball over that little lip into a better (and lower) world. Which is best: knowledge-full or knowledge-less? Well, that depends on the nature of the problem, the intrinsic value of the knowledge, etc etc. But the general engineering trade-off is that knowledge-less approaches are faster to build and maintain, while the knowledge-full approaches perform comparatively better. FYI- I used to work on knowledge-full approaches but I have found my life to be easier since I switched to knowledge-less . Walkting the Territory A.k.a. multi-objective domination","title":"Domination"},{"location":"lectures/domination/#domination","text":"Which house do you want: Lets do it again. Given Fig4 of this paper what options would you reject? Congraulations, you've just worked out how to do multi-objective optimization Build Popluation[ i=0 ] by making lots of guesses Experiment: (mutate, cross over) Select the best results (a.k.a know as find the Pareto frontier ) Set Population[ i+1 ] to the frontier. Goto step1 \"Give me the fruitful error any time, full of seeds, bursting with its own corrections. You can keep your sterile truth for yourself.\" \u2015 Vilfredo Pareto \"For many events, roughly 80% of the effects come from 20% of the causes.\" \u2015 Vilfredo Pareto","title":"Domination"},{"location":"lectures/domination/#domination-measures","text":"","title":"Domination measures:"},{"location":"lectures/domination/#binary-domination","text":"X domiantes Y if better on at least one, and worse on none. Returns a boolean. Widely used. Often stops distinguishing things after 3+ goals. Insenstive to the size of the domnation","title":"Binary Domination"},{"location":"lectures/domination/#indicator-domiantion","text":"From Zitler 2004 : X dominates Y if moving X to Y loses less than moving Y to X. To make the signal stronger, we shout the difference (raise it to some expendial power). To gauge the overall signal, we average across all goals To handle goal we want to minimize, maximize, we add a \" w \" constant (-1,1). Returns a number. Better at distinguishing higher problems with 3+ goals , see Table8. To rank rowX , I N=100 times (say), pick rowY at random. Q: Why not comapre to all rows? If indicatorDomiantes( X,Y ) then +1/N to rowX Then I add a new dom column to the data, wheich we want to maximize. see dom.html For example, here are some rows of auto.csv : %cylinders displacement horsepower weight acceltn model origin mpg 8 340:360 150:160 3609 8 70:70 1 10 6 198:225 83:86 2587 16 70:70 1 20 8 383:455 165:230 4425 10 70:70 1 10 8 302:305 120:140 3449 10.5 70:70 1 20 6 198:225 92:97 2833 15.5 70:70 1 20 8 383:455 165:230 3850 8.5 70:70 1 20 4 104:114 92:97 2372 15 70:70 3 20 4 104:114 92:97 2375 17.5 70:70 2 30 8 340:360 165:230 3693 11.5 70:70 1 20 .... From the first row, we see we want to minimize weight and maximize acceleration and mpg. From the other rows, we see that some discretizer has gotten to the displacement and horsepower values are replaced them with some string describing a range e.g. lo:hi = 340:360 . Here's the same data, with dom score added. Shown here are the 5 best and worst rows. %cylinders displacement horsepower weight acceltn model origin mpg dom 8 383:455 165:230 4746 12 71:71 1 10 0 8 383:455 165:230 4951 11 72:73 1 10 0 8 383:455 165:230 4952 11.5 72:73 1 10 0 8 383:455 165:230 4955 11.5 71:71 1 10 0 8 383:455 165:230 5140 12 71:71 1 10 0 8 383:455 165:230 4354 9 70:70 1 10 0.01 8 383:455 165:230 4425 10 70:70 1 10 0.01 8 383:455 165:230 4464 11.5 71:71 1 10 0.01 8 383:455 165:230 4735 11 72:73 1 10 0.01 8 383:455 165:230 4906 12.5 72:73 1 10 0.01 .. ... ... ... .. ... ... ... ... 4 85:91 69:72 2070 18.6 78:78 3 40 0.98 4 85:91 ? 1835 17.3 80:80 2 40 0.98 4 68:85 46:65 1825 18.6 77:77 2 40 0.99 4 68:85 69:72 1613 18 71:71 3 40 0.99 4 85:91 46:65 2335 23.7 80:80 2 40 0.99 4 85:91 46:65 1968 18.8 80:80 3 40 1.0 4 85:91 46:65 1975 19.4 81:81 3 40 1.0 4 85:91 46:65 1985 21.5 78:78 2 40 1.0 4 85:91 46:65 2085 21.7 80:80 2 40 1.0 4 96:97 46:65 2130 24.6 82:82 2 40 1.0 Observe that the highest dom scores are assocaited wiht rows with least weight, most acceleration and most mpg (and the lowest dom scores are associated with the reverse).","title":"Indicator Domiantion"},{"location":"lectures/domination/#the-lay-of-the-land","text":"The real story is that underneath surface features of all these problems is a common mathematical structure called, you guessed it, the landscape. To know the landscape is to know how to optimize, how to avoid getting stuck on being over-adapted (hence overspecialized) on some local peak, when as Stewart Brand so aptly puts it... \"Whole mountain ranges of opportunity could be glimpsed in the distance, but getting to them involved venturing 'downhill' into regions of lower fitness\". Studying such landscapes made Brand distrust claims for \"optimality\" since what you call \"optimum\" may actually be just a stepping zone to a better place. Brand's favorite landscape comes from a 1932 genetics paper that discusses how different breeding strategies respond well (or badly) to environmental pressures. In the following, the x-y axis might be \"length of hair\" and \"body weight\" and the z-axis might \"probability of winning a fight\". Says Brand: \"The first two illustrate how low selection pressure or a high rate of mutation (which comes with small populations) can broaden the range of a species whereas intense selection pressure or a low mutation rate can severely limit a species to the very peak of local fitness. The third diagram shows what happens when the landscape itself shifts, and the population has to evolve to shift with it.\" \"The bottom row explores how small populations respond to inbreeding by wandering ineffectively. The best mode of exploration Wright deemed the final diagram, showing how a species can divide into an array of races that interact with one another. That jostling crowd explores well, and it can respond to opportunity.\"","title":"The Lay of the Land"},{"location":"lectures/domination/#holes-poles-saddles-local-minima-flat-brittle","text":"So to understand search, understand the landscape. If you know the landscape, you can see where it can trap and where it can help us out. One such trap is the saddle which, in the above diagram is the flat space between the mountain (called a pole ) and the hole next to it. Note that if walk along the saddle, you might think you are in a stable space of solutions. But be warned, one slip to the left or right and the landscape changes dramatically. In the above space you might fall into a hole or a pole. Another trap is the local minima that seems like a good idea but if you get sucked into it, you may never find the much better place: Another bad landscape is one that is completely flat. Try as you like, you walk along way around this one before finding anything better or worse: The opposite of flat is bumpy : Bumpy landscapes are common so Harman comments that understanding the neighborhood of our solutions is an open and pressing issue in search-based software engineering (SBSE): \"In some software engineering applications, solution robustness may be as im-portant as solution functionality. For example, it may be better to locate an area of the search space that is rich in fit solutions, rather than identifying an even better solution that is surrounded by a set of far less fit solutions.\" \"Hitherto, research on SBSE has tended to focus on the production of the fittest possible results. However, many application areas require solutions in a search space that may be subject to change. This makes robustness a natural second order property to which the research community could and should turn its attention.\" Bumpy landscapes mean that, sometimes, to achieve better goals you may have to first give up some of your current achievements. In the history of A.I. this is also called Sussmann's anomaly - that sometimes the way to \"better\" is via \"worse\". There are many ways to \"jump over\" these anomalies. Sussman (and his supervisor, Marvin Minsky) believed that intelligence requires an exolicit list of exceptions or tricks and that any plan for making things better had better be auditted by a \"debugging\" system. That is a knowledge-full approach that requires some analyst to supply descriptions of \"when to jump around\". Alternate knowledge-less approaches are: Stochastic jumps using, say, simulated annealing ; The retries mechanism discussed below; Momentum constants added to the mutators of stocahstic search. Such momentum constants resist sudden stops to the search: if a local maximum is reached, the momentum constant would push the inference a little further just to see if anything better lies beyond the current position. For example, in the following landscape, momentum would nudge the ball over that little lip into a better (and lower) world. Which is best: knowledge-full or knowledge-less? Well, that depends on the nature of the problem, the intrinsic value of the knowledge, etc etc. But the general engineering trade-off is that knowledge-less approaches are faster to build and maintain, while the knowledge-full approaches perform comparatively better. FYI- I used to work on knowledge-full approaches but I have found my life to be easier since I switched to knowledge-less .","title":"Holes, poles, saddles, local minima, flat, brittle"},{"location":"lectures/domination/#walkting-the-territory","text":"A.k.a. multi-objective domination","title":"Walkting the Territory"},{"location":"lectures/dt101/","text":"Decision Trees 101 Before lookin at those, your going to need to know: Decision trees are recursive diversity reduction algorithms Find the split that most reduces diversity Recurse on each split. Stop when (e.g.) too few examples in each split. RandomForests says \"if one tree is good, why not build 100?\" each time, grab (say) log(N) of the attributes and some percent of the rows build N trees make a conclusion by voting across the forest Example1 let us measure diversity using standard deviaton sqrt(( square(x - ))/(n-1)) standard deviation of 9,2,5,4,12,7 has = 6.5 and =3.619. Learners like CART and M5prime and Random Forest Regressorts used standard deviation. Why? Cause these learners predict for numeric class variables . Example2: let us measure diversity using entropy ; i.e. -1 p log2(p) e.g. 1 orange, 1 apple, 2 bananas, and 4 grapes occur at probability 1/8, 1/8, 1/4, and 1/2 8 =2*2*2 so log2( 1/8 ) = -3 4 =2*2 so log2( 1/4 ) = -2 2 =2 so log2( 1/2 ) = -1 what is entropy of (o,a,b,b,g,g,g,g) -1 * (1/8*-3 + 1/8*-3 + 1/4*-2 + 1/2*-1) = -1 * (1/8*-3 + 1/8*-3 + 2/8*-2 + 4/8*-1) = -1/8 * (-6 + -4 + -4) = 14/8 = 1.75 Learners like decision trees and random forests use entropy Why? Cause these learners predict for symbolic class variables . Example3: What is the best split for this data? Here are the options (note that this is four different splits): Consider the outlook tree. We have three sub-branches so the expected value of the diversity after the split is 5/14 * entropy of sunny split 4/14 * entropy of overcast split 5/14 * entropy of rainy split The overcast split is easy: we only have yes so p(yes) = 1 and log2(p) = 0 and 4/14 is zero The sunny and rainy split are symmetric sunny: 2 yes and 3 no = -1 * (2/5 * log2(2/5) + 3/5 * log2(3/5)) = 0.97 raning: 3 yes and 2 no = same entropy as sunny So the expected value after the outlook split is (5/14 * 0.97) + (4/14 * 0) + (5/14 * 0.97) = 0.69 (BTW, this is an improvement since before the split we have 9 yes, 5 no; ie. entropy was 0.94; i.e. more diversity). If we repeat this calc over all splits, we get outlook split: 0.69 temperate split: 0.91 = humidity split: 0.79 windy split: 0.89 So we would split on outlook.","title":"Decision trees 101 &#10004"},{"location":"lectures/dt101/#decision-trees-101","text":"Before lookin at those, your going to need to know: Decision trees are recursive diversity reduction algorithms Find the split that most reduces diversity Recurse on each split. Stop when (e.g.) too few examples in each split. RandomForests says \"if one tree is good, why not build 100?\" each time, grab (say) log(N) of the attributes and some percent of the rows build N trees make a conclusion by voting across the forest","title":"Decision Trees 101"},{"location":"lectures/dt101/#example1","text":"let us measure diversity using standard deviaton sqrt(( square(x - ))/(n-1)) standard deviation of 9,2,5,4,12,7 has = 6.5 and =3.619. Learners like CART and M5prime and Random Forest Regressorts used standard deviation. Why? Cause these learners predict for numeric class variables .","title":"Example1"},{"location":"lectures/dt101/#example2","text":"let us measure diversity using entropy ; i.e. -1 p log2(p) e.g. 1 orange, 1 apple, 2 bananas, and 4 grapes occur at probability 1/8, 1/8, 1/4, and 1/2 8 =2*2*2 so log2( 1/8 ) = -3 4 =2*2 so log2( 1/4 ) = -2 2 =2 so log2( 1/2 ) = -1 what is entropy of (o,a,b,b,g,g,g,g) -1 * (1/8*-3 + 1/8*-3 + 1/4*-2 + 1/2*-1) = -1 * (1/8*-3 + 1/8*-3 + 2/8*-2 + 4/8*-1) = -1/8 * (-6 + -4 + -4) = 14/8 = 1.75 Learners like decision trees and random forests use entropy Why? Cause these learners predict for symbolic class variables .","title":"Example2:"},{"location":"lectures/dt101/#example3","text":"What is the best split for this data? Here are the options (note that this is four different splits): Consider the outlook tree. We have three sub-branches so the expected value of the diversity after the split is 5/14 * entropy of sunny split 4/14 * entropy of overcast split 5/14 * entropy of rainy split The overcast split is easy: we only have yes so p(yes) = 1 and log2(p) = 0 and 4/14 is zero The sunny and rainy split are symmetric sunny: 2 yes and 3 no = -1 * (2/5 * log2(2/5) + 3/5 * log2(3/5)) = 0.97 raning: 3 yes and 2 no = same entropy as sunny So the expected value after the outlook split is (5/14 * 0.97) + (4/14 * 0) + (5/14 * 0.97) = 0.69 (BTW, this is an improvement since before the split we have 9 yes, 5 no; ie. entropy was 0.94; i.e. more diversity). If we repeat this calc over all splits, we get outlook split: 0.69 temperate split: 0.91 = humidity split: 0.79 windy split: 0.89 So we would split on outlook.","title":"Example3:"},{"location":"lectures/eval/","text":"Evaluation XXX must show SA trueth isnt what we thought. t tif a example depends of goals onwhat we are willing to comprosmise Re-run on Multiple Samples e.g. cross-val Divide into \" x bins Test on one bin, train on the others Runs the risk of using future data to train for testing on the past Using combined with some stochastic re-orderings So \" M \" times, randomly rarrange order of data Then do an \" N \"-way cross val for each order Avoids \"order effects\" where the results are some quirky result based on the order of data colelction/generation. M=N=10 is common but I've never seen the point for more than M=N=5. e.g. round robin Given N projects Train on N-1, test on the nth. e.g Github issue close time, Table4 e.g. incremental validation. Divide into \" x \" buckets, Train on buckets 1..i, test on i+1 http://www.cs.le.ac.uk/people/llm11/publications/MinkuYaoICSE14.pdf e.g. moving validation. Divide into \" x \" buckets, learn on buckets i..i+n, test on i+n+1. eg. Krishna's K-test. e.g. RRS (repeated random streaming) e.g. repeatedly stream over the data, each time using n% of the data selected at random Q: What \"n\"? A: Engineering judgement BTW, Beyond \"Evaluation\" Evaluation can be so tedious and time-consuming that many researchers have asked if all that inference can be applied to improving the model: So \"evaluation\" becomes \"improvement\" or \"monitor and repair\" Cross val to ensembles to bagging to boosting Round robin to transfer learning Anomaly detection to repair Incremental learning SAWTOOTH: Dumb as all hell Active learning uncertainty sampling certainty sampling Bayesian Parameter optimization (widely used) FLASH","title":"Evaluation"},{"location":"lectures/eval/#evaluation","text":"XXX must show SA trueth isnt what we thought. t tif a example depends of goals onwhat we are willing to comprosmise","title":"Evaluation"},{"location":"lectures/eval/#re-run-on-multiple-samples","text":"","title":"Re-run on Multiple Samples"},{"location":"lectures/eval/#eg-cross-val","text":"Divide into \" x bins Test on one bin, train on the others Runs the risk of using future data to train for testing on the past Using combined with some stochastic re-orderings So \" M \" times, randomly rarrange order of data Then do an \" N \"-way cross val for each order Avoids \"order effects\" where the results are some quirky result based on the order of data colelction/generation. M=N=10 is common but I've never seen the point for more than M=N=5.","title":"e.g. cross-val"},{"location":"lectures/eval/#eg-round-robin","text":"Given N projects Train on N-1, test on the nth. e.g Github issue close time, Table4","title":"e.g. round robin"},{"location":"lectures/eval/#eg-incremental-validation","text":"Divide into \" x \" buckets, Train on buckets 1..i, test on i+1 http://www.cs.le.ac.uk/people/llm11/publications/MinkuYaoICSE14.pdf","title":"e.g. incremental validation."},{"location":"lectures/eval/#eg-moving-validation","text":"Divide into \" x \" buckets, learn on buckets i..i+n, test on i+n+1. eg. Krishna's K-test.","title":"e.g. moving validation."},{"location":"lectures/eval/#eg-rrs-repeated-random-streaming","text":"e.g. repeatedly stream over the data, each time using n% of the data selected at random Q: What \"n\"? A: Engineering judgement","title":"e.g. RRS (repeated random streaming)"},{"location":"lectures/eval/#btw-beyond-evaluation","text":"Evaluation can be so tedious and time-consuming that many researchers have asked if all that inference can be applied to improving the model: So \"evaluation\" becomes \"improvement\" or \"monitor and repair\" Cross val to ensembles to bagging to boosting Round robin to transfer learning Anomaly detection to repair","title":"BTW, Beyond \"Evaluation\""},{"location":"lectures/eval/#incremental-learning","text":"SAWTOOTH: Dumb as all hell Active learning uncertainty sampling certainty sampling Bayesian Parameter optimization (widely used) FLASH","title":"Incremental learning"},{"location":"lectures/explain/","text":"Explain from the fse fft paper from the swan paper from the EMSE paper ICML Workshop on Human Interpretability in Machine Learning","title":"Explanation"},{"location":"lectures/explain/#explain","text":"from the fse fft paper from the swan paper from the EMSE paper ICML Workshop on Human Interpretability in Machine Learning","title":"Explain"},{"location":"lectures/simple/","text":"Simpler. Please. Enough Inference, But Not Too Much \"As complexity rises, precise statements lose meaning and meaningful statements lose precision.\" -- Lofti Zadeh \"Simplicity is the ultimate form of sophistication.\" -- Leonardo da Vinci \"No! No! No!\" -- Business user running in fear from overly complex maths \"Less, But Better\" -- Dieter Rams Are our AI tools designed \"good\"? Based on my experience with industrial developers, I would say perhaps not. According to Dieter Rams, good design: is innovative \u2013 The possibilities for progression are not, by any means, exhausted. Technological development is always offering new opportunities for original designs. But imaginative design always develops in tandem with improving technology, and can never be an end in itself. makes a product useful \u2013 A product is bought to be used. It has to satisfy not only functional, but also psychological and aesthetic criteria. Good design emphasizes the usefulness of a product whilst disregarding anything that could detract from it. is aesthetic \u2013 The aesthetic quality of a product is integral to its usefulness because products are used every day and have an effect on people and their well-being. Only well-executed objects can be beautiful. makes a product understandable \u2013 It clarifies the product\u2019s structure. Better still, it can make the product clearly express its function by making use of the user's intuition. At best, it is self-explanatory. is unobtrusive \u2013 Products fulfilling a purpose are like tools. They are neither decorative objects nor works of art. Their design should therefore be both neutral and restrained, to leave room for the user's self-expression. is honest \u2013 It does not make a product appear more innovative, powerful or valuable than it really is. It does not attempt to manipulate the consumer with promises that cannot be kept. is long-lasting \u2013 It avoids being fashionable and therefore never appears antiquated. Unlike fashionable design, it lasts many years \u2013 even in today's throw away society. is thorough down to the last detail \u2013 Nothing must be arbitrary or left to chance. Care and accuracy in the design process show respect towards the consumer. is environmentally friendly \u2013 Design makes an important contribution to the preservation of the environment. It conserves resources and minimizes physical and visual pollution throughout the life cycle of the product. is as little design as possible \u2013 Less, but better \u2013 because it concentrates on the essential aspects, and the products are not burdened with non-essentials. Back to purity, back to simplicity. The Obvious Counter-argument Against \"Keep it Simple\" Yes, sometimes, complexity is necessary e.g. 2% optimizations of turbulent non-linear air-flows across a wing in the transonic range But when it ain\u2019t Needless complexity = is just silliness Much industrial success with very complex image processing based on deep learners that derive fascinating internal features uses layers of neural nets All good stuff For \"large problems\", ever increasing exact inference is... silly. Example of \"large \": Many SE inference results a large i.e. small changes to the training data or the AI model leads to large changes in the performance. 77 \"equals\" 81 when variance is large. And for SE, its often large. For example, here are 10*3 cross-val software effort estimation results (10 times: randomize order of data, divide into three bins; train on 2, test on the other): And there are many reasons to reflect on how not to do \"it\" simpler. Reasons for less Your next 15 weeks Fewer headaches Higher marks We would be foolish not to exploit inherent simplicities. Because at least in SE, there are inherent simpliticies Why is this so? Not clear. But: Mathematically, models are either simple or poorly supported by the data: See here Programming languages are a subset of natural language and both kinds of languages exhibit remarkably simple \"language models\" ; i.e. given the last N symbols, you can pretty much guess the next symbol. Hindle et al. : \"Programming languages, in theory, are complex, flexible and powerful, but the programs that real people actually write are mostly simple and rather repetitive, and thus they have usefully predictable statistical proper- ties that can be captured in statistical language models and leveraged for software engineering tasks.\" Because in SE, More Complex is often superfluous Data from Norman Fenton\u2019s Bayes nets discussing software defects = yes, no Given classes x,y then Fx, Fy is frequency of some ranges in x,y Log Odds Ratio = log(Fx/Fy ) If zero if no difference in x,y The secret of big data is small data . The best thing to do with data is to ignore most of it. Results: Most variables do not contribute to determination of defects Data from Papakroni 's masters thesis TRAIN: Project 21 features onto first 2 components of PCA Recursively divide two dimensions (at median) Stopping a SQRT(N) In each leaf, replace N projects with median centroid TEST: Estimate = interpolate 2 near centroids Performs no worse, and sometimes better, than Random forests, NaiveBayes For more, see Data Mining for very busy people Because, Historically, Simpler is often Better PCA, 1901 Narrows: Amarel 1960s Prototypes: Chen 1975 Frames: Minsky, 1975 Min environments: DeKleer, 1986 Saturation: Horgan Mathur: 1980 Homogeneous propagation: Michael: 1981 Master variables: Crawford Baker, 1995 Clumps, Druzdel, 1997 Feature subset section, Kohavi, 1997, Back doors, Williams, 2002 Active learning: many people (2000+) From Section 2.2 of this paper Simpler methods may build smaller models that use fewer attributes from the data. Such models are more likely to be robust against overfitting, especially on small and noisy data (and have been found to predict data at levels comparable with regression). Very simple rule-based methpds can perform comparably well to more complex models in a range of domains e.g., public health, medical risk management, performance sci- ence, etc. Neth and Gigerenzer argue that such rule-bases are tools that work well under conditions of uncertainty. Brighton showed that rule-based models can perform better than complex nonlinear algorithms such as neural networks, exemplar models, and classification/regression trees. Speed speed speed Analytics = work flow to condense much low-value data down to a few diamonds. Fisher et al. survey of 16 industrial data scientists, Due to computational cost of analytics, \"the luxuries of interactivity, direct manipulation, and fast system response are gone\". Modern cloud-based analytics as a throwback to the 1960s\u2013 batch processing \"Fast iteration is key, but incompatible with the way jobs are submitted and processed in the cloud. It\u2019s frustrating to wait for hours, only to realize you need a slight tweak to your feature set.\" Less cost (local hardware, cloud services) The following are somewhat extreme examples. But suppose we could do the following tasks orders of magnitude faster. Just imagine what else could we use all that saved CPU for? 15 years of CPU, one FSE 2013 analytics paper, Wang et al/ Evaluate automatic tuning tools for code clone recognition tools $1.2million Cost or renting of commercial cloud hardware One project, 5 students, 3 years FYI: that would EAT and SWALLOW the standard budgets NSF gives researchers like me Three years of CPU/day Anonymous NCSU grad student tuning deep learning networks at XXXX for summer 2018 Support the edge Edge computing Move some computational towards the edge of the network to harness computational capabilities that are currently untapped in edge nodes, such as base stations, routers and switches Internet of Things Simple AI could make better use of billions and billions of low power devices, many of which are operating at very low power Less Energy Consumption Power off your phone Stare at it How long before you stop? Phone - power = silly From Green in Software Engineering : Present-day primary energy footprints of three business software apps add up to as much as 373 Peta Joules/ year. People claim they will pay more for a \"green\" product. nearly half of surveyed organizations design their business models on the basis of sustainability. Sustainability = new source of innovation, cost-cutting opportunity , mechanism for gaining competitive advantage. Less pollution Creating that Energy IT-related services now account for 2% of all global carbon emissions-- roughly the same as the aviation sector Simpler AI lets us breathe easier. The cloud runs on http://time.com/46777/your-data-is-dirty-the-carbon-price-of-cloud-computing/ . Simpler explanation Less generation of solutions Less confusion Verrappa and Letier : \"..for industrial problems, these algorithms generate (many) solutions (makes) understanding them and selecting one among them difficult and time consuming\" Simpler Customization We are already delivering software more complex than what people can manage (see fig1 and fig3). Many software systems have poorly chosen defaults. Hence, it is useful to seek better configurations. Van Aken et al. report that the default MySQL configurations in 2016 assume that it will be installed on a machine that has 160MB of RAM (which, at that time, was incorrect by, at least, an order of magnitude) Herodotou et al. show how standard settings for text mining appli- cations in Hadoop result in worst-case execution times. In the same vein, Jamshidi et al. reports for text mining applications on Apache Storm, the throughput achieved using the worst configuration is 480 times slower than the throughput achieved by the best configuration. Understanding, the configuration space of software systems with large configuration space, is challenging. Exploring more than just a handful of configurations is usually infeasible due to long benchmarking time Because we need a baseline Because better science needs better baselines: Empirical methods in AI : Supposedly newer more sophisticated methods should be baselined against a seemingly simpler alternative Warning: when I do that, I often find simpler is better. Search-based SE: A Baseline Method For Search-Based Software Engineering Data mining: Bellwethers: A Baseline Method For Transfer Learning Optimizing: \"Sampling\"' as a Baseline Optimizer for Search-based Software Engineering Because better engineering needs better baselines: So many AI tools, so many ways to tune them, so many ways to combine them So given a new problem, which learner/optimizer should we apply? Hard to say, apriori. When new data arrives, you need commissioning experiments ; i.e. try a variety of techniques before you can find what words best for the local data. Comissioning needs baselines i.e. an algorithm which can generate floor performance values. Lets a developer quickly rule out any method that falls \u201cbelow the floor\u201d. Lets us achieve fast early results, while also gaining some guidance in all their subsequent experimentation (specifically: \"try to beat the baseline\"). Other Quicker more effective training, experimentation Less to understand, faster to understand. Less to twiddle, less to explore Easier Reproducibility Large config spaces are the bane of reproducibility. You did not get what I got? Di you set the X to 2, Y to 3, and Z to 10,30,age/shoe size+temperature? Solutions more trust-able More understandable, more explainable, more I want to use. Solutions easier to apply Less to do, faster to do Cause its just good science If we do not know what we are doing, we are very clumsy; When we understand it better, we are more elegant; So use this test to check if your thinking is confused, or clarified: are your current methods very complex/ time-consuming? A Common Recipe (For Me, at Least) Row reduction: Replace N rows of data with M N exemplars e.g. cluster and report just some items per cluster Column reduction: Prune C columns of data with the sqrt(C) most influential columns Range reduction (in columns): Discretize numerics, but only cut into ranges in sub-ranges have less variance Range pruning If there is a class variable: only cut if the associated class ranges are less varied Else, cluster row/column/range data and only cut if associated ranges are less varied in different clusters. Cautions So that\u2019s it? Just find the few dimensions that matter, then stop?o Well... The reduced data space exists... but how to find it? Feature selection on raw data? Or may be synthesized (PCA, spectral, etc) Also: And the dimensions that matter NOW May not matter SOON So an agent that assumes low dimensionality (but fyi, we should be doing that with all data mining approaches, anyway). Will always be checking and revising their dimensional","title":"Simplicity ;"},{"location":"lectures/simple/#simpler-please","text":"","title":"Simpler. Please."},{"location":"lectures/simple/#enough-inference-but-not-too-much","text":"\"As complexity rises, precise statements lose meaning and meaningful statements lose precision.\" -- Lofti Zadeh \"Simplicity is the ultimate form of sophistication.\" -- Leonardo da Vinci \"No! No! No!\" -- Business user running in fear from overly complex maths \"Less, But Better\" -- Dieter Rams Are our AI tools designed \"good\"? Based on my experience with industrial developers, I would say perhaps not. According to Dieter Rams, good design: is innovative \u2013 The possibilities for progression are not, by any means, exhausted. Technological development is always offering new opportunities for original designs. But imaginative design always develops in tandem with improving technology, and can never be an end in itself. makes a product useful \u2013 A product is bought to be used. It has to satisfy not only functional, but also psychological and aesthetic criteria. Good design emphasizes the usefulness of a product whilst disregarding anything that could detract from it. is aesthetic \u2013 The aesthetic quality of a product is integral to its usefulness because products are used every day and have an effect on people and their well-being. Only well-executed objects can be beautiful. makes a product understandable \u2013 It clarifies the product\u2019s structure. Better still, it can make the product clearly express its function by making use of the user's intuition. At best, it is self-explanatory. is unobtrusive \u2013 Products fulfilling a purpose are like tools. They are neither decorative objects nor works of art. Their design should therefore be both neutral and restrained, to leave room for the user's self-expression. is honest \u2013 It does not make a product appear more innovative, powerful or valuable than it really is. It does not attempt to manipulate the consumer with promises that cannot be kept. is long-lasting \u2013 It avoids being fashionable and therefore never appears antiquated. Unlike fashionable design, it lasts many years \u2013 even in today's throw away society. is thorough down to the last detail \u2013 Nothing must be arbitrary or left to chance. Care and accuracy in the design process show respect towards the consumer. is environmentally friendly \u2013 Design makes an important contribution to the preservation of the environment. It conserves resources and minimizes physical and visual pollution throughout the life cycle of the product. is as little design as possible \u2013 Less, but better \u2013 because it concentrates on the essential aspects, and the products are not burdened with non-essentials. Back to purity, back to simplicity.","title":"Enough Inference, But Not Too Much"},{"location":"lectures/simple/#the-obvious-counter-argument-against-keep-it-simple","text":"Yes, sometimes, complexity is necessary e.g. 2% optimizations of turbulent non-linear air-flows across a wing in the transonic range But when it ain\u2019t Needless complexity = is just silliness Much industrial success with very complex image processing based on deep learners that derive fascinating internal features uses layers of neural nets All good stuff For \"large problems\", ever increasing exact inference is... silly. Example of \"large \": Many SE inference results a large i.e. small changes to the training data or the AI model leads to large changes in the performance. 77 \"equals\" 81 when variance is large. And for SE, its often large. For example, here are 10*3 cross-val software effort estimation results (10 times: randomize order of data, divide into three bins; train on 2, test on the other): And there are many reasons to reflect on how not to do \"it\" simpler.","title":"The Obvious Counter-argument Against \"Keep it Simple\""},{"location":"lectures/simple/#reasons-for-less","text":"","title":"Reasons for less"},{"location":"lectures/simple/#your-next-15-weeks","text":"Fewer headaches Higher marks We would be foolish not to exploit inherent simplicities.","title":"Your next 15 weeks"},{"location":"lectures/simple/#because-at-least-in-se-there-are-inherent-simpliticies","text":"Why is this so? Not clear. But: Mathematically, models are either simple or poorly supported by the data: See here Programming languages are a subset of natural language and both kinds of languages exhibit remarkably simple \"language models\" ; i.e. given the last N symbols, you can pretty much guess the next symbol. Hindle et al. : \"Programming languages, in theory, are complex, flexible and powerful, but the programs that real people actually write are mostly simple and rather repetitive, and thus they have usefully predictable statistical proper- ties that can be captured in statistical language models and leveraged for software engineering tasks.\"","title":"Because at least in SE, there are  inherent simpliticies"},{"location":"lectures/simple/#because-in-se-more-complex-is-often-superfluous","text":"Data from Norman Fenton\u2019s Bayes nets discussing software defects = yes, no Given classes x,y then Fx, Fy is frequency of some ranges in x,y Log Odds Ratio = log(Fx/Fy ) If zero if no difference in x,y The secret of big data is small data . The best thing to do with data is to ignore most of it. Results: Most variables do not contribute to determination of defects Data from Papakroni 's masters thesis TRAIN: Project 21 features onto first 2 components of PCA Recursively divide two dimensions (at median) Stopping a SQRT(N) In each leaf, replace N projects with median centroid TEST: Estimate = interpolate 2 near centroids Performs no worse, and sometimes better, than Random forests, NaiveBayes For more, see Data Mining for very busy people","title":"Because in SE, More Complex is often superfluous"},{"location":"lectures/simple/#because-historically-simpler-is-often-better","text":"PCA, 1901 Narrows: Amarel 1960s Prototypes: Chen 1975 Frames: Minsky, 1975 Min environments: DeKleer, 1986 Saturation: Horgan Mathur: 1980 Homogeneous propagation: Michael: 1981 Master variables: Crawford Baker, 1995 Clumps, Druzdel, 1997 Feature subset section, Kohavi, 1997, Back doors, Williams, 2002 Active learning: many people (2000+) From Section 2.2 of this paper Simpler methods may build smaller models that use fewer attributes from the data. Such models are more likely to be robust against overfitting, especially on small and noisy data (and have been found to predict data at levels comparable with regression). Very simple rule-based methpds can perform comparably well to more complex models in a range of domains e.g., public health, medical risk management, performance sci- ence, etc. Neth and Gigerenzer argue that such rule-bases are tools that work well under conditions of uncertainty. Brighton showed that rule-based models can perform better than complex nonlinear algorithms such as neural networks, exemplar models, and classification/regression trees.","title":"Because, Historically, Simpler is often Better"},{"location":"lectures/simple/#speed-speed-speed","text":"Analytics = work flow to condense much low-value data down to a few diamonds. Fisher et al. survey of 16 industrial data scientists, Due to computational cost of analytics, \"the luxuries of interactivity, direct manipulation, and fast system response are gone\". Modern cloud-based analytics as a throwback to the 1960s\u2013 batch processing \"Fast iteration is key, but incompatible with the way jobs are submitted and processed in the cloud. It\u2019s frustrating to wait for hours, only to realize you need a slight tweak to your feature set.\"","title":"Speed speed speed"},{"location":"lectures/simple/#less-cost-local-hardware-cloud-services","text":"The following are somewhat extreme examples. But suppose we could do the following tasks orders of magnitude faster. Just imagine what else could we use all that saved CPU for? 15 years of CPU, one FSE 2013 analytics paper, Wang et al/ Evaluate automatic tuning tools for code clone recognition tools $1.2million Cost or renting of commercial cloud hardware One project, 5 students, 3 years FYI: that would EAT and SWALLOW the standard budgets NSF gives researchers like me Three years of CPU/day Anonymous NCSU grad student tuning deep learning networks at XXXX for summer 2018","title":"Less cost (local hardware, cloud services)"},{"location":"lectures/simple/#support-the-edge","text":"Edge computing Move some computational towards the edge of the network to harness computational capabilities that are currently untapped in edge nodes, such as base stations, routers and switches Internet of Things Simple AI could make better use of billions and billions of low power devices, many of which are operating at very low power","title":"Support the edge"},{"location":"lectures/simple/#less-energy-consumption","text":"Power off your phone Stare at it How long before you stop? Phone - power = silly From Green in Software Engineering : Present-day primary energy footprints of three business software apps add up to as much as 373 Peta Joules/ year. People claim they will pay more for a \"green\" product. nearly half of surveyed organizations design their business models on the basis of sustainability. Sustainability = new source of innovation, cost-cutting opportunity , mechanism for gaining competitive advantage.","title":"Less Energy Consumption"},{"location":"lectures/simple/#less-pollution-creating-that-energy","text":"IT-related services now account for 2% of all global carbon emissions-- roughly the same as the aviation sector Simpler AI lets us breathe easier. The cloud runs on http://time.com/46777/your-data-is-dirty-the-carbon-price-of-cloud-computing/ .","title":"Less pollution Creating that Energy"},{"location":"lectures/simple/#simpler-explanation","text":"Less generation of solutions Less confusion Verrappa and Letier : \"..for industrial problems, these algorithms generate (many) solutions (makes) understanding them and selecting one among them difficult and time consuming\"","title":"Simpler explanation"},{"location":"lectures/simple/#simpler-customization","text":"We are already delivering software more complex than what people can manage (see fig1 and fig3). Many software systems have poorly chosen defaults. Hence, it is useful to seek better configurations. Van Aken et al. report that the default MySQL configurations in 2016 assume that it will be installed on a machine that has 160MB of RAM (which, at that time, was incorrect by, at least, an order of magnitude) Herodotou et al. show how standard settings for text mining appli- cations in Hadoop result in worst-case execution times. In the same vein, Jamshidi et al. reports for text mining applications on Apache Storm, the throughput achieved using the worst configuration is 480 times slower than the throughput achieved by the best configuration. Understanding, the configuration space of software systems with large configuration space, is challenging. Exploring more than just a handful of configurations is usually infeasible due to long benchmarking time","title":"Simpler Customization"},{"location":"lectures/simple/#because-we-need-a-baseline","text":"Because better science needs better baselines: Empirical methods in AI : Supposedly newer more sophisticated methods should be baselined against a seemingly simpler alternative Warning: when I do that, I often find simpler is better. Search-based SE: A Baseline Method For Search-Based Software Engineering Data mining: Bellwethers: A Baseline Method For Transfer Learning Optimizing: \"Sampling\"' as a Baseline Optimizer for Search-based Software Engineering Because better engineering needs better baselines: So many AI tools, so many ways to tune them, so many ways to combine them So given a new problem, which learner/optimizer should we apply? Hard to say, apriori. When new data arrives, you need commissioning experiments ; i.e. try a variety of techniques before you can find what words best for the local data. Comissioning needs baselines i.e. an algorithm which can generate floor performance values. Lets a developer quickly rule out any method that falls \u201cbelow the floor\u201d. Lets us achieve fast early results, while also gaining some guidance in all their subsequent experimentation (specifically: \"try to beat the baseline\").","title":"Because we need a baseline"},{"location":"lectures/simple/#other","text":"Quicker more effective training, experimentation Less to understand, faster to understand. Less to twiddle, less to explore Easier Reproducibility Large config spaces are the bane of reproducibility. You did not get what I got? Di you set the X to 2, Y to 3, and Z to 10,30,age/shoe size+temperature? Solutions more trust-able More understandable, more explainable, more I want to use. Solutions easier to apply Less to do, faster to do Cause its just good science If we do not know what we are doing, we are very clumsy; When we understand it better, we are more elegant; So use this test to check if your thinking is confused, or clarified: are your current methods very complex/ time-consuming?","title":"Other"},{"location":"lectures/simple/#a-common-recipe-for-me-at-least","text":"Row reduction: Replace N rows of data with M N exemplars e.g. cluster and report just some items per cluster Column reduction: Prune C columns of data with the sqrt(C) most influential columns Range reduction (in columns): Discretize numerics, but only cut into ranges in sub-ranges have less variance Range pruning If there is a class variable: only cut if the associated class ranges are less varied Else, cluster row/column/range data and only cut if associated ranges are less varied in different clusters.","title":"A Common Recipe (For Me, at Least)"},{"location":"lectures/simple/#cautions","text":"So that\u2019s it? Just find the few dimensions that matter, then stop?o Well... The reduced data space exists... but how to find it? Feature selection on raw data? Or may be synthesized (PCA, spectral, etc) Also: And the dimensions that matter NOW May not matter SOON So an agent that assumes low dimensionality (but fyi, we should be doing that with all data mining approaches, anyway). Will always be checking and revising their dimensional","title":"Cautions"},{"location":"lectures/stats/","text":"Stats Comparing Treatments Your task: Download stats0.txt stat2.txt stats.py Repeat the following exercise for stat2.txt , then stats0.txt Look at the data in stats0.txt; Sort them by their median; Draw percentile charts for each (no need to be super accurate, near enough is good enough); Do any of these seven groups cluster together? When you have answers to all the above (and not before), compare your results to cat statX.txt | python stats.py --text 30 Run python stat4.py | python stats.py and comment if you agree or disagree with the output. How to Comapre Results To compare if treatment 1 is better than another, apply the followng rules: Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; That is: When faced with new data, always chant the following mantra: First visualize it to get some intuitions; Then apply some statistics to double check those intuitions. That is, it is strong recommended that, prior doing any statistical work, an analyst generates a visualization of the data. (And some even say, do not do stats at all: \"If you need statistics, you did the wrong experiment.\" -- Enrest Rutherford.) Sometimes, visualizations are enough: CHIRP: A new classifier based on Composite Hypercubes on Iterated Random Projections Simpler Questions Can Lead To Better Insights , from Perspectives on Data Science for Software Engineering, Morgan-Kaufmann, 2015 Percentile Charts Percentile charts a simple way to display very large populations in very little space. The advantage of percentile charts is that we can show a lot of data in very little space. For example, here's an example where the xtile Python function shows 2000 numbers on two lines: Consider two distributions, of 1000 samples each: one shows square root of a rand() and the other shows the square of a rand() . 10: def _tile() : 11: import random 12: r = random.random 13: def show(lst): 14: return xtile(lst,lo=0, hi=1,width=25, 15: show= lambda s:\" %3.2f\" % s) 16: print \"one\", show([r()*0.5 for x in range(1000)]) 17: print \"two\", show([r()*2 for x in range(1000)]) In the following quintile charts, we show these distributions: The range is 0 to 1. One line shows the square of 1000 random numbers; The other line shows the square root of 1000 random numbers; Note the brevity of the display: one -----| * --- , 0.32, 0.55, 0.70, 0.84, 0.95 two -- * |-------- , 0.01, 0.10, 0.27, 0.51, 0.85 Quintiles divide the data into the 10th, 30th, 50th, 70th, 90th percentile. Dashes ( \"-\" ) mark the range (10,30)th and (70,90)th percentiles; White space marks the ranges (30,50)th and (50,70)th percentiles. The vertical bar \"|\" shows half way between the display's min and max . BTW, there are many more ways to view results than just percentiles Medians To compare if one optimizer is better than another, apply the followng rules: Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; All things considered, means do not mean much, especially for highly skewed distributions. For example: Bill Gates and 35 homeless people are in the same room. Their mean annual income is over a billion dollars each- which is a number that characterized neither Mr. Gates or the homeless people. On the other hand, the median income of that population is close to zero- which is a number that characterizes most of that population. The median of a list is the middle item of the sorted values, if the list is of an odd size. If the list size is even, the median is the two values either side of the middle: def median(lst,ordered=False): lst = lst if ordered else sorted(lst) n = len(lst) p = n // 2 if (n % 2): return lst[p] p,q = p-1,p q = max(0,(min(q,n))) return (lst[p] + lst[q]) * 0.5 Check for \"Small Effects\" To compare if one optimizer is better than another, apply the followng rules: Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; An effect size test is a sanity check that can be summarizes as follows: Don't sweat the small stuff; I.e. ignore small differences between items in the samples. There parametric and non-parametric tests for \"small effects\" (which, if we find, we should just ignore). Parametric tests assume that the numbers fit some simple distribution (e.g. the normal Gaussian curve). Cohen's rule: compare means = 1- 2 between two samples; compute the standard deviation of the combined samples; large effect if 0.5* medium effect if 0.3* small effect if 0.1* ; And \"small effect\" means \"yawn\", too small to be interesting. Widely viewed as too simplistic. Hedge's rule (using g ): Still parametric Modifies w.r.t. the standard deviation of both samples. Adds a correction factor c for small sample sizes. In their review of use of effect size in SE, Kampenses et al. report that many papers use something like g 0.38 is the boundary between small effects and bigger effects. - Systematic Review of Effect Size in Software Engineering Experiments Kampenes, Vigdis By, et al. Information and Software Technology 49.11 (2007): 1073-1086. - See equations 2,3,4 and Figure 9 def hedges(i,j,small=0.38): Hedges effect size test. Returns true if the i and j difference is only a small effect. i and j are objects reporing mean (i.mu), standard deviation (i.s) and size (i.n) of two population of numbers. num = (i.n - 1)*i.s**2 + (j.n - 1)*j.s**2 denom = (i.n - 1) + (j.n - 1) sp = ( num / denom )**0.5 delta = abs(i.mu - j.mu) / sp c = 1 - 3.0 / (4*(i.n + j.n - 2) - 1) return delta * c small Code Cliff's Delta non-parametric Cliff's delta counts bigger and smaller . def cliffsDelta(lst1, lst2, small=0.147): # assumes all samples are nums Cliff's delta between two list of numbers i,j. lt = gt = 0 for x in lst1: for y in lst2 : if x y: gt += 1 if x y: lt += 1 z = abs(lt - gt) / (len(lst1) * len(lst2)) return z small # true is small effect in difference As above, could be optimized with a pre-sort . Statistically Significantly Different To compare if one optimizer is better than another, apply the followng rules: Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone. A significance test checks that the observed effect is not due to noise, to degree of certainty \"c\". Note that the term significance does not imply importance and the term statistical significance is not the same as research, theoretical, or practical significance. For example: Code can be developed by local teams or distributed teams spread around the world. It turns out the bug rate of these two methods is statistically significantly different. But the size of the different is about zero (as detected by the Hedge's test, shown below). From Ekrem Kocaguneli, Thomas Zimmermann, Christian Bird, Nachiappan Nagappan, and Tim Menzies. 2013. Distributed development considered harmful?. In Proceedings of the 2013 International Conference on Software Engineering (ICSE '13). IEEE Press, Piscataway, NJ, USA, 882-890. For these reasons, standard statistical tests are coming under fire: Various high profile journals are banning the use the null hypothesis significance testing procedure (NHSTP) (a classic statistical significance test) from their articles 1 , 2 , 3 To go from signficance to importance, we should: at least check for the abscence of small effects shown above and the rank tests shown below at most ask a domain expert if the observed difference matters a hoot. For example, here are some charts showing the effects on a population as we apply more and more of some treatment. Note that the mean of the populations remains unchanged, yet we might still endorse the treatment since it reduces the uncertainty associated with each population. Note the large overlap in the top two curves in those plots. When distributions exhibit a very large overlap, it is very hard to determine if one is really different to the other. So large variances can mean that even if the means are better , we cannot really say that the values in one distribution are usually better than the other. In any case, what a signifcance test does is report how small is the overlap between two distributions (and if it is very small, then we say the differences are statistically significant . T-test (parametric Significance Test) Assuming the populations are bell-shaped curve, when are two curves not significantly different? class Num: An Accumulator for numbers def __init__(i,inits=[]): i.n = i.m2 = i.mu = 0.0 for x in inits: i.add(x) def s(i): return (i.m2/(i.n - 1))**0.5 def add(i,x): i._median=None i.n += 1 delta = x - i.mu i.mu += delta*1.0/i.n i.m2 += delta*(x - i.mu) def tTestSame(i,j,conf=0.95): nom = abs(i.mu - j.mu) s1,s2 = i.s(), j.s() denom = ((s1/i.n + s2/j.n)**0.5) if s1+s2 else 1 df = min(i.n - 1, j.n - 1) return criticalValue(df, conf) = nom/denom The above needs a magic threshold )(on the last line) for sayng enough is enough def criticalValue(df,conf=0.95, xs= [ 1, 2, 5, 10, 15, 20, 25, 30, 60, 100], ys= {0.9: [ 3.078, 1.886, 1.476, 1.372, 1.341, 1.325, 1.316, 1.31, 1.296, 1.29], 0.95: [ 6.314, 2.92, 2.015, 1.812, 1.753, 1.725, 1.708, 1.697, 1.671, 1.66], 0.99: [31.821, 6.965, 3.365, 2.764, 2.602, 2.528, 2.485, 2.457, 2.39, 2.364]}): return interpolate(df, xs, ys[conf]) def interpolate(x,xs,ys): if x = xs[0] : return ys[0] if x = xs[-1]: return ys[-1] x0, y0 = xs[0], ys[0] for x1,y1 in zip(xs,ys): if x x0 or x xs[-1] or x0 = x x1: break x0, y0 = x1, y1 gap = (x - x0)/(x1 - x0) return y0 + gap*(y1 - y0) Many distributions are not normal so I use this tTestSame as a heuristic for speed criticl calcs. E.g. in the inner inner loop of some search where i need a quick opinion, is \"this\" the same as \"that\". But when assessing experimental results after all the algorithms have terminated, I use a much safer, but somewhat slower, procedure: Bootstrap (Non-parametric Significance Test) The world is not normal: Here are 50 different SQL queries and the distribution of times CPU is waiting on hard drive i/o: So, when the world is a funny shape, what to do? The following bootstrap method was introduced in 1979 by Bradley Efron at Stanford University. It was inspired by earlier work on the jackknife. Improved estimates of the variance were developed later . To check if two populations (y0,z0) are different using the bootstrap, many times sample with replacement from both to generate (y1,z1), (y2,z2), (y3,z3) .. etc. def sampleWithReplacement(lst): returns a list same size as list def any(n) : return random.uniform(0,n) def one(lst): return lst[ int(any(len(lst))) ] return [one(lst) for _ in lst] Then, for all those samples, check if some testStatistic in the original pair hold for all the other pairs. If it does more than (say) 99% of the time, then we are 99% confident in that the populations are the same. In such a bootstrap hypothesis test, the some property is the difference between the two populations, muted by the joint standard deviation of the populations. def testStatistic(y,z): Checks if two means are different, tempered by the sample size of 'y' and 'z' tmp1 = tmp2 = 0 for y1 in y.all: tmp1 += (y1 - y.mu)**2 for z1 in z.all: tmp2 += (z1 - z.mu)**2 s1 = (float(tmp1)/(y.n - 1))**0.5 s2 = (float(tmp2)/(z.n - 1))**0.5 delta = z.mu - y.mu if s1+s2: delta = delta/((s1/y.n + s2/z.n)**0.5) return delta The rest is just details: Efron advises to make the mean of the populations the same (see the yhat,zhat stuff shown below). The class total is a just a quick and dirty accumulation class. For more details see the Efron text . def bootstrap(y0,z0,conf=The.conf,b=The.b): The bootstrap hypothesis test from p220 to 223 of Efron's book 'An introduction to the boostrap. class total(): quick and dirty data collector def __init__(i,some=[]): i.sum = i.n = i.mu = 0 ; i.all=[] for one in some: i.put(one) def put(i,x): i.all.append(x); i.sum +=x; i.n += 1; i.mu = float(i.sum)/i.n def __add__(i1,i2): return total(i1.all + i2.all) y, z = total(y0), total(z0) x = y + z tobs = testStatistic(y,z) yhat = [y1 - y.mu + x.mu for y1 in y.all] zhat = [z1 - z.mu + x.mu for z1 in z.all] bigger = 0.0 for i in range(b): if testStatistic(total(sampleWithReplacement(yhat)), total(sampleWithReplacement(zhat))) tobs: bigger += 1 return bigger / b conf Warning- bootstrap can be slow. As to how many bootstraps are enough, that depends on the data. There are results saying 200 to 400 are enough but, since I am suspicious man, I run it for 1000. Which means the runtimes associated with bootstrapping is a significant issue. To reduce that runtime, I avoid things like an all-pairs comparison of all treatments (see below: Scott-knott). Also, BEFORE I do the boostrap, I first run the effect size test (and only go to bootstrapping in effect size passes: def different(l1,l2): #return bootstrap(l1,l2) and a12(l2,l1) return not a12(l2,l1) and bootstrap(l1,l2) Scott-Knott So, How to Rank? The following code, which you can use verbatim from stats.py does the following: + All treatments are recursively bi-clustered into ranks . + At each level, the treatments are split at the point where the expected values of the treatments after the split is most different to before, + Before recursing downwards, Bootstrap+A12 is called to check that that the two splits are actually different (if not: halt!) In practice, + Dozens of treatments end up generating just a handful of ranks. + The numbers of calls to the hypothesis tests are minimized: + Treatments are sorted by their median value. + Treatments are divided into two groups such that the expected value of the mean values after the split is minimized; + Hypothesis tests are called to test if the two groups are truly difference. + All hypothesis tests are non-parametric and include (1) effect size tests and (2) tests for statistically significant numbers; + Slow bootstraps are executed if the faster A12 tests are passed; In practice, this means that the hypothesis tests (with confidence of say, 95%) are called on only a logarithmic number of times. So... With this method, 16 treatments can be studied using less than 1,2,4,8,16 log 2 i =15 hypothesis tests and confidence 0.99 15 =0.86 . But if did this with the 120 all-pairs comparisons of the 16 treatments, we would have total confidence 0.99 120 =0.30 . For examples on using this code, run cat statX.txt | python stats.py . The results of a Scott-Knott+Bootstrap+A12 is a very simple presentation of a very complex set of results: img/notnorm8.png","title":"Stats"},{"location":"lectures/stats/#stats","text":"","title":"Stats"},{"location":"lectures/stats/#comparing-treatments","text":"Your task: Download stats0.txt stat2.txt stats.py Repeat the following exercise for stat2.txt , then stats0.txt Look at the data in stats0.txt; Sort them by their median; Draw percentile charts for each (no need to be super accurate, near enough is good enough); Do any of these seven groups cluster together? When you have answers to all the above (and not before), compare your results to cat statX.txt | python stats.py --text 30 Run python stat4.py | python stats.py and comment if you agree or disagree with the output.","title":"Comparing Treatments"},{"location":"lectures/stats/#how-to-comapre-results","text":"To compare if treatment 1 is better than another, apply the followng rules: Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; That is: When faced with new data, always chant the following mantra: First visualize it to get some intuitions; Then apply some statistics to double check those intuitions. That is, it is strong recommended that, prior doing any statistical work, an analyst generates a visualization of the data. (And some even say, do not do stats at all: \"If you need statistics, you did the wrong experiment.\" -- Enrest Rutherford.) Sometimes, visualizations are enough: CHIRP: A new classifier based on Composite Hypercubes on Iterated Random Projections Simpler Questions Can Lead To Better Insights , from Perspectives on Data Science for Software Engineering, Morgan-Kaufmann, 2015","title":"How to Comapre Results"},{"location":"lectures/stats/#percentile-charts","text":"Percentile charts a simple way to display very large populations in very little space. The advantage of percentile charts is that we can show a lot of data in very little space. For example, here's an example where the xtile Python function shows 2000 numbers on two lines: Consider two distributions, of 1000 samples each: one shows square root of a rand() and the other shows the square of a rand() . 10: def _tile() : 11: import random 12: r = random.random 13: def show(lst): 14: return xtile(lst,lo=0, hi=1,width=25, 15: show= lambda s:\" %3.2f\" % s) 16: print \"one\", show([r()*0.5 for x in range(1000)]) 17: print \"two\", show([r()*2 for x in range(1000)]) In the following quintile charts, we show these distributions: The range is 0 to 1. One line shows the square of 1000 random numbers; The other line shows the square root of 1000 random numbers; Note the brevity of the display: one -----| * --- , 0.32, 0.55, 0.70, 0.84, 0.95 two -- * |-------- , 0.01, 0.10, 0.27, 0.51, 0.85 Quintiles divide the data into the 10th, 30th, 50th, 70th, 90th percentile. Dashes ( \"-\" ) mark the range (10,30)th and (70,90)th percentiles; White space marks the ranges (30,50)th and (50,70)th percentiles. The vertical bar \"|\" shows half way between the display's min and max . BTW, there are many more ways to view results than just percentiles","title":"Percentile Charts"},{"location":"lectures/stats/#medians","text":"To compare if one optimizer is better than another, apply the followng rules: Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; All things considered, means do not mean much, especially for highly skewed distributions. For example: Bill Gates and 35 homeless people are in the same room. Their mean annual income is over a billion dollars each- which is a number that characterized neither Mr. Gates or the homeless people. On the other hand, the median income of that population is close to zero- which is a number that characterizes most of that population. The median of a list is the middle item of the sorted values, if the list is of an odd size. If the list size is even, the median is the two values either side of the middle: def median(lst,ordered=False): lst = lst if ordered else sorted(lst) n = len(lst) p = n // 2 if (n % 2): return lst[p] p,q = p-1,p q = max(0,(min(q,n))) return (lst[p] + lst[q]) * 0.5","title":"Medians"},{"location":"lectures/stats/#check-for-small-effects","text":"To compare if one optimizer is better than another, apply the followng rules: Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; An effect size test is a sanity check that can be summarizes as follows: Don't sweat the small stuff; I.e. ignore small differences between items in the samples. There parametric and non-parametric tests for \"small effects\" (which, if we find, we should just ignore). Parametric tests assume that the numbers fit some simple distribution (e.g. the normal Gaussian curve).","title":"Check for \"Small Effects\""},{"location":"lectures/stats/#cohens-rule","text":"compare means = 1- 2 between two samples; compute the standard deviation of the combined samples; large effect if 0.5* medium effect if 0.3* small effect if 0.1* ; And \"small effect\" means \"yawn\", too small to be interesting. Widely viewed as too simplistic.","title":"Cohen's rule:"},{"location":"lectures/stats/#hedges-rule-using-g","text":"Still parametric Modifies w.r.t. the standard deviation of both samples. Adds a correction factor c for small sample sizes. In their review of use of effect size in SE, Kampenses et al. report that many papers use something like g 0.38 is the boundary between small effects and bigger effects. - Systematic Review of Effect Size in Software Engineering Experiments Kampenes, Vigdis By, et al. Information and Software Technology 49.11 (2007): 1073-1086. - See equations 2,3,4 and Figure 9 def hedges(i,j,small=0.38): Hedges effect size test. Returns true if the i and j difference is only a small effect. i and j are objects reporing mean (i.mu), standard deviation (i.s) and size (i.n) of two population of numbers. num = (i.n - 1)*i.s**2 + (j.n - 1)*j.s**2 denom = (i.n - 1) + (j.n - 1) sp = ( num / denom )**0.5 delta = abs(i.mu - j.mu) / sp c = 1 - 3.0 / (4*(i.n + j.n - 2) - 1) return delta * c small Code","title":"Hedge's rule (using g):"},{"location":"lectures/stats/#cliffs-delta","text":"non-parametric Cliff's delta counts bigger and smaller . def cliffsDelta(lst1, lst2, small=0.147): # assumes all samples are nums Cliff's delta between two list of numbers i,j. lt = gt = 0 for x in lst1: for y in lst2 : if x y: gt += 1 if x y: lt += 1 z = abs(lt - gt) / (len(lst1) * len(lst2)) return z small # true is small effect in difference As above, could be optimized with a pre-sort .","title":"Cliff's Delta"},{"location":"lectures/stats/#statistically-significantly-different","text":"To compare if one optimizer is better than another, apply the followng rules: Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone. A significance test checks that the observed effect is not due to noise, to degree of certainty \"c\". Note that the term significance does not imply importance and the term statistical significance is not the same as research, theoretical, or practical significance. For example: Code can be developed by local teams or distributed teams spread around the world. It turns out the bug rate of these two methods is statistically significantly different. But the size of the different is about zero (as detected by the Hedge's test, shown below). From Ekrem Kocaguneli, Thomas Zimmermann, Christian Bird, Nachiappan Nagappan, and Tim Menzies. 2013. Distributed development considered harmful?. In Proceedings of the 2013 International Conference on Software Engineering (ICSE '13). IEEE Press, Piscataway, NJ, USA, 882-890. For these reasons, standard statistical tests are coming under fire: Various high profile journals are banning the use the null hypothesis significance testing procedure (NHSTP) (a classic statistical significance test) from their articles 1 , 2 , 3 To go from signficance to importance, we should: at least check for the abscence of small effects shown above and the rank tests shown below at most ask a domain expert if the observed difference matters a hoot. For example, here are some charts showing the effects on a population as we apply more and more of some treatment. Note that the mean of the populations remains unchanged, yet we might still endorse the treatment since it reduces the uncertainty associated with each population. Note the large overlap in the top two curves in those plots. When distributions exhibit a very large overlap, it is very hard to determine if one is really different to the other. So large variances can mean that even if the means are better , we cannot really say that the values in one distribution are usually better than the other. In any case, what a signifcance test does is report how small is the overlap between two distributions (and if it is very small, then we say the differences are statistically significant .","title":"Statistically Significantly Different"},{"location":"lectures/stats/#t-test-parametric-significance-test","text":"Assuming the populations are bell-shaped curve, when are two curves not significantly different? class Num: An Accumulator for numbers def __init__(i,inits=[]): i.n = i.m2 = i.mu = 0.0 for x in inits: i.add(x) def s(i): return (i.m2/(i.n - 1))**0.5 def add(i,x): i._median=None i.n += 1 delta = x - i.mu i.mu += delta*1.0/i.n i.m2 += delta*(x - i.mu) def tTestSame(i,j,conf=0.95): nom = abs(i.mu - j.mu) s1,s2 = i.s(), j.s() denom = ((s1/i.n + s2/j.n)**0.5) if s1+s2 else 1 df = min(i.n - 1, j.n - 1) return criticalValue(df, conf) = nom/denom The above needs a magic threshold )(on the last line) for sayng enough is enough def criticalValue(df,conf=0.95, xs= [ 1, 2, 5, 10, 15, 20, 25, 30, 60, 100], ys= {0.9: [ 3.078, 1.886, 1.476, 1.372, 1.341, 1.325, 1.316, 1.31, 1.296, 1.29], 0.95: [ 6.314, 2.92, 2.015, 1.812, 1.753, 1.725, 1.708, 1.697, 1.671, 1.66], 0.99: [31.821, 6.965, 3.365, 2.764, 2.602, 2.528, 2.485, 2.457, 2.39, 2.364]}): return interpolate(df, xs, ys[conf]) def interpolate(x,xs,ys): if x = xs[0] : return ys[0] if x = xs[-1]: return ys[-1] x0, y0 = xs[0], ys[0] for x1,y1 in zip(xs,ys): if x x0 or x xs[-1] or x0 = x x1: break x0, y0 = x1, y1 gap = (x - x0)/(x1 - x0) return y0 + gap*(y1 - y0) Many distributions are not normal so I use this tTestSame as a heuristic for speed criticl calcs. E.g. in the inner inner loop of some search where i need a quick opinion, is \"this\" the same as \"that\". But when assessing experimental results after all the algorithms have terminated, I use a much safer, but somewhat slower, procedure:","title":"T-test (parametric Significance Test)"},{"location":"lectures/stats/#bootstrap-non-parametric-significance-test","text":"The world is not normal: Here are 50 different SQL queries and the distribution of times CPU is waiting on hard drive i/o: So, when the world is a funny shape, what to do? The following bootstrap method was introduced in 1979 by Bradley Efron at Stanford University. It was inspired by earlier work on the jackknife. Improved estimates of the variance were developed later . To check if two populations (y0,z0) are different using the bootstrap, many times sample with replacement from both to generate (y1,z1), (y2,z2), (y3,z3) .. etc. def sampleWithReplacement(lst): returns a list same size as list def any(n) : return random.uniform(0,n) def one(lst): return lst[ int(any(len(lst))) ] return [one(lst) for _ in lst] Then, for all those samples, check if some testStatistic in the original pair hold for all the other pairs. If it does more than (say) 99% of the time, then we are 99% confident in that the populations are the same. In such a bootstrap hypothesis test, the some property is the difference between the two populations, muted by the joint standard deviation of the populations. def testStatistic(y,z): Checks if two means are different, tempered by the sample size of 'y' and 'z' tmp1 = tmp2 = 0 for y1 in y.all: tmp1 += (y1 - y.mu)**2 for z1 in z.all: tmp2 += (z1 - z.mu)**2 s1 = (float(tmp1)/(y.n - 1))**0.5 s2 = (float(tmp2)/(z.n - 1))**0.5 delta = z.mu - y.mu if s1+s2: delta = delta/((s1/y.n + s2/z.n)**0.5) return delta The rest is just details: Efron advises to make the mean of the populations the same (see the yhat,zhat stuff shown below). The class total is a just a quick and dirty accumulation class. For more details see the Efron text . def bootstrap(y0,z0,conf=The.conf,b=The.b): The bootstrap hypothesis test from p220 to 223 of Efron's book 'An introduction to the boostrap. class total(): quick and dirty data collector def __init__(i,some=[]): i.sum = i.n = i.mu = 0 ; i.all=[] for one in some: i.put(one) def put(i,x): i.all.append(x); i.sum +=x; i.n += 1; i.mu = float(i.sum)/i.n def __add__(i1,i2): return total(i1.all + i2.all) y, z = total(y0), total(z0) x = y + z tobs = testStatistic(y,z) yhat = [y1 - y.mu + x.mu for y1 in y.all] zhat = [z1 - z.mu + x.mu for z1 in z.all] bigger = 0.0 for i in range(b): if testStatistic(total(sampleWithReplacement(yhat)), total(sampleWithReplacement(zhat))) tobs: bigger += 1 return bigger / b conf Warning- bootstrap can be slow. As to how many bootstraps are enough, that depends on the data. There are results saying 200 to 400 are enough but, since I am suspicious man, I run it for 1000. Which means the runtimes associated with bootstrapping is a significant issue. To reduce that runtime, I avoid things like an all-pairs comparison of all treatments (see below: Scott-knott). Also, BEFORE I do the boostrap, I first run the effect size test (and only go to bootstrapping in effect size passes: def different(l1,l2): #return bootstrap(l1,l2) and a12(l2,l1) return not a12(l2,l1) and bootstrap(l1,l2)","title":"Bootstrap (Non-parametric Significance Test)"},{"location":"lectures/stats/#scott-knott-so-how-to-rank","text":"The following code, which you can use verbatim from stats.py does the following: + All treatments are recursively bi-clustered into ranks . + At each level, the treatments are split at the point where the expected values of the treatments after the split is most different to before, + Before recursing downwards, Bootstrap+A12 is called to check that that the two splits are actually different (if not: halt!) In practice, + Dozens of treatments end up generating just a handful of ranks. + The numbers of calls to the hypothesis tests are minimized: + Treatments are sorted by their median value. + Treatments are divided into two groups such that the expected value of the mean values after the split is minimized; + Hypothesis tests are called to test if the two groups are truly difference. + All hypothesis tests are non-parametric and include (1) effect size tests and (2) tests for statistically significant numbers; + Slow bootstraps are executed if the faster A12 tests are passed; In practice, this means that the hypothesis tests (with confidence of say, 95%) are called on only a logarithmic number of times. So... With this method, 16 treatments can be studied using less than 1,2,4,8,16 log 2 i =15 hypothesis tests and confidence 0.99 15 =0.86 . But if did this with the 120 all-pairs comparisons of the 16 treatments, we would have total confidence 0.99 120 =0.30 . For examples on using this code, run cat statX.txt | python stats.py . The results of a Scott-Knott+Bootstrap+A12 is a very simple presentation of a very complex set of results: img/notnorm8.png","title":"Scott-Knott So, How to Rank?"},{"location":"lectures/tables/","text":"Different Learners for Different Data Let us start at the very beginning (a very good place to start). When you read you begin with A-B-C. When you mine, you begin with data. Different kinds of data miners work best of different kinds of data. Such data may be viewed as tables of examples: Tables have one column per feature and one row per example. The columns may be numeric (have numbers) or symbolic (contain discrete values). Also, some columns are goals (things we want to predict using the other columns). Finally, columns may contain missing values. For example, in text mining, where there is one column per word and one row per document, the columns contain many missing values (since not all words appear in all documents) and there may be hundreds of thousands of columns. While text mining applications can have many columns, Big Data applications can have any number of columns and millions to billions of rows. For such very large datasets, a complete analysis may be impossible. Hence, these might be sampled probabilistically (e.g., using the naive Bayesian algorithm discussed below). On the other hand, when there are very few rows, data mining may fail since there are too few examples to support summarization. For such sparse tables, k- nearest neighbors (kNN) may be best. kNN makes conclusions about new examples by looking at their neighborhood in the space of old examples. Hence, kNN only needs a few (or even only one) similar examples to make conclusions. If a table has no goal columns, then this is an unsupervised learning problem that might be addressed by (say) finding clusters of similar rows using, say, K- means or expectation maximization. An alternate approach, taken by the Apriori association rule learner, is to assume that every column is a goal and to look for what combinations of any values predict for any combination of any other. If a table has one goal, then this is a supervised learning problem where the task is to find combinations of values from the other columns that predict for the goal values. Note that for datasets with one discrete goal feature, it is common to call that goal the class of the dataset. For example, here is a table of data for a simple data mining problem: outlook, temp,humid,wind,play ----------------------------- sunny, 85, 85, FALSE, no sunny, 80, 90, TRUE, no overcast, 83, 86, FALSE, yes rainy, 70, 96, FALSE, yes rainy, 68, 80, FALSE, yes rainy, 65, 70, TRUE, no overcast, 64, 65, TRUE, yes sunny, 72, 95, FALSE, no sunny, 69, 70, FALSE, yes rainy, 75, 80, FALSE, yes sunny, 75, 70, TRUE, yes overcast, 72, 90, TRUE, yes overcast, 81, 75, FALSE, yes rainy, 71, 91, TRUE, no In this table, we are trying to predict for the goal of play?, given a record of the weather. Each row is one example where we did or did not play golf (and the goal of data mining is to find what weather predicts for playing golf). Note that temp and humidity are numeric columns and there are no missing values. Such simple tables are characterized by just a few columns and not many rows (say, dozens to thousands). Traditionally, such simple data mining problems have been explored by C4.5 and CART. However, with some clever sampling of the data, it is possible to scale these traditional learners to Big Data problems. Y = F(X) One way to look at a table of data is an example of some function that computes columns \" Y \" from input columns \" X \". Splits Another way to look at a table of data is as a source of Split s. Columns have ranges Most ranges are not interesting (not useful for decision making) So most columns are not interesting Standard lesson: need only sqrt(col) of the columns (and for text mining data, even fewer) Sym columns: Splits are solo simples or disjunctions Num columns: Splits can be found oh so many ways. In one file called auto, I can set a min imum size (say sqrt(size(rows)) and combine adjacent bins when the combination is no different to the parts. This is the expected value calculation let X be a bin of size n and standard deviation s if X =2*min return X else - Split X into Y,Z of size n1,n2 and standard deviation s1,s2 - If n1/n+s1 + n2/n*s2 s , then recurse to return split(Y) , split(Z) - Else return X Why Split? Timm's rule: the best thing to do with data is to throw most of it away. Simpler models, Quicker to explain, audit Less work for anything down stream that has to watch or act on any variable Occam's Razor English philosopher, William of Occam (1300-1349) propounded Occam's Razor: Entia non sunt multiplicanda praeter necessitatem. (Latin for \"Entities should not be multiplied more than necessary\"). That is, the fewer assumptions an explanation of a phenomenon depends on, the better it is. (BTW, Occam's razor did not survive into the 21st century. The data mining community modified it to the Minimum Description Length (MDL) principle. MDL: the best theory is the smallest BOTH is size AND number of errors). Many ways to throw away data feature selection range selection instance selection (prototype generation) Feature Section The case for FSS Repeated result: throwing out features rarely damages a theory And, sometimes, feature removal is very useful: E.g. linear regression on bn.arff yielded: Defects = 82.2602 * S1=L,M,VH + 158.6082 * S1=M,VH + 249.407 * S1=VH + 41.0281 * S2=L,H + 68.9153 * S2=H + 151.9207 * S3=M,H + 125.4786 * S3=H + 257.8698 * S4=H,M,VL + 108.1679 * S4=VL + 134.9064 * S5=L,M + -385.7142 * S6=H,M,VH + 115.5933 * S6=VH + -178.9595 * S7=H,L,M,VL + ... [ 50 lines deleted ] On a 10-way cross-validation, this correlates 0.45 from predicted to actuals. 10 times, take 90% of the date and run a WRAPPER - a best first search through combinations of attributes. At each step, linear regression was called to asses a particular combination of attributes. In those ten experiments, WRAPPER found that adding feature X to features A,B,C,... improved correlation the following number of times: number of folds (%) attribute 2( 20 %) 1 S1 0( 0 %) 2 S2 2( 20 %) 3 S3 1( 10 %) 4 S4 0( 0 %) 5 S5 1( 10 %) 6 S6 6( 60 %) 7 S7 == 1( 10 %) 8 F1 1( 10 %) 9 F2 2( 20 %) 10 F3 2( 20 %) 11 D1 0( 0 %) 12 D2 5( 50 %) 13 D3 == 0( 0 %) 14 D4 0( 0 %) 15 T1 1( 10 %) 16 T2 1( 10 %) 17 T3 1( 10 %) 18 T4 0( 0 %) 19 P1 1( 10 %) 20 P2 0( 0 %) 21 P3 1( 10 %) 22 P4 6( 60 %) 23 P5 == 1( 10 %) 24 P6 2( 20 %) 25 P7 1( 10 %) 26 P8 0( 0 %) 27 P9 2( 20 %) 28 Hours 8( 80 %) 29 KLoC == 4( 40 %) 30 Language 3( 30 %) 32 log(hours) Four variables appeared in the majority of folds. A second run did a 10-way using just those variables to yield a smaller model with (much) larger correlation (98\\%): Defects = 876.3379 * S7=VL + -292.9474 * D3=L,M + 483.6206 * P5=M + 5.5113 * KLoC + 95.4278 Excess attributes Confuse decision tree learners Too much early splitting of data Less data available for each sub-tree Too many things correlated to class? Dump some of them! Why FSS? throw away noisy attributes throw away redundant attributes smaller model= better accuracies (often) smaller model= simpler explanation smaller model= less variance smaller model= any downstream processing will thank you Problem Exploring all subsets exponential Need heuristic methods to cull search; e.g. forward/back select Forward select: start with empty set grow via hill climbing: repeat try adding one thing and if that improves things try again using the remaining attributes until no improvement after N additions OR nothing to add Back select as above but start with all attributes and discard, don't add Usually, we throw away most attributes: so forward select often better exception: J48 exploits interactions more than,say, NB. so, possibly, back select is better when wrapping j48 so, possibly, forward select is as good as it gets for NB Supervised vs Unsupervised Supervised, use the class variable in column2 to discretize column1. E.g. split column1 such that the etropy of the column2 symbols are minimized. see below Unsupervised, just refect on column1. E.g. find splits that minimize the variance afer the spits. E.g. here unsupervised discretization running on the horsepower column of auto.csv . Note these numbers run 46 to 230 and this code](http://menzies.us/lean/unsuper.html) decides that this should be divided into: less that 65 66 to 69 69 to 72 74 to 82 83 to 86 87 to 89 90 to 91 92 to 97 98 to 105 107 to 116 120 to 140 142 to 160 over 165 46.. 230 |.. 46.. 116 |.. |.. 46.. 82 |.. |.. |.. 46.. 65 (..65) |.. |.. |.. 66.. 82 |.. |.. |.. |.. 66.. 72 |.. |.. |.. |.. |.. 66.. 69 (66..69) |.. |.. |.. |.. |.. 69.. 72 (69..72) |.. |.. |.. |.. 74.. 82 (74..82) |.. |.. 83.. 116 |.. |.. |.. 83.. 97 |.. |.. |.. |.. 83.. 91 |.. |.. |.. |.. |.. 83.. 86 (83..86) |.. |.. |.. |.. |.. 87.. 91 |.. |.. |.. |.. |.. |.. 87.. 89 (87..89) |.. |.. |.. |.. |.. |.. 90.. 91 (90..91) |.. |.. |.. |.. 92.. 97 (92..97) |.. |.. |.. 98.. 116 |.. |.. |.. |.. 98.. 105 (98..105) |.. |.. |.. |.. 107.. 116 (107..116) |.. 120.. 230 |.. |.. 120.. 160 |.. |.. |.. 120.. 140 (120..140) |.. |.. |.. 142.. 160 (142..160) |.. |.. 165.. 230 (165..) FSS types: filters vs wrappers: wrappers: use an actual target learners e.g. WRAPPER filters: study aspects of the data e.g. the rest filters are faster! wrappers exploit bias of target learner so often perform better, when they terminate don't terminate on large data sets solo vs combinations: evaluate solo attributes: e.g. INFO GAIN, RELIEF evaluate combinations: e.g. PCA, SVD, CFS, CBS, WRAPPER solos can be faster than combinations supervised vs unsupervised: use/ignores class values e.g. PCA/SVD is unsupervised, reset supervised numeric vs discrete search methods ranker: for schemes that numerically score attributes e.g. RELIEF, INFO GAIN, best first: for schemes that do heuristic search e.g. CBS, CFS, WRAPPER Hall and Holmes: This paper: pre-discretize numerics using entropy. Hall Holmes INFO GAIN often useful in high-dimensional problems real simple to calculate attributes scored based on info gain: H(C) - H(C|A) Sort of like doing decision tree learning, just to one level. RELIEF Kononenko97 useful attributes differentiate between instances from other class randomly pick some instances (here, 250) find something similar, in an another class compute distance this one to the other one Stochastic sampler: scales to large data sets. Binary RELIEF (two class system) for \"n\" instances for weights on features \"F\" set all weights W[f]=0 for i = 1 to n; do randomly select instance R with class C find nearest hit H // closest thing of same class find nearest miss M // closest thing of difference class for f = 1 to #features; do W[f] = W[f] - diff(f,R,H)/n + diff(f,R,M)/n done done diff: discrete differences: 0 if same 1 if not. continuous: differences absolute differences normalized to 0:1 When values are missing, see Kononenko97 , p4. N-class RELIEF: not 1 near hit/miss, but k nearest misses for each class C W[f]= W[f] - \u2211i=1..k diff(f,R, Hi) / (n*k) + \u2211C \u2260 class(R) \u2211i=1..k ( P(C) / ( 1 - P(class(R))) * diff(f,R, Mi(C)) / (n*k) ) The P(C) / (1 - P(class(R)) expression is a normalization function that demotes the effect of R from rare classes and rewards the effect of near hits from common classes. CBS (consistency-based evaluation) Seek combinations of attributes that divide data containing a strong single class majority. Kind of like info gain, but emphasis of single winner Discrete attributes Forward select to find subsets of attributes WRAPPER Forward select attributes score each combination using a 5-way cross val When wrapping, best to try different target learners Check that we aren't over exploiting the learner's bias e.g. J48 and NB PRINCIPAL COMPONENTS ANALYSIS (PCA) (The traditional way to do FSS.) Only unsupervised method studied here Transform dimensions Find covariance matrix C[i,j] is the correlation i to j; C[i,i]=1; C[i,j]=C[j,i] Find eigenvectors Transform the original space to the eigenvectors Rank them by the variance in their predictions Report the top ranked vectors Makes things easier, right? Well... if domain1 = 0.180 then NoDefects else if domain1 0.180 then if domain1 = 0.371 then NoDefects else if domain1 0.371 then Defects domain1 = 0.241 * loc + 0.236 * v(g) + 0.222 * ev(g) + 0.236 * iv(g) + 0.241 * n + 0.238 * v - 0.086 * l + 0.199 * d + 0.216 * i + 0.225 * e + 0.236 * b + 0.221 * t + 0.241 * lOCode + 0.179 * lOComment + 0.221 * lOBlank + 0.158 * lOCodeAndComment + 0.163 * uniqO p + 0.234 * uniqOpnd + 0.241 * totalOp + 0.241 * totalOpnd + 0.236 * branchCount CFS (correlation-based feature selection) Scores high subsets with strong correlation to class and weak correlation to each other. Numerator: how predictive Denominator: how redundant FIRST ranks correlation of solo attributes THEN heuristic search to explore subsets And the winner is: Wrapper! and it that is too slow... CFS, Relief are best all round performers CFS selects fewer features Phew. Hall invented CFS Instance Selection Prototype Selection with Clusters Step1: Feature selection: sort columns by their Infogain score. Delete bottom half. Step2: Cluster: return one example pre centroid. Reduction of 800 rows by 24 attributes to 5 attributes by 22 rows For many data sets: Note that for classification by weighted scores from 2 nearest neighbors, the reduced data as accurate as the full data.","title":"Tables"},{"location":"lectures/tables/#different-learners-for-different-data","text":"Let us start at the very beginning (a very good place to start). When you read you begin with A-B-C. When you mine, you begin with data. Different kinds of data miners work best of different kinds of data. Such data may be viewed as tables of examples: Tables have one column per feature and one row per example. The columns may be numeric (have numbers) or symbolic (contain discrete values). Also, some columns are goals (things we want to predict using the other columns). Finally, columns may contain missing values. For example, in text mining, where there is one column per word and one row per document, the columns contain many missing values (since not all words appear in all documents) and there may be hundreds of thousands of columns. While text mining applications can have many columns, Big Data applications can have any number of columns and millions to billions of rows. For such very large datasets, a complete analysis may be impossible. Hence, these might be sampled probabilistically (e.g., using the naive Bayesian algorithm discussed below). On the other hand, when there are very few rows, data mining may fail since there are too few examples to support summarization. For such sparse tables, k- nearest neighbors (kNN) may be best. kNN makes conclusions about new examples by looking at their neighborhood in the space of old examples. Hence, kNN only needs a few (or even only one) similar examples to make conclusions. If a table has no goal columns, then this is an unsupervised learning problem that might be addressed by (say) finding clusters of similar rows using, say, K- means or expectation maximization. An alternate approach, taken by the Apriori association rule learner, is to assume that every column is a goal and to look for what combinations of any values predict for any combination of any other. If a table has one goal, then this is a supervised learning problem where the task is to find combinations of values from the other columns that predict for the goal values. Note that for datasets with one discrete goal feature, it is common to call that goal the class of the dataset. For example, here is a table of data for a simple data mining problem: outlook, temp,humid,wind,play ----------------------------- sunny, 85, 85, FALSE, no sunny, 80, 90, TRUE, no overcast, 83, 86, FALSE, yes rainy, 70, 96, FALSE, yes rainy, 68, 80, FALSE, yes rainy, 65, 70, TRUE, no overcast, 64, 65, TRUE, yes sunny, 72, 95, FALSE, no sunny, 69, 70, FALSE, yes rainy, 75, 80, FALSE, yes sunny, 75, 70, TRUE, yes overcast, 72, 90, TRUE, yes overcast, 81, 75, FALSE, yes rainy, 71, 91, TRUE, no In this table, we are trying to predict for the goal of play?, given a record of the weather. Each row is one example where we did or did not play golf (and the goal of data mining is to find what weather predicts for playing golf). Note that temp and humidity are numeric columns and there are no missing values. Such simple tables are characterized by just a few columns and not many rows (say, dozens to thousands). Traditionally, such simple data mining problems have been explored by C4.5 and CART. However, with some clever sampling of the data, it is possible to scale these traditional learners to Big Data problems.","title":"Different Learners for Different Data"},{"location":"lectures/tables/#y-fx","text":"One way to look at a table of data is an example of some function that computes columns \" Y \" from input columns \" X \".","title":"Y = F(X)"},{"location":"lectures/tables/#splits","text":"Another way to look at a table of data is as a source of Split s. Columns have ranges Most ranges are not interesting (not useful for decision making) So most columns are not interesting Standard lesson: need only sqrt(col) of the columns (and for text mining data, even fewer) Sym columns: Splits are solo simples or disjunctions Num columns: Splits can be found oh so many ways. In one file called auto, I can set a min imum size (say sqrt(size(rows)) and combine adjacent bins when the combination is no different to the parts. This is the expected value calculation let X be a bin of size n and standard deviation s if X =2*min return X else - Split X into Y,Z of size n1,n2 and standard deviation s1,s2 - If n1/n+s1 + n2/n*s2 s , then recurse to return split(Y) , split(Z) - Else return X","title":"Splits"},{"location":"lectures/tables/#why-split","text":"Timm's rule: the best thing to do with data is to throw most of it away. Simpler models, Quicker to explain, audit Less work for anything down stream that has to watch or act on any variable Occam's Razor English philosopher, William of Occam (1300-1349) propounded Occam's Razor: Entia non sunt multiplicanda praeter necessitatem. (Latin for \"Entities should not be multiplied more than necessary\"). That is, the fewer assumptions an explanation of a phenomenon depends on, the better it is. (BTW, Occam's razor did not survive into the 21st century. The data mining community modified it to the Minimum Description Length (MDL) principle. MDL: the best theory is the smallest BOTH is size AND number of errors). Many ways to throw away data feature selection range selection instance selection (prototype generation)","title":"Why Split?"},{"location":"lectures/tables/#feature-section","text":"The case for FSS Repeated result: throwing out features rarely damages a theory And, sometimes, feature removal is very useful: E.g. linear regression on bn.arff yielded: Defects = 82.2602 * S1=L,M,VH + 158.6082 * S1=M,VH + 249.407 * S1=VH + 41.0281 * S2=L,H + 68.9153 * S2=H + 151.9207 * S3=M,H + 125.4786 * S3=H + 257.8698 * S4=H,M,VL + 108.1679 * S4=VL + 134.9064 * S5=L,M + -385.7142 * S6=H,M,VH + 115.5933 * S6=VH + -178.9595 * S7=H,L,M,VL + ... [ 50 lines deleted ] On a 10-way cross-validation, this correlates 0.45 from predicted to actuals. 10 times, take 90% of the date and run a WRAPPER - a best first search through combinations of attributes. At each step, linear regression was called to asses a particular combination of attributes. In those ten experiments, WRAPPER found that adding feature X to features A,B,C,... improved correlation the following number of times: number of folds (%) attribute 2( 20 %) 1 S1 0( 0 %) 2 S2 2( 20 %) 3 S3 1( 10 %) 4 S4 0( 0 %) 5 S5 1( 10 %) 6 S6 6( 60 %) 7 S7 == 1( 10 %) 8 F1 1( 10 %) 9 F2 2( 20 %) 10 F3 2( 20 %) 11 D1 0( 0 %) 12 D2 5( 50 %) 13 D3 == 0( 0 %) 14 D4 0( 0 %) 15 T1 1( 10 %) 16 T2 1( 10 %) 17 T3 1( 10 %) 18 T4 0( 0 %) 19 P1 1( 10 %) 20 P2 0( 0 %) 21 P3 1( 10 %) 22 P4 6( 60 %) 23 P5 == 1( 10 %) 24 P6 2( 20 %) 25 P7 1( 10 %) 26 P8 0( 0 %) 27 P9 2( 20 %) 28 Hours 8( 80 %) 29 KLoC == 4( 40 %) 30 Language 3( 30 %) 32 log(hours) Four variables appeared in the majority of folds. A second run did a 10-way using just those variables to yield a smaller model with (much) larger correlation (98\\%): Defects = 876.3379 * S7=VL + -292.9474 * D3=L,M + 483.6206 * P5=M + 5.5113 * KLoC + 95.4278","title":"Feature Section"},{"location":"lectures/tables/#excess-attributes","text":"Confuse decision tree learners Too much early splitting of data Less data available for each sub-tree Too many things correlated to class? Dump some of them!","title":"Excess attributes"},{"location":"lectures/tables/#why-fss","text":"throw away noisy attributes throw away redundant attributes smaller model= better accuracies (often) smaller model= simpler explanation smaller model= less variance smaller model= any downstream processing will thank you","title":"Why FSS?"},{"location":"lectures/tables/#problem","text":"Exploring all subsets exponential Need heuristic methods to cull search; e.g. forward/back select Forward select: start with empty set grow via hill climbing: repeat try adding one thing and if that improves things try again using the remaining attributes until no improvement after N additions OR nothing to add Back select as above but start with all attributes and discard, don't add Usually, we throw away most attributes: so forward select often better exception: J48 exploits interactions more than,say, NB. so, possibly, back select is better when wrapping j48 so, possibly, forward select is as good as it gets for NB","title":"Problem"},{"location":"lectures/tables/#supervised-vs-unsupervised","text":"Supervised, use the class variable in column2 to discretize column1. E.g. split column1 such that the etropy of the column2 symbols are minimized. see below Unsupervised, just refect on column1. E.g. find splits that minimize the variance afer the spits. E.g. here unsupervised discretization running on the horsepower column of auto.csv . Note these numbers run 46 to 230 and this code](http://menzies.us/lean/unsuper.html) decides that this should be divided into: less that 65 66 to 69 69 to 72 74 to 82 83 to 86 87 to 89 90 to 91 92 to 97 98 to 105 107 to 116 120 to 140 142 to 160 over 165 46.. 230 |.. 46.. 116 |.. |.. 46.. 82 |.. |.. |.. 46.. 65 (..65) |.. |.. |.. 66.. 82 |.. |.. |.. |.. 66.. 72 |.. |.. |.. |.. |.. 66.. 69 (66..69) |.. |.. |.. |.. |.. 69.. 72 (69..72) |.. |.. |.. |.. 74.. 82 (74..82) |.. |.. 83.. 116 |.. |.. |.. 83.. 97 |.. |.. |.. |.. 83.. 91 |.. |.. |.. |.. |.. 83.. 86 (83..86) |.. |.. |.. |.. |.. 87.. 91 |.. |.. |.. |.. |.. |.. 87.. 89 (87..89) |.. |.. |.. |.. |.. |.. 90.. 91 (90..91) |.. |.. |.. |.. 92.. 97 (92..97) |.. |.. |.. 98.. 116 |.. |.. |.. |.. 98.. 105 (98..105) |.. |.. |.. |.. 107.. 116 (107..116) |.. 120.. 230 |.. |.. 120.. 160 |.. |.. |.. 120.. 140 (120..140) |.. |.. |.. 142.. 160 (142..160) |.. |.. 165.. 230 (165..)","title":"Supervised vs Unsupervised"},{"location":"lectures/tables/#fss-types","text":"filters vs wrappers: wrappers: use an actual target learners e.g. WRAPPER filters: study aspects of the data e.g. the rest filters are faster! wrappers exploit bias of target learner so often perform better, when they terminate don't terminate on large data sets solo vs combinations: evaluate solo attributes: e.g. INFO GAIN, RELIEF evaluate combinations: e.g. PCA, SVD, CFS, CBS, WRAPPER solos can be faster than combinations supervised vs unsupervised: use/ignores class values e.g. PCA/SVD is unsupervised, reset supervised numeric vs discrete search methods ranker: for schemes that numerically score attributes e.g. RELIEF, INFO GAIN, best first: for schemes that do heuristic search e.g. CBS, CFS, WRAPPER","title":"FSS types:"},{"location":"lectures/tables/#hall-and-holmes","text":"This paper: pre-discretize numerics using entropy. Hall Holmes","title":"Hall and Holmes:"},{"location":"lectures/tables/#info-gain","text":"often useful in high-dimensional problems real simple to calculate attributes scored based on info gain: H(C) - H(C|A) Sort of like doing decision tree learning, just to one level.","title":"INFO GAIN"},{"location":"lectures/tables/#relief","text":"Kononenko97 useful attributes differentiate between instances from other class randomly pick some instances (here, 250) find something similar, in an another class compute distance this one to the other one Stochastic sampler: scales to large data sets. Binary RELIEF (two class system) for \"n\" instances for weights on features \"F\" set all weights W[f]=0 for i = 1 to n; do randomly select instance R with class C find nearest hit H // closest thing of same class find nearest miss M // closest thing of difference class for f = 1 to #features; do W[f] = W[f] - diff(f,R,H)/n + diff(f,R,M)/n done done diff: discrete differences: 0 if same 1 if not. continuous: differences absolute differences normalized to 0:1 When values are missing, see Kononenko97 , p4. N-class RELIEF: not 1 near hit/miss, but k nearest misses for each class C W[f]= W[f] - \u2211i=1..k diff(f,R, Hi) / (n*k) + \u2211C \u2260 class(R) \u2211i=1..k ( P(C) / ( 1 - P(class(R))) * diff(f,R, Mi(C)) / (n*k) ) The P(C) / (1 - P(class(R)) expression is a normalization function that demotes the effect of R from rare classes and rewards the effect of near hits from common classes.","title":"RELIEF"},{"location":"lectures/tables/#cbs-consistency-based-evaluation","text":"Seek combinations of attributes that divide data containing a strong single class majority. Kind of like info gain, but emphasis of single winner Discrete attributes Forward select to find subsets of attributes","title":"CBS (consistency-based evaluation)"},{"location":"lectures/tables/#wrapper","text":"Forward select attributes score each combination using a 5-way cross val When wrapping, best to try different target learners Check that we aren't over exploiting the learner's bias e.g. J48 and NB","title":"WRAPPER"},{"location":"lectures/tables/#principal-components-analysis-pca","text":"(The traditional way to do FSS.) Only unsupervised method studied here Transform dimensions Find covariance matrix C[i,j] is the correlation i to j; C[i,i]=1; C[i,j]=C[j,i] Find eigenvectors Transform the original space to the eigenvectors Rank them by the variance in their predictions Report the top ranked vectors Makes things easier, right? Well... if domain1 = 0.180 then NoDefects else if domain1 0.180 then if domain1 = 0.371 then NoDefects else if domain1 0.371 then Defects domain1 = 0.241 * loc + 0.236 * v(g) + 0.222 * ev(g) + 0.236 * iv(g) + 0.241 * n + 0.238 * v - 0.086 * l + 0.199 * d + 0.216 * i + 0.225 * e + 0.236 * b + 0.221 * t + 0.241 * lOCode + 0.179 * lOComment + 0.221 * lOBlank + 0.158 * lOCodeAndComment + 0.163 * uniqO p + 0.234 * uniqOpnd + 0.241 * totalOp + 0.241 * totalOpnd + 0.236 * branchCount","title":"PRINCIPAL COMPONENTS ANALYSIS (PCA)"},{"location":"lectures/tables/#cfs-correlation-based-feature-selection","text":"Scores high subsets with strong correlation to class and weak correlation to each other. Numerator: how predictive Denominator: how redundant FIRST ranks correlation of solo attributes THEN heuristic search to explore subsets","title":"CFS (correlation-based feature selection)"},{"location":"lectures/tables/#and-the-winner-is","text":"Wrapper! and it that is too slow... CFS, Relief are best all round performers CFS selects fewer features Phew. Hall invented CFS","title":"And the winner is:"},{"location":"lectures/tables/#instance-selection","text":"","title":"Instance Selection"},{"location":"lectures/tables/#prototype-selection-with-clusters","text":"Step1: Feature selection: sort columns by their Infogain score. Delete bottom half. Step2: Cluster: return one example pre centroid. Reduction of 800 rows by 24 attributes to 5 attributes by 22 rows For many data sets: Note that for classification by weighted scores from 2 nearest neighbors, the reduced data as accurate as the full data.","title":"Prototype Selection with Clusters"},{"location":"proj/ProjectIdeas/","text":"Potential Ideas For Class Projects NOTE: For full credits, you MUST demonstrate you methods' effectiveness in one or more of the fifteen categories from this checklist Statistcal evaluations Review last ten years of highly cited papers in softare analytics List the statistiacal methods they use Apply them all to the same results See if we can do early stopping in hyperparameter optimization (what are the odds that after N statistically significant large changes, that we will see one more?) Data 95 methods Across all treatments, are ranking stable? Cluster top-down, bottom-up, best-first, worst-first. Parametric, non-parametric effect size test, yes/no Anomaly Detection If the performance is not what you'd expect, then that is an anomaly. How would you detect anomalies in SE data? What is the current state-of-the-art? How about measures like Heoffding Bounds? How about something like kNN on the leaves of a decision tree? Other ideas enouraged ... Explanation vs. Performance of Decision Trees Decisions Trees can be limited to any depth? How does restricting the depth of the tree effect performance (see Holte 1R) for binary classes? For n-ary classes? If we had a model in an incomprehensible format (that of say NB, deep learning, neural net) and we ran the training data through decisions trees, what is the performance vs explanation tradeoff? How do we quantify explanations? https://arxiv.org/pdf/1803.05067.pdf https://link.springer.com/article/10.1007/s10664-018-9638-1 FLASH for tuning Defect Prediction Look at Vivek Nair's TSE article on FLASH Can you apply this to construct better defect predictors? Can you tune defect predictors for better precision, false alarm, recall? FLASH for effort estimation? Same as above, but this time for effort estimation. Can you beat FLASH with Baysian Parameter Optimization What are the tradeoffs with using BPO Reasoning over instance-based evolutionary algorithms Pick a task to optimize over (tuning defect predictors for example) Cache generation zero of an evolutionary algorithm (say DE) Run the algorithm Cache generation N of the same algorithm Run a rule learner over the generations to learn rules about the most informative features. Questions How good is that learner at selecting for best (last gen) instances? How much is that goodness effected by sampling from how much of non-best? Develop better evolutionary algorithms Experiment with mutation strategy Parallelize the algorithm Other ideas? Scalable planning with XTREE Look at Rahul Krishna's paper on XTREE This currently works on smaller datasets, can you create a scalable implementation of this that works on a streaming data? Use ideas from Very Fast Decision Trees: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf Apply a novel data mining trick to SE data Here's a great starter book for data mining in Python. Do not fret, if you choose to do this, we'll give you a digital copy of this book. The code can be found here Pick two novel techniques from any two chapters of this book and implement it on any software engineering data. Remember, for full credits you will have to demonstrate it's effectiveness in one or more of the fifteen categories from this checklist How to discover the Bellwether? Look at Rahul Krishna's paper on Bellwethers . See section 5.2 and Figure 3. Briefly, given N projects, find the one project that can be used to train a supervised learner. Then, using that one dataset, predict for the class variable in other projects. As of now, we find this one dataset (aka Bellwether) by running a for loop over all available pairs of projects. Unfortunately, this is O(n^2) algorithm. Can you find a better way to discover this faster? Must be faster than O(n^2). Meet Rahul (TA) for datasets and potential ideas on how this can be done. Predict if a commit is a bug-fix. Commit messages are usually descriptive natural language text that describe the changes the developers made. Example, Try to fix the problem with Tbuildsys#40 (/bin/sh: configure: command not found) Can you use any NLP techniques (word2vec, sentiment analysis, etc..) to automatically classify a commit message as a bug-fix or not. Again, talk to Rahul (TA) for datasets and other potential ideas.","title":"Potential Ideas For Class Projects"},{"location":"proj/ProjectIdeas/#potential-ideas-for-class-projects","text":"NOTE: For full credits, you MUST demonstrate you methods' effectiveness in one or more of the fifteen categories from this checklist","title":"Potential Ideas For Class Projects"},{"location":"proj/ProjectIdeas/#statistcal-evaluations","text":"Review last ten years of highly cited papers in softare analytics List the statistiacal methods they use Apply them all to the same results See if we can do early stopping in hyperparameter optimization (what are the odds that after N statistically significant large changes, that we will see one more?) Data 95 methods Across all treatments, are ranking stable? Cluster top-down, bottom-up, best-first, worst-first. Parametric, non-parametric effect size test, yes/no","title":"Statistcal evaluations"},{"location":"proj/ProjectIdeas/#anomaly-detection","text":"If the performance is not what you'd expect, then that is an anomaly. How would you detect anomalies in SE data? What is the current state-of-the-art? How about measures like Heoffding Bounds? How about something like kNN on the leaves of a decision tree? Other ideas enouraged ...","title":"Anomaly Detection"},{"location":"proj/ProjectIdeas/#explanation-vs-performance-of-decision-trees","text":"Decisions Trees can be limited to any depth? How does restricting the depth of the tree effect performance (see Holte 1R) for binary classes? For n-ary classes? If we had a model in an incomprehensible format (that of say NB, deep learning, neural net) and we ran the training data through decisions trees, what is the performance vs explanation tradeoff? How do we quantify explanations? https://arxiv.org/pdf/1803.05067.pdf https://link.springer.com/article/10.1007/s10664-018-9638-1","title":"Explanation vs. Performance of Decision Trees"},{"location":"proj/ProjectIdeas/#flash-for-tuning-defect-prediction","text":"Look at Vivek Nair's TSE article on FLASH Can you apply this to construct better defect predictors? Can you tune defect predictors for better precision, false alarm, recall?","title":"FLASH for tuning Defect Prediction"},{"location":"proj/ProjectIdeas/#flash-for-effort-estimation","text":"Same as above, but this time for effort estimation.","title":"FLASH for effort estimation?"},{"location":"proj/ProjectIdeas/#can-you-beat-flash-with-baysian-parameter-optimization","text":"What are the tradeoffs with using BPO","title":"Can you beat FLASH with Baysian Parameter Optimization"},{"location":"proj/ProjectIdeas/#reasoning-over-instance-based-evolutionary-algorithms","text":"Pick a task to optimize over (tuning defect predictors for example) Cache generation zero of an evolutionary algorithm (say DE) Run the algorithm Cache generation N of the same algorithm Run a rule learner over the generations to learn rules about the most informative features. Questions How good is that learner at selecting for best (last gen) instances? How much is that goodness effected by sampling from how much of non-best?","title":"Reasoning over instance-based evolutionary algorithms"},{"location":"proj/ProjectIdeas/#develop-better-evolutionary-algorithms","text":"Experiment with mutation strategy Parallelize the algorithm Other ideas?","title":"Develop better evolutionary algorithms"},{"location":"proj/ProjectIdeas/#scalable-planning-with-xtree","text":"Look at Rahul Krishna's paper on XTREE This currently works on smaller datasets, can you create a scalable implementation of this that works on a streaming data? Use ideas from Very Fast Decision Trees: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf","title":"Scalable planning with XTREE"},{"location":"proj/ProjectIdeas/#apply-a-novel-data-mining-trick-to-se-data","text":"Here's a great starter book for data mining in Python. Do not fret, if you choose to do this, we'll give you a digital copy of this book. The code can be found here Pick two novel techniques from any two chapters of this book and implement it on any software engineering data. Remember, for full credits you will have to demonstrate it's effectiveness in one or more of the fifteen categories from this checklist","title":"Apply a novel data mining trick to SE data"},{"location":"proj/ProjectIdeas/#how-to-discover-the-bellwether","text":"Look at Rahul Krishna's paper on Bellwethers . See section 5.2 and Figure 3. Briefly, given N projects, find the one project that can be used to train a supervised learner. Then, using that one dataset, predict for the class variable in other projects. As of now, we find this one dataset (aka Bellwether) by running a for loop over all available pairs of projects. Unfortunately, this is O(n^2) algorithm. Can you find a better way to discover this faster? Must be faster than O(n^2). Meet Rahul (TA) for datasets and potential ideas on how this can be done.","title":"How to discover the Bellwether?"},{"location":"proj/ProjectIdeas/#predict-if-a-commit-is-a-bug-fix","text":"Commit messages are usually descriptive natural language text that describe the changes the developers made. Example, Try to fix the problem with Tbuildsys#40 (/bin/sh: configure: command not found) Can you use any NLP techniques (word2vec, sentiment analysis, etc..) to automatically classify a commit message as a bug-fix or not. Again, talk to Rahul (TA) for datasets and other potential ideas.","title":"Predict if a commit is a bug-fix."},{"location":"proj/stats/","text":"# Stats.py from future import division,print_function import sys,random, argparse sys.dont_write_bytecode=True class o(): \"Anonymous container\" def init (i,**fields) : i.override(fields) def override(i,d): i. dict .update(d); return i def repr (i): d = i. dict name = i. class . name return name+'{'+' '.join([':%s %s' % (k,d[k]) for k in i.show()])+ '}' def show(i): return [k for k in sorted(i. dict .keys()) if not \"_\" in k] The=o(cohen=0.3, small=3, epsilon=0.01, width=50,lo=0,hi=100,conf=0.01,b=1000,a12=0.56) parser = argparse.ArgumentParser( description=\"Apply Scott-Knot test to data read from standard input\") p=parser.add_argument p(\"--demo\",default=False, action=\"store_true\") p(\"--cohen\", type=float, default=0.3, metavar='N', help=\"too small if delta less than N*std of the data)\") p(\"--small\",type=int, metavar=\"N\",default=3, help=\"too small if hold less than N items\") p(\"--epsilon\", type=float, default=0.01,metavar=\"N\", help=\"a range is too small of its hi - lo N\") p(\"--width\",type=int,default=50,metavar=\"N\", help=\"width of quintile display\") p(\"--text\",type=int,default=12,metavar=\"N\", help=\"width of text display\") p(\"--conf\", type=float, default=0.01,metavar=\"N\", help=\"bootstrap tests with confidence 1-n\") p(\"--a12\",type=float, default=0.56, metavar=\"N\", help=\"threshold for a12 test: disable,small,med,large=0,0.56,0.64,0.71\") p(\"--useA12\",default=False, metavar=\"N\", help=\"True if you want to use A12 instead of cliff's delta\") p(\"--latex\",default=False,metavar=\"N\", help=\"default is false and True for getting a latex table for the data\") p(\"--cdelta\",default=0.147,metavar=\"N\", help=\"value for cliff's delta to be considered not a small effect\") args = parser.parse_args() The.cohen = args.cohen The.small = args.small The.epsilon = args.epsilon The.conf = args.conf The.width = args.width + 0 The.a12 = args.a12 + 0 The.text = args.text + 0 The.latex = args.latex The.useA12 = args.useA12 The.cdelta = args.cdelta TODO try: opts, args = getopt.getopt(argv, \"hg:d\", [\"help\", \"grammar=\"]) 2 except getopt.GetoptError: 3 usage() 4 sys.exit(2) Analysis of Experimental Data This page is about the non-parametric statistical tests. It is also a chance for us to discuss a little statistical theory. Before we begin... Imagine the following example contain objective scores gained from different optimizers x1,x2,x3,x4,...etc . Which results are ranked one, two, three etc... Lesson Zero Some differences are obvious def rdiv0(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,6, 7, 8, 9] ]) rank , name , med , iqr 1 , x1 , 51 , 11 ( | ), 0.34, 0.49, 0.51, 0.51, 0.60 2 , x2 , 800 , 200 ( | ---- -- ), 6.00, 7.00, 8.00, 8.00, 9.00 Lesson One Some similarities are obvious... def rdiv1(): rdivDemo([ [ x1 ,0.1, 0.2, 0.3, 0.4], [ x2 ,0.1, 0.2, 0.3, 0.4], [ x3 ,6, 7, 8, 9] ]) rank , name , med , iqr 1 , x1 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x2 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 2 , x3 , 800 , 200 ( | ---- *-- ), 6.00, 7.00, 8.00, 8.00, 9.00 Lesson Two Many results often clump into less-than-many ranks. def rdiv2(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,0.6, 0.7, 0.8, 0.9], [ x3 ,0.15, 0.25, 0.4, 0.35], [ x4 ,0.6, 0.7, 0.8, 0.9], [ x5 ,0.1, 0.2, 0.3, 0.4] ]) rank , name , med , iqr 1 , x5 , 30 , 20 (--- --- | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x3 , 35 , 15 ( ---- - | ), 0.15, 0.25, 0.35, 0.35, 0.40 2 , x1 , 51 , 11 ( ------ -- ), 0.34, 0.49, 0.51, 0.51, 0.60 3 , x2 , 80 , 20 ( | ---- -- ), 0.60, 0.70, 0.80, 0.80, 0.90 3 , x4 , 80 , 20 ( | ---- *-- ), 0.60, 0.70, 0.80, 0.80, 0.90 Lesson Three Some results even clump into one rank (the great null result). def rdiv3(): rdivDemo([ [ x1 ,101, 100, 99, 101, 99.5], [ x2 ,101, 100, 99, 101, 100], [ x3 ,101, 100, 99.5, 101, 99], [ x4 ,101, 100, 99, 101, 100] ]) rank , name , med , iqr 1 , x1 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x2 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 1 , x3 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x4 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 Lesson Four Heh? Where's lesson four? Lesson Five Some things had better clump to one thing (sanity check for the ranker). def rdiv5(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x3 ,11,11,11]]) rank , name , med , iqr 1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x3 , 1100 , 0 (* | ),11.00, 11.00, 11.00, 11.00, 11.00 Lesson Six Some things had better clump to one thing (sanity check for the ranker). def rdiv6(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x4 ,32,33,34,35]]) rank , name , med , iqr 1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 2 , x4 , 3400 , 200 ( | - * ),32.00, 33.00, 34.00, 34.00, 35.00 Lesson Seven All the above scales to succinct summaries of hundreds, thousands, millions of numbers def rdiv7(): rdivDemo([ [ x1 ] + [rand()**0.5 for _ in range(256)], [ x2 ] + [rand()**2 for _ in range(256)], [ x3 ] + [rand() for _ in range(256)] ]) rank , name , med , iqr 1 , x2 , 25 , 50 (-- * -|--------- ), 0.01, 0.09, 0.25, 0.47, 0.86 2 , x3 , 49 , 47 ( ------ *| ------- ), 0.08, 0.29, 0.49, 0.66, 0.89 3 , x1 , 73 , 37 ( ------|- * --- ), 0.32, 0.57, 0.73, 0.86, 0.95 So, How to Rank? For the most part, we are concerned with very high-level issues that strike to the heart of the human condition: What does it mean to find controlling principles in the world? How can we find those principles better, faster, cheaper? But sometimes we have to leave those lofty heights to discuss more pragmatic issues. Specifically, how to present the results of an optimizer and, sometimes, how to compare and rank the results from different optimizers. Note that there is no best way, and often the way we present results depends on our goals, the data we are procesing, and the audience we are trying to reach. So the statistical methods discussed below are more like first-pass approximations to something you may have to change extensively, depending on the task at hand. In any case, in order to have at least one report that that you quickly generate, then.... Theory The test that one optimizer is better than another can be recast as four checks on the distribution of performance scores. Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; The first step is very important. Stats should always be used as sanity checks on intuitions gained by other means. So look at the data before making, possibly bogus, inferences from it. For example, here are some charts showing the effects on a population as we apply more and more of some treatment. Note that the mean of the populations remains unchanged, yet we might still endorse the treatment since it reduces the uncertainty associated with each population. Note that 2 and 3 and 4 must be all be true to assert that one thing generates better numbers than another. For example, one bogus conclusion would be to just check median values (step2) and ignore steps3 and steps4. BAD IDEA . Medians can be very misleading unless you consider the overall distributions (as done in step3 and step4). (As an aside, note that the above requests a check for median , not mean . This is required since, all things considered, means do not mean much, especially for highly skewed distributions. For example, Bill Gates and 35 homeless people are in the same room. Their mean annual income is over a billion dollars each- which is a number that characterized neither Mr. Gates or the homeless people. On the other hand, the median income of that population is close to zero- which is a number that characterizes most of that population. ) In practice, step2,step3,step4 are listed in increasing order of effort (e.g. the bootstrap sample method discussed later in this subject is an example of step4, and this can take a while to compute). So pragmatically, it is useful to explore the above in the order step1 then step2 then step3 then step4 (and stopping along the way if any part fails). For example, one possible bogus inference would be to apply step4 without the step3 since if the small effect test fails, then the third significance test is misleading. For example, returning to the above distributions, note the large overlap in the top two curves in those plots. When distributions exhibit a very large overlap, it is very hard to determine if one is really different to the other. So large variances can mean that even if the means are better , we cannot really say that the values in one distribution are usually better than the other. Step1: Visualization Suppose we had two optimizers which in a 10 repeated runs generated performance from two models: 1: def _tile2(): 2: def show(lst): 3: return xtile(lst,lo=0, hi=1,width=25, 4: show= lambda s:\" %3.2f\" % s) 5: print \"one\", show([0.21, 0.29, 0.28, 0.32, 0.32, 6: 0.28, 0.29, 0.41, 0.42, 0.48]) 7: print \"two\", show([0.71, 0.92, 0.80, 0.79, 0.78, 8: 0.9, 0.71, 0.82, 0.79, 0.98]) When faced with new data, always chant the following mantra: First visualize it to get some intuitions; Then apply some statistics to double check those intuitions. That is, it is strong recommended that, prior doing any statistical work, an analyst generates a visualization of the data. Percentile charts a simple way to display very large populations in very little space. For example, here are our results from one , displayed on a range from 0.00 to 1.00. one * --| , 0.28, 0.29, 0.32, 0.41, 0.48 two | -- * -- , 0.71, 0.79, 0.80, 0.90, 0.98 In this percentile chart, the 2nd and 3rd percentiles as little dashes left and right of the median value, shown with a \"*\" , (learner two 's 3rd percentile is so small that it actually disappears in this display). The vertical bar \"|\" shows half way between the display's min and max (in this case, that would be (0.0+1.00)/2= 0.50) Xtile The advantage of percentile charts is that we can show a lot of data in very little space. For example, here's an example where the xtile Python function shows 2000 numbers on two lines: Quintiles divide the data into the 10th, 30th, 50th, 70th, 90th percentile. Dashes ( \"-\" ) mark the range (10,30)th and (70,90)th percentiles; White space marks the ranges (30,50)th and (50,70)th percentiles. Consider two distributions, of 1000 samples each: one shows square root of a rand() and the other shows the square of a rand() . 10: def _tile() : 11: import random 12: r = random.random 13: def show(lst): 14: return xtile(lst,lo=0, hi=1,width=25, 15: show= lambda s:\" %3.2f\" % s) 16: print \"one\", show([r()*0.5 for x in range(1000)]) 17: print \"two\", show([r()2 for x in range(1000)]) In the following quintile charts, we show these distributions: The range is 0 to 1. One line shows the square of 1000 random numbers; The other line shows the square root of 1000 random numbers; Note the brevity of the display: one -----| * --- , 0.32, 0.55, 0.70, 0.84, 0.95 two -- * |-------- , 0.01, 0.10, 0.27, 0.51, 0.85 As before, the median value, shown with a \"*\" ; and the point half-way between min and max (in this case, 0.5) is shown as a vertical bar \"|\" . Step2: Check Medians The median of a list is the middle item of the sorted values, if the list is of an odd size. If the list size is even, the median is the two values either side of the middle: def median(lst,ordered=False): lst = lst if ordered else sorted(lst) n = len(lst) p = n // 2 if (n % 2): return lst[p] p,q = p-1,p q = max(0,(min(q,n))) return (lst[p] + lst[q]) * 0.5 Step3: Effect size An effect size test is a sanity check that can be summarizes as follows: Don't sweat the small stuff; I.e. ignore small differences between items in the samples. My preferred test for small effect has: a simple intuition; which makes no assumptions about (say) Gaussian assumptions; and which has a solid lineage in the literature. Such a test is Vargha and Delaney 's A12 statistic. The statistic was proposed in Vargha and Delaney's 2000 paper was endorsed in many places including in Acruci and Briad 's ICSE 2011 paper. After I describe it to you, you will wonder why anyone would ever want to use anything else. Given a performance measure seen in m measures of X and n measures of Y , the A12 statistics measures the probability that running algorithm X yields higher values than running another algorithm Y . Specifically, it counts how often we seen larger numbers in X than Y (and if the same numbers are found in both, we add a half mark): a12= #(X.i Y.j) / (n*m) + .5#(X.i == Y.j) / (n*m) According to Vargha and Delaney, a small, medium, large difference between two populations is: large if a12 is over 71%; medium if a12 is over 64%; small if a12 is 56%, or less. A naive version of this code is shown here in the ab12slow function. While simple to code, this ab12slow function runs in polynomial time (since for each item in lst1 , it runs over all of lst2 ): def _ab12(): def a12slow(lst1,lst2): more = same = 0.0 for x in sorted(lst1): for y in sorted(lst2): if x==y : same += 1 elif x y : more += 1 return (more + 0.5*same) / (len(lst1)*len(lst2)) random.seed(1) l1 = [random.random() for x in range(5000)] more = [random.random()*2 for x in range(5000)] l2 = [random.random() for x in range(5000)] less = [random.random()/2.0 for x in range(5000)] for tag, one,two in [( 1less ,l1,more), ( 1more ,more,less),( same ,l1,l2)]: t1 = msecs(lambda : a12(l1,less)) t2 = msecs(lambda : a12slow(l1,less)) print( \\n ,tag, \\n ,t1,a12(one,two)) print(t2, a12slow(one,two)) Note that the test code _ ab12 shows that our fast and slow method generate the same A12 score, but the fast way does so thousands of times faster. The following tests show runtimes for lists of 5000 numbers: experimemt msecs(fast) a12(fast) msecs(slow) a12(slow) 1less 13 0.257 9382 0.257 1more 20 0.868 9869 0.868 same 11 0,502 9937 0.502 Significance Tests Standard Utils Didn't we do this before? Misc functions: rand = random.random any = random.choice seed = random.seed exp = lambda n: math.e**n ln = lambda n: math.log(n,math.e) g = lambda n: round(n,2) def median(lst,ordered=False): if not ordered: lst= sorted(lst) n = len(lst) p = n//2 if n % 2: return lst[p] q = p - 1 q = max(0,min(q,n)) return (lst[p] + lst[q])/2 def msecs(f): import time t1 = time.time() f() return (time.time() - t1) * 1000 def pairs(lst): Return all pairs of items i,i+1 from a list. last=lst[0] for i in lst[1:]: yield last,i last = i def xtile(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star return '('+''.join(out) + ), + pretty(what) def _tileX() : import random random.seed(1) nums = [random.random()**2 for _ in range(100)] print(xtile(nums,lo=0,hi=1.0,width=25,show= %5.2f )) Standard Accumulator for Numbers Note the lt method: this accumulator can be sorted by median values. Warning: this accumulator keeps all numbers. Might be better to use a bounded cache. class Num: An Accumulator for numbers def __init__(i,name,inits=[]): i.n = i.m2 = i.mu = 0.0 i.all=[] i._median=None i.name = name i.rank = 0 for x in inits: i.add(x) def s(i) : return (i.m2/(i.n - 1))**0.5 def add(i,x): i._median=None i.n += 1 i.all += [x] delta = x - i.mu i.mu += delta*1.0/i.n i.m2 += delta*(x - i.mu) def __add__(i,j): return Num(i.name + j.name,i.all + j.all) def quartiles(i): def p(x) : return float(g(xs[x])) i.median() xs = i.all n = int(len(xs)*0.25) return p(n) , p(2*n) , p(3*n) def median(i): if not i._median: i.all = sorted(i.all) i._median=median(i.all) return i._median def __lt__(i,j): return i.median() j.median() def spread(i): i.all=sorted(i.all) n1=i.n*0.25 n2=i.n*0.75 if len(i.all) = 1: return 0 if len(i.all) == 2: return i.all[1] - i.all[0] else: return i.all[int(n2)] - i.all[int(n1)] Cliff's Delta def cliffsDelta(lst1, lst2, **dull): Returns delta and true if there are more than 'dull' differences # if not dull: # dull = {'small': 0.147, 'medium': 0.33, 'large': 0.474} # effect sizes from (Hess and Kromrey, 2004) size = True m, n = len(lst1), len(lst2) lst2 = sorted(lst2) j = more = less = 0 for repeats, x in runs(sorted(lst1)): while j = (n - 1) and lst2[j] x: j += 1 more += j*repeats while j = (n - 1) and lst2[j] == x: j += 1 less += (n - j)*repeats d = (more - less) / (m*n) if abs(d) The.cdelta: size = False return size def lookup_size(delta, dull): :type delta: float :type dull: dict, a dictionary of small, medium, large thresholds. delta = abs(delta) if delta = dull['small']: return False if dull['small'] delta dull['medium']: return True if dull['medium'] = delta dull['large']: return True if delta = dull['large']: return True def runs(lst): Iterator, chunks repeated values for j, two in enumerate(lst): if j == 0: one, i = two, 0 if one != two: yield j - i, one i = j one = two yield j - i + 1, two The A12 Effect Size Test As above def a12slow(lst1,lst2): how often is x in lst1 more than y in lst2? more = same = 0.0 for x in lst1: for y in lst2: if x == y : same += 1 elif x y : more += 1 x= (more + 0.5*same) / (len(lst1)*len(lst2)) return x def a12(lst1,lst2): how often is x in lst1 more than y in lst2? def loop(t,t1,t2): while t1.j t1.n and t2.j t2.n: h1 = t1.l[t1.j] h2 = t2.l[t2.j] h3 = t2.l[t2.j+1] if t2.j+1 t2.n else None if h1 h2: t1.j += 1; t1.gt += t2.n - t2.j elif h1 == h2: if h3 and h1 h3 : t1.gt += t2.n - t2.j - 1 t1.j += 1; t1.eq += 1; t2.eq += 1 else: t2,t1 = t1,t2 return t.gt*1.0, t.eq*1.0 #-------------------------- lst1 = sorted(lst1, reverse=True) lst2 = sorted(lst2, reverse=True) n1 = len(lst1) n2 = len(lst2) t1 = o(l=lst1,j=0,eq=0,gt=0,n=n1) t2 = o(l=lst2,j=0,eq=0,gt=0,n=n2) gt,eq= loop(t1, t1, t2) return gt/(n1*n2) + eq/2/(n1*n2) = The.a12 def _a12(): def f1(): return a12slow(l1,l2) def f2(): return a12(l1,l2) for n in [100,200,400,800,1600,3200,6400]: l1 = [rand() for _ in xrange(n)] l2 = [rand() for _ in xrange(n)] t1 = msecs(f1) t2 = msecs(f2) print(n, g(f1()),g(f2()),int((t1/t2))) n a12(fast) a12(slow) tfast / tslow 100 0.53 0.53 4 200 0.48 0.48 6 400 0.49 0.49 28 800 0.5 0.5 26 1600 0.51 0.51 72 3200 0.49 0.49 109 6400 0.5 0.5 244 ```` Non-Parametric Hypothesis Testing The following bootstrap method was introduced in 1979 by Bradley Efron at Stanford University. It was inspired by earlier work on the jackknife. Improved estimates of the variance were developed later . To check if two populations (y0,z0) are different, many times sample with replacement from both to generate (y1,z1), (y2,z2), (y3,z3) .. etc. def sampleWithReplacement(lst): returns a list same size as list def any(n) : return random.uniform(0,n) def one(lst): return lst[ int(any(len(lst))) ] return [one(lst) for _ in lst] Then, for all those samples, check if some testStatistic in the original pair hold for all the other pairs. If it does more than (say) 99% of the time, then we are 99% confident in that the populations are the same. In such a bootstrap hypothesis test, the some property is the difference between the two populations, muted by the joint standard deviation of the populations. def testStatistic(y,z): Checks if two means are different, tempered by the sample size of 'y' and 'z' tmp1 = tmp2 = 0 for y1 in y.all: tmp1 += (y1 - y.mu)**2 for z1 in z.all: tmp2 += (z1 - z.mu)**2 s1 = (float(tmp1)/(y.n - 1))**0.5 s2 = (float(tmp2)/(z.n - 1))**0.5 delta = z.mu - y.mu if s1+s2: delta = delta/((s1/y.n + s2/z.n)**0.5) return delta The rest is just details: Efron advises to make the mean of the populations the same (see the yhat,zhat stuff shown below). The class total is a just a quick and dirty accumulation class. For more details see the Efron text . def bootstrap(y0,z0,conf=The.conf,b=The.b): The bootstrap hypothesis test from p220 to 223 of Efron's book 'An introduction to the boostrap. class total(): quick and dirty data collector def __init__(i,some=[]): i.sum = i.n = i.mu = 0 ; i.all=[] for one in some: i.put(one) def put(i,x): i.all.append(x); i.sum +=x; i.n += 1; i.mu = float(i.sum)/i.n def __add__(i1,i2): return total(i1.all + i2.all) y, z = total(y0), total(z0) x = y + z tobs = testStatistic(y,z) yhat = [y1 - y.mu + x.mu for y1 in y.all] zhat = [z1 - z.mu + x.mu for z1 in z.all] bigger = 0.0 for i in range(b): if testStatistic(total(sampleWithReplacement(yhat)), total(sampleWithReplacement(zhat))) tobs: bigger += 1 return bigger / b conf Examples def _bootstraped(): def worker(n=1000, mu1=10, sigma1=1, mu2=10.2, sigma2=1): def g(mu,sigma) : return random.gauss(mu,sigma) x = [g(mu1,sigma1) for i in range(n)] y = [g(mu2,sigma2) for i in range(n)] return n,mu1,sigma1,mu2,sigma2,\\ 'different' if bootstrap(x,y) else 'same' # very different means, same std print(worker(mu1=10, sigma1=10, mu2=100, sigma2=10)) # similar means and std print(worker(mu1= 10.1, sigma1=1, mu2= 10.2, sigma2=1)) # slightly different means, same std print(worker(mu1= 10.1, sigma1= 1, mu2= 10.8, sigma2= 1)) # different in mu eater by large std print(worker(mu1= 10.1, sigma1= 10, mu2= 10.8, sigma2= 1)) Output: #_bootstraped() (1000, 10, 10, 100, 10, 'different') (1000, 10.1, 1, 10.2, 1, 'same') (1000, 10.1, 1, 10.8, 1, 'different') (1000, 10.1, 10, 10.8, 1, 'same') Warning- the above took 8 seconds to generate since we used 1000 bootstraps. As to how many bootstraps are enough, that depends on the data. There are results saying 200 to 400 are enough but, since I am suspicious man, I run it for 1000. Which means the runtimes associated with bootstrapping is a significant issue. To reduce that runtime, I avoid things like an all-pairs comparison of all treatments (see below: Scott-knott). Also, BEFORE I do the boostrap, I first run the effect size test (and only go to bootstrapping in effect size passes: def different(l1,l2): #return bootstrap(l1,l2) and a12(l2,l1) #return a12(l2,l1) and bootstrap(l1,l2) if The.useA12: return a12(l2,l1) and bootstrap(l1,l2) else: return cliffsDelta(l1,l2) and bootstrap(l1,l2) Saner Hypothesis Testing The following code, which you should use verbatim does the following: All treatments are clustered into ranks . In practice, dozens of treatments end up generating just a handful of ranks. The numbers of calls to the hypothesis tests are minimized: Treatments are sorted by their median value. Treatments are divided into two groups such that the expected value of the mean values after the split is minimized; Hypothesis tests are called to test if the two groups are truly difference. All hypothesis tests are non-parametric and include (1) effect size tests and (2) tests for statistically significant numbers; Slow bootstraps are executed if the faster A12 tests are passed; In practice, this means that the hypothesis tests (with confidence of say, 95%) are called on only a logarithmic number of times. So... With this method, 16 treatments can be studied using less than 1,2,4,8,16 log 2 i =15 hypothesis tests and confidence 0.99 15 =0.86 . But if did this with the 120 all-pairs comparisons of the 16 treatments, we would have total confidence _0.99 120 =0.30. For examples on using this code, see rdivDemo (below). def scottknott(data,cohen=The.cohen,small=The.small,useA12=The.a12 0, epsilon=The.epsilon): Recursively split data, maximizing delta of the expected value of the mean before and after the splits. Reject splits with under 3 items all = reduce(lambda x,y:x+y,data) same = lambda l,r: abs(l.median() - r.median()) = all.s()*cohen if useA12: same = lambda l, r: not different(l.all,r.all) big = lambda n: n small return rdiv(data,all,minMu,big,same,epsilon) def rdiv(data, # a list of class Nums all, # all the data combined into one num div, # function: find the best split big, # function: rejects small splits same, # function: rejects similar splits epsilon): # small enough to split two parts Looks for ways to split sorted data, Recurses into each split. Assigns a 'rank' number to all the leaf splits found in this way. def recurse(parts,all,rank=0): Split, then recurse on each part. cut,left,right = maybeIgnore(div(parts,all,big,epsilon), same,parts) if cut: # if cut, rank right higher than left rank = recurse(parts[:cut],left,rank) + 1 rank = recurse(parts[cut:],right,rank) else: # if no cut, then all get same rank for part in parts: part.rank = rank return rank recurse(sorted(data),all) return data def maybeIgnore((cut,left,right), same,parts): if cut: if same(sum(parts[:cut],Num('upto')), sum(parts[cut:],Num('above'))): cut = left = right = None return cut,left,right def minMu(parts,all,big,epsilon): Find a cut in the parts that maximizes the expected value of the difference in the mean before and after the cut. Reject splits that are insignificantly different or that generate very small subsets. cut,left,right = None,None,None before, mu = 0, all.mu for i,l,r in leftRight(parts,epsilon): if big(l.n) and big(r.n): n = all.n * 1.0 now = l.n/n*(mu- l.mu)**2 + r.n/n*(mu- r.mu)**2 if now before: before,cut,left,right = now,i,l,r return cut,left,right def leftRight(parts,epsilon=The.epsilon): Iterator. For all items in 'parts', return everything to the left and everything from here to the end. For reasons of efficiency, take a first pass over the data to pre-compute and cache right-hand-sides rights = {} n = j = len(parts) - 1 while j 0: rights[j] = parts[j] if j n: rights[j] += rights[j+1] j -=1 left = parts[0] for i,one in enumerate(parts): if i 0: if parts[i]._median - parts[i-1]._median epsilon: yield i,left,rights[i] left += one Putting it All Together Driver for the demos: def rdivDemo(data,latex = True): def zzz(x): return int(100 * (x - lo) / (hi - lo + 0.00001)) data = map(lambda lst:Num(lst[0],lst[1:]), data) print( ) ranks=[] for x in scottknott(data,useA12=True): ranks += [(x.rank,x.median(),x)] all=[] for _,__,x in sorted(ranks): all += x.all all = sorted(all) lo, hi = all[0], all[-1] line = ---------------------------------------------------- last = None formatStr = '%%4s , %%%ss , %%s , %%4s ' % The.text # print((formatStr % \\ # ('rank', 'name', 'med', 'iqr')) + \\n + line) if latex: latexPrint(ranks,all) for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() print((formatStr % \\ (x.rank+1, x.name, q2, q3 - q1)) + \\ xtile(x.all,lo=lo,hi=hi,width=30,show= %5.2f )) last = x.rank def _rdivs(): seed(1) rdiv0(); rdiv1(); rdiv2(); rdiv3(); rdiv5(); rdiv6(); print( ### ); rdiv7() def latexPrint(ranks,all): print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%Latex Table%%%%%%%%%%%%%%%%%%%%%%%%% ) print( \\\\begin{figure}[!t] {\\small {\\small \\\\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} \\\\arrayrulecolor{darkgray} \\\\rowcolor[gray]{.9} rank treatment median IQR \\\\\\\\ ) lo, hi = all[0], all[-1] for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() q1,q2,q3 = q1*100,q2*100,q3*100 print( %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\\\\\ % (x.rank+1,x.name.replace('_','\\_'),q2,q3-q1,q1,q2-q1,q3,q3-q2)) last = x.rank print( \\end{tabular}} } \\\\caption{%%%Enter Caption%%% }\\\\label{fig:my fig} \\\\end{figure} ) print( %%%%%%%%%%%%%%%%%%%%%%%End Latex Table%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) def xtile_modified(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star print(what) return what #################################### def thing(x): Numbers become numbers; every other x is a symbol. try: return int(x) except ValueError: try: return float(x) except ValueError: return x def main(): log=None latex = The.latex all={} now=[] for line in sys.stdin: for word in line.split(): word = thing(word) if isinstance(word,str): now = all[word] = all.get(word,[]) else: now += [word] rdivDemo( [ [k] + v for k,v in all.items() ],latex) if args.demo: _rdivs() else: main() #print( \\begin{figure}[!t] # #{\\small # #{\\small \\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} #\\arrayrulecolor{darkgray} #\\rowcolor[gray]{.9} rank treatment median IQR #%min= 20, max= 117 #\\\\ + # %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\ #+ \\end{tabular}} # #% :learn 4.64 :analyze 1.69 :boots 3 effects 5 :conf 0.970299 # #} #\\caption{COCOMO vs just lines #of code. SE values seen in #leave-one-studies, repeated ten times. #For each of the four tables in this figure, #{\\em better} methods appear {\\em higher} in the tables. #In these tables, #median and IQR are the 50th and the #(75-25)th percentiles. The IQR range is #shown in the right column #with black dot at the median. Horizontal lines #divide the ``ranks'' found by our Scott-Knott+bootstrapping+effect size tests (shown in left column). #}\\label{fig:loc} #\\end{figure} )","title":"Stats.py"},{"location":"proj/stats/#analysis-of-experimental-data","text":"This page is about the non-parametric statistical tests. It is also a chance for us to discuss a little statistical theory.","title":"Analysis of Experimental Data"},{"location":"proj/stats/#before-we-begin","text":"Imagine the following example contain objective scores gained from different optimizers x1,x2,x3,x4,...etc . Which results are ranked one, two, three etc...","title":"Before we begin..."},{"location":"proj/stats/#lesson-zero","text":"Some differences are obvious def rdiv0(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,6, 7, 8, 9] ])","title":"Lesson Zero"},{"location":"proj/stats/#rank-name-med-iqr","text":"1 , x1 , 51 , 11 ( | ), 0.34, 0.49, 0.51, 0.51, 0.60 2 , x2 , 800 , 200 ( | ---- -- ), 6.00, 7.00, 8.00, 8.00, 9.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-one","text":"Some similarities are obvious... def rdiv1(): rdivDemo([ [ x1 ,0.1, 0.2, 0.3, 0.4], [ x2 ,0.1, 0.2, 0.3, 0.4], [ x3 ,6, 7, 8, 9] ])","title":"Lesson One"},{"location":"proj/stats/#rank-name-med-iqr_1","text":"1 , x1 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x2 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 2 , x3 , 800 , 200 ( | ---- *-- ), 6.00, 7.00, 8.00, 8.00, 9.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-two","text":"Many results often clump into less-than-many ranks. def rdiv2(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,0.6, 0.7, 0.8, 0.9], [ x3 ,0.15, 0.25, 0.4, 0.35], [ x4 ,0.6, 0.7, 0.8, 0.9], [ x5 ,0.1, 0.2, 0.3, 0.4] ])","title":"Lesson Two"},{"location":"proj/stats/#rank-name-med-iqr_2","text":"1 , x5 , 30 , 20 (--- --- | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x3 , 35 , 15 ( ---- - | ), 0.15, 0.25, 0.35, 0.35, 0.40 2 , x1 , 51 , 11 ( ------ -- ), 0.34, 0.49, 0.51, 0.51, 0.60 3 , x2 , 80 , 20 ( | ---- -- ), 0.60, 0.70, 0.80, 0.80, 0.90 3 , x4 , 80 , 20 ( | ---- *-- ), 0.60, 0.70, 0.80, 0.80, 0.90","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-three","text":"Some results even clump into one rank (the great null result). def rdiv3(): rdivDemo([ [ x1 ,101, 100, 99, 101, 99.5], [ x2 ,101, 100, 99, 101, 100], [ x3 ,101, 100, 99.5, 101, 99], [ x4 ,101, 100, 99, 101, 100] ])","title":"Lesson Three"},{"location":"proj/stats/#rank-name-med-iqr_3","text":"1 , x1 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x2 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 1 , x3 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x4 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-four","text":"Heh? Where's lesson four?","title":"Lesson Four"},{"location":"proj/stats/#lesson-five","text":"Some things had better clump to one thing (sanity check for the ranker). def rdiv5(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x3 ,11,11,11]])","title":"Lesson Five"},{"location":"proj/stats/#rank-name-med-iqr_4","text":"1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x3 , 1100 , 0 (* | ),11.00, 11.00, 11.00, 11.00, 11.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-six","text":"Some things had better clump to one thing (sanity check for the ranker). def rdiv6(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x4 ,32,33,34,35]])","title":"Lesson Six"},{"location":"proj/stats/#rank-name-med-iqr_5","text":"1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 2 , x4 , 3400 , 200 ( | - * ),32.00, 33.00, 34.00, 34.00, 35.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-seven","text":"All the above scales to succinct summaries of hundreds, thousands, millions of numbers def rdiv7(): rdivDemo([ [ x1 ] + [rand()**0.5 for _ in range(256)], [ x2 ] + [rand()**2 for _ in range(256)], [ x3 ] + [rand() for _ in range(256)] ])","title":"Lesson Seven"},{"location":"proj/stats/#rank-name-med-iqr_6","text":"1 , x2 , 25 , 50 (-- * -|--------- ), 0.01, 0.09, 0.25, 0.47, 0.86 2 , x3 , 49 , 47 ( ------ *| ------- ), 0.08, 0.29, 0.49, 0.66, 0.89 3 , x1 , 73 , 37 ( ------|- * --- ), 0.32, 0.57, 0.73, 0.86, 0.95","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#so-how-to-rank","text":"For the most part, we are concerned with very high-level issues that strike to the heart of the human condition: What does it mean to find controlling principles in the world? How can we find those principles better, faster, cheaper? But sometimes we have to leave those lofty heights to discuss more pragmatic issues. Specifically, how to present the results of an optimizer and, sometimes, how to compare and rank the results from different optimizers. Note that there is no best way, and often the way we present results depends on our goals, the data we are procesing, and the audience we are trying to reach. So the statistical methods discussed below are more like first-pass approximations to something you may have to change extensively, depending on the task at hand. In any case, in order to have at least one report that that you quickly generate, then....","title":"So, How to Rank?"},{"location":"proj/stats/#theory","text":"The test that one optimizer is better than another can be recast as four checks on the distribution of performance scores. Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; The first step is very important. Stats should always be used as sanity checks on intuitions gained by other means. So look at the data before making, possibly bogus, inferences from it. For example, here are some charts showing the effects on a population as we apply more and more of some treatment. Note that the mean of the populations remains unchanged, yet we might still endorse the treatment since it reduces the uncertainty associated with each population. Note that 2 and 3 and 4 must be all be true to assert that one thing generates better numbers than another. For example, one bogus conclusion would be to just check median values (step2) and ignore steps3 and steps4. BAD IDEA . Medians can be very misleading unless you consider the overall distributions (as done in step3 and step4). (As an aside, note that the above requests a check for median , not mean . This is required since, all things considered, means do not mean much, especially for highly skewed distributions. For example, Bill Gates and 35 homeless people are in the same room. Their mean annual income is over a billion dollars each- which is a number that characterized neither Mr. Gates or the homeless people. On the other hand, the median income of that population is close to zero- which is a number that characterizes most of that population. ) In practice, step2,step3,step4 are listed in increasing order of effort (e.g. the bootstrap sample method discussed later in this subject is an example of step4, and this can take a while to compute). So pragmatically, it is useful to explore the above in the order step1 then step2 then step3 then step4 (and stopping along the way if any part fails). For example, one possible bogus inference would be to apply step4 without the step3 since if the small effect test fails, then the third significance test is misleading. For example, returning to the above distributions, note the large overlap in the top two curves in those plots. When distributions exhibit a very large overlap, it is very hard to determine if one is really different to the other. So large variances can mean that even if the means are better , we cannot really say that the values in one distribution are usually better than the other.","title":"Theory"},{"location":"proj/stats/#step1-visualization","text":"Suppose we had two optimizers which in a 10 repeated runs generated performance from two models: 1: def _tile2(): 2: def show(lst): 3: return xtile(lst,lo=0, hi=1,width=25, 4: show= lambda s:\" %3.2f\" % s) 5: print \"one\", show([0.21, 0.29, 0.28, 0.32, 0.32, 6: 0.28, 0.29, 0.41, 0.42, 0.48]) 7: print \"two\", show([0.71, 0.92, 0.80, 0.79, 0.78, 8: 0.9, 0.71, 0.82, 0.79, 0.98]) When faced with new data, always chant the following mantra: First visualize it to get some intuitions; Then apply some statistics to double check those intuitions. That is, it is strong recommended that, prior doing any statistical work, an analyst generates a visualization of the data. Percentile charts a simple way to display very large populations in very little space. For example, here are our results from one , displayed on a range from 0.00 to 1.00. one * --| , 0.28, 0.29, 0.32, 0.41, 0.48 two | -- * -- , 0.71, 0.79, 0.80, 0.90, 0.98 In this percentile chart, the 2nd and 3rd percentiles as little dashes left and right of the median value, shown with a \"*\" , (learner two 's 3rd percentile is so small that it actually disappears in this display). The vertical bar \"|\" shows half way between the display's min and max (in this case, that would be (0.0+1.00)/2= 0.50)","title":"Step1: Visualization"},{"location":"proj/stats/#xtile","text":"The advantage of percentile charts is that we can show a lot of data in very little space. For example, here's an example where the xtile Python function shows 2000 numbers on two lines: Quintiles divide the data into the 10th, 30th, 50th, 70th, 90th percentile. Dashes ( \"-\" ) mark the range (10,30)th and (70,90)th percentiles; White space marks the ranges (30,50)th and (50,70)th percentiles. Consider two distributions, of 1000 samples each: one shows square root of a rand() and the other shows the square of a rand() . 10: def _tile() : 11: import random 12: r = random.random 13: def show(lst): 14: return xtile(lst,lo=0, hi=1,width=25, 15: show= lambda s:\" %3.2f\" % s) 16: print \"one\", show([r()*0.5 for x in range(1000)]) 17: print \"two\", show([r()2 for x in range(1000)]) In the following quintile charts, we show these distributions: The range is 0 to 1. One line shows the square of 1000 random numbers; The other line shows the square root of 1000 random numbers; Note the brevity of the display: one -----| * --- , 0.32, 0.55, 0.70, 0.84, 0.95 two -- * |-------- , 0.01, 0.10, 0.27, 0.51, 0.85 As before, the median value, shown with a \"*\" ; and the point half-way between min and max (in this case, 0.5) is shown as a vertical bar \"|\" .","title":"Xtile"},{"location":"proj/stats/#step2-check-medians","text":"The median of a list is the middle item of the sorted values, if the list is of an odd size. If the list size is even, the median is the two values either side of the middle: def median(lst,ordered=False): lst = lst if ordered else sorted(lst) n = len(lst) p = n // 2 if (n % 2): return lst[p] p,q = p-1,p q = max(0,(min(q,n))) return (lst[p] + lst[q]) * 0.5","title":"Step2: Check Medians"},{"location":"proj/stats/#step3-effect-size","text":"An effect size test is a sanity check that can be summarizes as follows: Don't sweat the small stuff; I.e. ignore small differences between items in the samples. My preferred test for small effect has: a simple intuition; which makes no assumptions about (say) Gaussian assumptions; and which has a solid lineage in the literature. Such a test is Vargha and Delaney 's A12 statistic. The statistic was proposed in Vargha and Delaney's 2000 paper was endorsed in many places including in Acruci and Briad 's ICSE 2011 paper. After I describe it to you, you will wonder why anyone would ever want to use anything else. Given a performance measure seen in m measures of X and n measures of Y , the A12 statistics measures the probability that running algorithm X yields higher values than running another algorithm Y . Specifically, it counts how often we seen larger numbers in X than Y (and if the same numbers are found in both, we add a half mark): a12= #(X.i Y.j) / (n*m) + .5#(X.i == Y.j) / (n*m) According to Vargha and Delaney, a small, medium, large difference between two populations is: large if a12 is over 71%; medium if a12 is over 64%; small if a12 is 56%, or less. A naive version of this code is shown here in the ab12slow function. While simple to code, this ab12slow function runs in polynomial time (since for each item in lst1 , it runs over all of lst2 ): def _ab12(): def a12slow(lst1,lst2): more = same = 0.0 for x in sorted(lst1): for y in sorted(lst2): if x==y : same += 1 elif x y : more += 1 return (more + 0.5*same) / (len(lst1)*len(lst2)) random.seed(1) l1 = [random.random() for x in range(5000)] more = [random.random()*2 for x in range(5000)] l2 = [random.random() for x in range(5000)] less = [random.random()/2.0 for x in range(5000)] for tag, one,two in [( 1less ,l1,more), ( 1more ,more,less),( same ,l1,l2)]: t1 = msecs(lambda : a12(l1,less)) t2 = msecs(lambda : a12slow(l1,less)) print( \\n ,tag, \\n ,t1,a12(one,two)) print(t2, a12slow(one,two)) Note that the test code _ ab12 shows that our fast and slow method generate the same A12 score, but the fast way does so thousands of times faster. The following tests show runtimes for lists of 5000 numbers: experimemt msecs(fast) a12(fast) msecs(slow) a12(slow) 1less 13 0.257 9382 0.257 1more 20 0.868 9869 0.868 same 11 0,502 9937 0.502","title":"Step3: Effect size"},{"location":"proj/stats/#significance-tests","text":"","title":"Significance Tests"},{"location":"proj/stats/#standard-utils","text":"Didn't we do this before? Misc functions: rand = random.random any = random.choice seed = random.seed exp = lambda n: math.e**n ln = lambda n: math.log(n,math.e) g = lambda n: round(n,2) def median(lst,ordered=False): if not ordered: lst= sorted(lst) n = len(lst) p = n//2 if n % 2: return lst[p] q = p - 1 q = max(0,min(q,n)) return (lst[p] + lst[q])/2 def msecs(f): import time t1 = time.time() f() return (time.time() - t1) * 1000 def pairs(lst): Return all pairs of items i,i+1 from a list. last=lst[0] for i in lst[1:]: yield last,i last = i def xtile(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star return '('+''.join(out) + ), + pretty(what) def _tileX() : import random random.seed(1) nums = [random.random()**2 for _ in range(100)] print(xtile(nums,lo=0,hi=1.0,width=25,show= %5.2f ))","title":"Standard Utils"},{"location":"proj/stats/#standard-accumulator-for-numbers","text":"Note the lt method: this accumulator can be sorted by median values. Warning: this accumulator keeps all numbers. Might be better to use a bounded cache. class Num: An Accumulator for numbers def __init__(i,name,inits=[]): i.n = i.m2 = i.mu = 0.0 i.all=[] i._median=None i.name = name i.rank = 0 for x in inits: i.add(x) def s(i) : return (i.m2/(i.n - 1))**0.5 def add(i,x): i._median=None i.n += 1 i.all += [x] delta = x - i.mu i.mu += delta*1.0/i.n i.m2 += delta*(x - i.mu) def __add__(i,j): return Num(i.name + j.name,i.all + j.all) def quartiles(i): def p(x) : return float(g(xs[x])) i.median() xs = i.all n = int(len(xs)*0.25) return p(n) , p(2*n) , p(3*n) def median(i): if not i._median: i.all = sorted(i.all) i._median=median(i.all) return i._median def __lt__(i,j): return i.median() j.median() def spread(i): i.all=sorted(i.all) n1=i.n*0.25 n2=i.n*0.75 if len(i.all) = 1: return 0 if len(i.all) == 2: return i.all[1] - i.all[0] else: return i.all[int(n2)] - i.all[int(n1)]","title":"Standard Accumulator for Numbers"},{"location":"proj/stats/#cliffs-delta","text":"def cliffsDelta(lst1, lst2, **dull): Returns delta and true if there are more than 'dull' differences # if not dull: # dull = {'small': 0.147, 'medium': 0.33, 'large': 0.474} # effect sizes from (Hess and Kromrey, 2004) size = True m, n = len(lst1), len(lst2) lst2 = sorted(lst2) j = more = less = 0 for repeats, x in runs(sorted(lst1)): while j = (n - 1) and lst2[j] x: j += 1 more += j*repeats while j = (n - 1) and lst2[j] == x: j += 1 less += (n - j)*repeats d = (more - less) / (m*n) if abs(d) The.cdelta: size = False return size def lookup_size(delta, dull): :type delta: float :type dull: dict, a dictionary of small, medium, large thresholds. delta = abs(delta) if delta = dull['small']: return False if dull['small'] delta dull['medium']: return True if dull['medium'] = delta dull['large']: return True if delta = dull['large']: return True def runs(lst): Iterator, chunks repeated values for j, two in enumerate(lst): if j == 0: one, i = two, 0 if one != two: yield j - i, one i = j one = two yield j - i + 1, two","title":"Cliff's Delta"},{"location":"proj/stats/#the-a12-effect-size-test","text":"As above def a12slow(lst1,lst2): how often is x in lst1 more than y in lst2? more = same = 0.0 for x in lst1: for y in lst2: if x == y : same += 1 elif x y : more += 1 x= (more + 0.5*same) / (len(lst1)*len(lst2)) return x def a12(lst1,lst2): how often is x in lst1 more than y in lst2? def loop(t,t1,t2): while t1.j t1.n and t2.j t2.n: h1 = t1.l[t1.j] h2 = t2.l[t2.j] h3 = t2.l[t2.j+1] if t2.j+1 t2.n else None if h1 h2: t1.j += 1; t1.gt += t2.n - t2.j elif h1 == h2: if h3 and h1 h3 : t1.gt += t2.n - t2.j - 1 t1.j += 1; t1.eq += 1; t2.eq += 1 else: t2,t1 = t1,t2 return t.gt*1.0, t.eq*1.0 #-------------------------- lst1 = sorted(lst1, reverse=True) lst2 = sorted(lst2, reverse=True) n1 = len(lst1) n2 = len(lst2) t1 = o(l=lst1,j=0,eq=0,gt=0,n=n1) t2 = o(l=lst2,j=0,eq=0,gt=0,n=n2) gt,eq= loop(t1, t1, t2) return gt/(n1*n2) + eq/2/(n1*n2) = The.a12 def _a12(): def f1(): return a12slow(l1,l2) def f2(): return a12(l1,l2) for n in [100,200,400,800,1600,3200,6400]: l1 = [rand() for _ in xrange(n)] l2 = [rand() for _ in xrange(n)] t1 = msecs(f1) t2 = msecs(f2) print(n, g(f1()),g(f2()),int((t1/t2))) n a12(fast) a12(slow) tfast / tslow 100 0.53 0.53 4 200 0.48 0.48 6 400 0.49 0.49 28 800 0.5 0.5 26 1600 0.51 0.51 72 3200 0.49 0.49 109 6400 0.5 0.5 244 ````","title":"The A12 Effect Size Test"},{"location":"proj/stats/#non-parametric-hypothesis-testing","text":"The following bootstrap method was introduced in 1979 by Bradley Efron at Stanford University. It was inspired by earlier work on the jackknife. Improved estimates of the variance were developed later . To check if two populations (y0,z0) are different, many times sample with replacement from both to generate (y1,z1), (y2,z2), (y3,z3) .. etc. def sampleWithReplacement(lst): returns a list same size as list def any(n) : return random.uniform(0,n) def one(lst): return lst[ int(any(len(lst))) ] return [one(lst) for _ in lst] Then, for all those samples, check if some testStatistic in the original pair hold for all the other pairs. If it does more than (say) 99% of the time, then we are 99% confident in that the populations are the same. In such a bootstrap hypothesis test, the some property is the difference between the two populations, muted by the joint standard deviation of the populations. def testStatistic(y,z): Checks if two means are different, tempered by the sample size of 'y' and 'z' tmp1 = tmp2 = 0 for y1 in y.all: tmp1 += (y1 - y.mu)**2 for z1 in z.all: tmp2 += (z1 - z.mu)**2 s1 = (float(tmp1)/(y.n - 1))**0.5 s2 = (float(tmp2)/(z.n - 1))**0.5 delta = z.mu - y.mu if s1+s2: delta = delta/((s1/y.n + s2/z.n)**0.5) return delta The rest is just details: Efron advises to make the mean of the populations the same (see the yhat,zhat stuff shown below). The class total is a just a quick and dirty accumulation class. For more details see the Efron text . def bootstrap(y0,z0,conf=The.conf,b=The.b): The bootstrap hypothesis test from p220 to 223 of Efron's book 'An introduction to the boostrap. class total(): quick and dirty data collector def __init__(i,some=[]): i.sum = i.n = i.mu = 0 ; i.all=[] for one in some: i.put(one) def put(i,x): i.all.append(x); i.sum +=x; i.n += 1; i.mu = float(i.sum)/i.n def __add__(i1,i2): return total(i1.all + i2.all) y, z = total(y0), total(z0) x = y + z tobs = testStatistic(y,z) yhat = [y1 - y.mu + x.mu for y1 in y.all] zhat = [z1 - z.mu + x.mu for z1 in z.all] bigger = 0.0 for i in range(b): if testStatistic(total(sampleWithReplacement(yhat)), total(sampleWithReplacement(zhat))) tobs: bigger += 1 return bigger / b conf","title":"Non-Parametric Hypothesis Testing"},{"location":"proj/stats/#examples","text":"def _bootstraped(): def worker(n=1000, mu1=10, sigma1=1, mu2=10.2, sigma2=1): def g(mu,sigma) : return random.gauss(mu,sigma) x = [g(mu1,sigma1) for i in range(n)] y = [g(mu2,sigma2) for i in range(n)] return n,mu1,sigma1,mu2,sigma2,\\ 'different' if bootstrap(x,y) else 'same' # very different means, same std print(worker(mu1=10, sigma1=10, mu2=100, sigma2=10)) # similar means and std print(worker(mu1= 10.1, sigma1=1, mu2= 10.2, sigma2=1)) # slightly different means, same std print(worker(mu1= 10.1, sigma1= 1, mu2= 10.8, sigma2= 1)) # different in mu eater by large std print(worker(mu1= 10.1, sigma1= 10, mu2= 10.8, sigma2= 1)) Output: #_bootstraped() (1000, 10, 10, 100, 10, 'different') (1000, 10.1, 1, 10.2, 1, 'same') (1000, 10.1, 1, 10.8, 1, 'different') (1000, 10.1, 10, 10.8, 1, 'same') Warning- the above took 8 seconds to generate since we used 1000 bootstraps. As to how many bootstraps are enough, that depends on the data. There are results saying 200 to 400 are enough but, since I am suspicious man, I run it for 1000. Which means the runtimes associated with bootstrapping is a significant issue. To reduce that runtime, I avoid things like an all-pairs comparison of all treatments (see below: Scott-knott). Also, BEFORE I do the boostrap, I first run the effect size test (and only go to bootstrapping in effect size passes: def different(l1,l2): #return bootstrap(l1,l2) and a12(l2,l1) #return a12(l2,l1) and bootstrap(l1,l2) if The.useA12: return a12(l2,l1) and bootstrap(l1,l2) else: return cliffsDelta(l1,l2) and bootstrap(l1,l2)","title":"Examples"},{"location":"proj/stats/#saner-hypothesis-testing","text":"The following code, which you should use verbatim does the following: All treatments are clustered into ranks . In practice, dozens of treatments end up generating just a handful of ranks. The numbers of calls to the hypothesis tests are minimized: Treatments are sorted by their median value. Treatments are divided into two groups such that the expected value of the mean values after the split is minimized; Hypothesis tests are called to test if the two groups are truly difference. All hypothesis tests are non-parametric and include (1) effect size tests and (2) tests for statistically significant numbers; Slow bootstraps are executed if the faster A12 tests are passed; In practice, this means that the hypothesis tests (with confidence of say, 95%) are called on only a logarithmic number of times. So... With this method, 16 treatments can be studied using less than 1,2,4,8,16 log 2 i =15 hypothesis tests and confidence 0.99 15 =0.86 . But if did this with the 120 all-pairs comparisons of the 16 treatments, we would have total confidence _0.99 120 =0.30. For examples on using this code, see rdivDemo (below). def scottknott(data,cohen=The.cohen,small=The.small,useA12=The.a12 0, epsilon=The.epsilon): Recursively split data, maximizing delta of the expected value of the mean before and after the splits. Reject splits with under 3 items all = reduce(lambda x,y:x+y,data) same = lambda l,r: abs(l.median() - r.median()) = all.s()*cohen if useA12: same = lambda l, r: not different(l.all,r.all) big = lambda n: n small return rdiv(data,all,minMu,big,same,epsilon) def rdiv(data, # a list of class Nums all, # all the data combined into one num div, # function: find the best split big, # function: rejects small splits same, # function: rejects similar splits epsilon): # small enough to split two parts Looks for ways to split sorted data, Recurses into each split. Assigns a 'rank' number to all the leaf splits found in this way. def recurse(parts,all,rank=0): Split, then recurse on each part. cut,left,right = maybeIgnore(div(parts,all,big,epsilon), same,parts) if cut: # if cut, rank right higher than left rank = recurse(parts[:cut],left,rank) + 1 rank = recurse(parts[cut:],right,rank) else: # if no cut, then all get same rank for part in parts: part.rank = rank return rank recurse(sorted(data),all) return data def maybeIgnore((cut,left,right), same,parts): if cut: if same(sum(parts[:cut],Num('upto')), sum(parts[cut:],Num('above'))): cut = left = right = None return cut,left,right def minMu(parts,all,big,epsilon): Find a cut in the parts that maximizes the expected value of the difference in the mean before and after the cut. Reject splits that are insignificantly different or that generate very small subsets. cut,left,right = None,None,None before, mu = 0, all.mu for i,l,r in leftRight(parts,epsilon): if big(l.n) and big(r.n): n = all.n * 1.0 now = l.n/n*(mu- l.mu)**2 + r.n/n*(mu- r.mu)**2 if now before: before,cut,left,right = now,i,l,r return cut,left,right def leftRight(parts,epsilon=The.epsilon): Iterator. For all items in 'parts', return everything to the left and everything from here to the end. For reasons of efficiency, take a first pass over the data to pre-compute and cache right-hand-sides rights = {} n = j = len(parts) - 1 while j 0: rights[j] = parts[j] if j n: rights[j] += rights[j+1] j -=1 left = parts[0] for i,one in enumerate(parts): if i 0: if parts[i]._median - parts[i-1]._median epsilon: yield i,left,rights[i] left += one","title":"Saner Hypothesis Testing"},{"location":"proj/stats/#putting-it-all-together","text":"Driver for the demos: def rdivDemo(data,latex = True): def zzz(x): return int(100 * (x - lo) / (hi - lo + 0.00001)) data = map(lambda lst:Num(lst[0],lst[1:]), data) print( ) ranks=[] for x in scottknott(data,useA12=True): ranks += [(x.rank,x.median(),x)] all=[] for _,__,x in sorted(ranks): all += x.all all = sorted(all) lo, hi = all[0], all[-1] line = ---------------------------------------------------- last = None formatStr = '%%4s , %%%ss , %%s , %%4s ' % The.text # print((formatStr % \\ # ('rank', 'name', 'med', 'iqr')) + \\n + line) if latex: latexPrint(ranks,all) for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() print((formatStr % \\ (x.rank+1, x.name, q2, q3 - q1)) + \\ xtile(x.all,lo=lo,hi=hi,width=30,show= %5.2f )) last = x.rank def _rdivs(): seed(1) rdiv0(); rdiv1(); rdiv2(); rdiv3(); rdiv5(); rdiv6(); print( ### ); rdiv7() def latexPrint(ranks,all): print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%Latex Table%%%%%%%%%%%%%%%%%%%%%%%%% ) print( \\\\begin{figure}[!t] {\\small {\\small \\\\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} \\\\arrayrulecolor{darkgray} \\\\rowcolor[gray]{.9} rank treatment median IQR \\\\\\\\ ) lo, hi = all[0], all[-1] for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() q1,q2,q3 = q1*100,q2*100,q3*100 print( %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\\\\\ % (x.rank+1,x.name.replace('_','\\_'),q2,q3-q1,q1,q2-q1,q3,q3-q2)) last = x.rank print( \\end{tabular}} } \\\\caption{%%%Enter Caption%%% }\\\\label{fig:my fig} \\\\end{figure} ) print( %%%%%%%%%%%%%%%%%%%%%%%End Latex Table%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) def xtile_modified(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star print(what) return what #################################### def thing(x): Numbers become numbers; every other x is a symbol. try: return int(x) except ValueError: try: return float(x) except ValueError: return x def main(): log=None latex = The.latex all={} now=[] for line in sys.stdin: for word in line.split(): word = thing(word) if isinstance(word,str): now = all[word] = all.get(word,[]) else: now += [word] rdivDemo( [ [k] + v for k,v in all.items() ],latex) if args.demo: _rdivs() else: main() #print( \\begin{figure}[!t] # #{\\small # #{\\small \\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} #\\arrayrulecolor{darkgray} #\\rowcolor[gray]{.9} rank treatment median IQR #%min= 20, max= 117 #\\\\ + # %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\ #+ \\end{tabular}} # #% :learn 4.64 :analyze 1.69 :boots 3 effects 5 :conf 0.970299 # #} #\\caption{COCOMO vs just lines #of code. SE values seen in #leave-one-studies, repeated ten times. #For each of the four tables in this figure, #{\\em better} methods appear {\\em higher} in the tables. #In these tables, #median and IQR are the 50th and the #(75-25)th percentiles. The IQR range is #shown in the right column #with black dot at the median. Horizontal lines #divide the ``ranks'' found by our Scott-Knott+bootstrapping+effect size tests (shown in left column). #}\\label{fig:loc} #\\end{figure} )","title":"Putting it All Together"},{"location":"proj/w1/","text":"Homework Week1, Week2 Todo Create a public Github repo (NOT in NC State Github, but in the other one) Add \"timm\" as a team member to that repo. How? go the repo's organization's settings on left-hand-side menu go to Collaberators and teams then Enter \"timm\" under \"Collaborators\". Start a file with the following header (containing class O ). For Week1, address the Python101 task. For Week2, address the Table reader task. Commit the code (w1.py, w2.py) and a transcript of the output (called w1.txt, w2.txt) to a sub-directory in your repo called w12 . Paste a link to that directory in the commit sheet. A Simple Unit Test Rig (in Python) import re,traceback class O: y=n=0 @staticmethod def report(): print( \\n# pass= %s fail= %s %%pass = %s%% % ( O.y,O.n, int(round(O.y*100/(O.y+O.n+0.001))))) @staticmethod def k(f): try: print( \\n-----| %s |----------------------- % f.__name__) if f.__doc__: print( # + re.sub(r'\\n[ \\t]*', \\n# ,f.__doc__)) f() print( # pass ) O.y += 1 except: O.n += 1 print(traceback.format_exc()) return f Test rig, in action Functions are called as a side-effect of load the file. The function comment is something the above rig prints out. If assertions fail, it prints the error but keeps on going to run the other tests. @O.k def testingFailure(): this one must fail.. just to test if the unit test system is working assert 1==2 @O.k def testingSuccess(): if this one fails, we have a problem! assert 1==1 if __name__== __main__ : O.report() For example, if you load this file with python3 thisfile.py you will see -----| testingFailure |----------------------- # this one must fail.. just to # test if the unit test system is working Traceback (most recent call last): File \"w1.py\", line 29, in k f() File \"w1.py\", line 52, in testingFailure assert 1==2 AssertionError -----| testingSuccess |----------------------- # if this one fails, we have a problem! # pass # pass= 1 fail= 1 %pass = 50% Note the last line (number of passes and failes in the code). Task1 (week1): Python101 Read Basic Python Write 27 functions like testingSuccess (above) that demonstrate you understand that the code on pages 5 to 33, skipping p21 (so one function for one thing on each page). Task2 (week2): Sample Table Data (that we want to read) Suppose we need to read in a table. DATA1 = outlook,$temp,?humidity,windy,play sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Then we need to learn the type of the data (on row1) which in this case is numeric (if name has \" $ \"); ignrore this column (if name has \" \"); string, otherwise. And some tables of data are more challenging that others. Here's one where there are comments (after a \" # \"); rows can continue onto the next line (if they end in \",\"); there can be blank lines in the file DATA2 = outlook, # weather forecast. $temp, # degrees farenheit ?humidity, # relative humidity windy, # wind is high play # yes,no sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64, 65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny, 75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes # unique day rainy,71,91,TRUE,no Regardless of those details, when we read both these strings, we see as output ['outlook', '$temp', 'windy', 'play'] ['sunny', 85.0, 'FALSE', 'no'] ['sunny', 80.0, 'TRUE', 'no'] ['overcast', 83.0, 'FALSE', 'yes'] ['rainy', 70.0, 'FALSE', 'yes'] ['rainy', 68.0, 'FALSE', 'yes'] ['rainy', 65.0, 'TRUE', 'no'] ['overcast', 64.0, 'TRUE', 'yes'] ['sunny', 72.0, 'FALSE', 'no'] ['sunny', 69.0, 'FALSE', 'yes'] ['rainy', 75.0, 'FALSE', 'yes'] ['sunny', 75.0, 'TRUE', 'yes'] ['overcast', 100.0, '90', 'TRUE'] ['overcast', 81.0, 'FALSE', 'yes'] ['rainy', 71.0, 'TRUE', 'no'] Functions The following functions implement the table reader. def lines(s): Return contents, one line at a time. yourCodeHere() def rows(src): Kill bad characters. If line ends in ',' then join to next. Skip blank lines. yourCodeHere() def cols(src): If a column name on row1 contains '?', then skip over that column. yourCodeHere() def prep(src): If a column name on row1 contains '$', coerce strings in that column to a float. yourCodeHere() Test cases def ok0(s): for row in prep(cols(rows(lines(s)))): print(row) @O.k def ok1(): ok0(DATA1) @O.k def ok2(): ok0(DATA2)","title":"One &#10004;"},{"location":"proj/w1/#homework-week1-week2","text":"","title":"Homework Week1, Week2"},{"location":"proj/w1/#todo","text":"Create a public Github repo (NOT in NC State Github, but in the other one) Add \"timm\" as a team member to that repo. How? go the repo's organization's settings on left-hand-side menu go to Collaberators and teams then Enter \"timm\" under \"Collaborators\". Start a file with the following header (containing class O ). For Week1, address the Python101 task. For Week2, address the Table reader task. Commit the code (w1.py, w2.py) and a transcript of the output (called w1.txt, w2.txt) to a sub-directory in your repo called w12 . Paste a link to that directory in the commit sheet.","title":"Todo"},{"location":"proj/w1/#a-simple-unit-test-rig-in-python","text":"import re,traceback class O: y=n=0 @staticmethod def report(): print( \\n# pass= %s fail= %s %%pass = %s%% % ( O.y,O.n, int(round(O.y*100/(O.y+O.n+0.001))))) @staticmethod def k(f): try: print( \\n-----| %s |----------------------- % f.__name__) if f.__doc__: print( # + re.sub(r'\\n[ \\t]*', \\n# ,f.__doc__)) f() print( # pass ) O.y += 1 except: O.n += 1 print(traceback.format_exc()) return f","title":"A Simple Unit Test Rig (in Python)"},{"location":"proj/w1/#test-rig-in-action","text":"Functions are called as a side-effect of load the file. The function comment is something the above rig prints out. If assertions fail, it prints the error but keeps on going to run the other tests. @O.k def testingFailure(): this one must fail.. just to test if the unit test system is working assert 1==2 @O.k def testingSuccess(): if this one fails, we have a problem! assert 1==1 if __name__== __main__ : O.report() For example, if you load this file with python3 thisfile.py you will see -----| testingFailure |----------------------- # this one must fail.. just to # test if the unit test system is working Traceback (most recent call last): File \"w1.py\", line 29, in k f() File \"w1.py\", line 52, in testingFailure assert 1==2 AssertionError -----| testingSuccess |----------------------- # if this one fails, we have a problem! # pass # pass= 1 fail= 1 %pass = 50% Note the last line (number of passes and failes in the code).","title":"Test rig, in action"},{"location":"proj/w1/#task1-week1-python101","text":"Read Basic Python Write 27 functions like testingSuccess (above) that demonstrate you understand that the code on pages 5 to 33, skipping p21 (so one function for one thing on each page).","title":"Task1 (week1): Python101"},{"location":"proj/w1/#task2-week2-sample-table-data-that-we-want-to-read","text":"Suppose we need to read in a table. DATA1 = outlook,$temp,?humidity,windy,play sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Then we need to learn the type of the data (on row1) which in this case is numeric (if name has \" $ \"); ignrore this column (if name has \" \"); string, otherwise. And some tables of data are more challenging that others. Here's one where there are comments (after a \" # \"); rows can continue onto the next line (if they end in \",\"); there can be blank lines in the file DATA2 = outlook, # weather forecast. $temp, # degrees farenheit ?humidity, # relative humidity windy, # wind is high play # yes,no sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64, 65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny, 75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes # unique day rainy,71,91,TRUE,no Regardless of those details, when we read both these strings, we see as output ['outlook', '$temp', 'windy', 'play'] ['sunny', 85.0, 'FALSE', 'no'] ['sunny', 80.0, 'TRUE', 'no'] ['overcast', 83.0, 'FALSE', 'yes'] ['rainy', 70.0, 'FALSE', 'yes'] ['rainy', 68.0, 'FALSE', 'yes'] ['rainy', 65.0, 'TRUE', 'no'] ['overcast', 64.0, 'TRUE', 'yes'] ['sunny', 72.0, 'FALSE', 'no'] ['sunny', 69.0, 'FALSE', 'yes'] ['rainy', 75.0, 'FALSE', 'yes'] ['sunny', 75.0, 'TRUE', 'yes'] ['overcast', 100.0, '90', 'TRUE'] ['overcast', 81.0, 'FALSE', 'yes'] ['rainy', 71.0, 'TRUE', 'no']","title":"Task2 (week2): Sample Table Data (that we want to read)"},{"location":"proj/w1/#functions","text":"The following functions implement the table reader. def lines(s): Return contents, one line at a time. yourCodeHere() def rows(src): Kill bad characters. If line ends in ',' then join to next. Skip blank lines. yourCodeHere() def cols(src): If a column name on row1 contains '?', then skip over that column. yourCodeHere() def prep(src): If a column name on row1 contains '$', coerce strings in that column to a float. yourCodeHere()","title":"Functions"},{"location":"proj/w1/#test-cases","text":"def ok0(s): for row in prep(cols(rows(lines(s)))): print(row) @O.k def ok1(): ok0(DATA1) @O.k def ok2(): ok0(DATA2)","title":"Test cases"},{"location":"proj/w3/","text":"Homework Week3 Todo Ensure Rahul is added as a collaborator to your Github repo In a new directory w3 , using the test engine from week1, address the following problems. Write code and unit tests. Collect out from your system that shows things load and run. Submit a pointer to that w3 directory. Note that the following offers a full implementation of everything you need to do... in another language (Lua). Your job is to use the Lua code as inspiration to build your Python. Fear not the Lua. Its a pretty simple language. Very Python-like. But it has no classes so it has certain kuldges. We'll talk to it in class. Three Incremental Sampling Methods An incremental sampling methods runs over the data once, updating its internal measures as it goes. A reservoir sampler keeps a fixed number of items (and if it sees more, it deletes existing ones at random). A Gaussian sampler incrementally updated means and standard deviations. A entropy sampler incrementally updates the ratios at which various symbols are used. The first two are for numbers and the last is for symbols. FYI: my implementation of the second uses the first as a sub-routine. The directory https://github.com/timm/lean/blob/master/tests/ contains unit tests for in sample.lua , num.lua , sym.lua . which are (respectively) a reservoir sampler ; a Gaussian sampler ; and a entropy sampler . Port the unit tests to my test engine method O.k . Use those tests to check your Python versions of sample.lua , num.lua , sym.lua in https://github.com/timm/lean/blob/master/src/ . the directory","title":"Three &#10004;"},{"location":"proj/w3/#homework-week3","text":"","title":"Homework Week3"},{"location":"proj/w3/#todo","text":"Ensure Rahul is added as a collaborator to your Github repo In a new directory w3 , using the test engine from week1, address the following problems. Write code and unit tests. Collect out from your system that shows things load and run. Submit a pointer to that w3 directory. Note that the following offers a full implementation of everything you need to do... in another language (Lua). Your job is to use the Lua code as inspiration to build your Python. Fear not the Lua. Its a pretty simple language. Very Python-like. But it has no classes so it has certain kuldges. We'll talk to it in class.","title":"Todo"},{"location":"proj/w3/#three-incremental-sampling-methods","text":"An incremental sampling methods runs over the data once, updating its internal measures as it goes. A reservoir sampler keeps a fixed number of items (and if it sees more, it deletes existing ones at random). A Gaussian sampler incrementally updated means and standard deviations. A entropy sampler incrementally updates the ratios at which various symbols are used. The first two are for numbers and the last is for symbols. FYI: my implementation of the second uses the first as a sub-routine. The directory https://github.com/timm/lean/blob/master/tests/ contains unit tests for in sample.lua , num.lua , sym.lua . which are (respectively) a reservoir sampler ; a Gaussian sampler ; and a entropy sampler . Port the unit tests to my test engine method O.k . Use those tests to check your Python versions of sample.lua , num.lua , sym.lua in https://github.com/timm/lean/blob/master/src/ . the directory","title":"Three Incremental Sampling Methods"},{"location":"proj/w4/","text":"Homework Week4 Todo The usual instructions. Directory = w4 . Write a table class that reads in data from disk, and maintains one num or sym instance for each column.as you read in data from disk, update each header, incrementally. When the data quits, spit out the statistic on each column. You results should look something like the following: weather.csv , n mode frequency 1 outlook 14 sunny 5 3 wind 14 FALSE 8 4 !play 14 yes 9 n mu sd 2 $temp 14 73.57 6.57 weatherLong.csv , n mode frequency 1 %outlook 28 sunny 10 4 wind 28 FALSE 16 5 !play 28 yes 18 n mu sd 2 $temp 28 73.57 6.45 3 humid 28 81.64 10.09 auto.csv , n mode frequency 1 %cylinders 398 4 204 7 origin 398 1 249 n mu sd 2 $displacement 398 193.43 104.27 3 $horsepower 392 104.47 38.49 4 weight 398 2970.42 846.84 5 acceltn 398 15.57 2.76 6 $model 398 76.01 3.70 8 mpg 398 23.84 8.34 Before the last data set: some of its numbers are \"?\" which means \"do not know\". You'll have to skip over that code. Sample code (in Lua) This files reads in data. The first row is translated into a column header and the others into rows of data rows.lua .","title":"Four &#10004;"},{"location":"proj/w4/#homework-week4","text":"","title":"Homework Week4"},{"location":"proj/w4/#todo","text":"The usual instructions. Directory = w4 . Write a table class that reads in data from disk, and maintains one num or sym instance for each column.as you read in data from disk, update each header, incrementally. When the data quits, spit out the statistic on each column. You results should look something like the following: weather.csv , n mode frequency 1 outlook 14 sunny 5 3 wind 14 FALSE 8 4 !play 14 yes 9 n mu sd 2 $temp 14 73.57 6.57 weatherLong.csv , n mode frequency 1 %outlook 28 sunny 10 4 wind 28 FALSE 16 5 !play 28 yes 18 n mu sd 2 $temp 28 73.57 6.45 3 humid 28 81.64 10.09 auto.csv , n mode frequency 1 %cylinders 398 4 204 7 origin 398 1 249 n mu sd 2 $displacement 398 193.43 104.27 3 $horsepower 392 104.47 38.49 4 weight 398 2970.42 846.84 5 acceltn 398 15.57 2.76 6 $model 398 76.01 3.70 8 mpg 398 23.84 8.34 Before the last data set: some of its numbers are \"?\" which means \"do not know\". You'll have to skip over that code.","title":"Todo"},{"location":"proj/w4/#sample-code-in-lua","text":"This files reads in data. The first row is translated into a column header and the others into rows of data rows.lua .","title":"Sample code (in Lua)"},{"location":"proj/w5/","text":"Homework5 What to hand in The usual drill. A new sub-directory. Python source coe. and Example out .txt file. Part 1: Domination (Easy) Port the code dom.lua to Python. Using that code, add the dom score to each row of a data file. Test that code on 2 files: weatherLong and auto . Test1 Input, from weatherLong : %outlook $temp humid wind !play over 64 65 TRUE yes over 64 65 TRUE yes over 72 90 TRUE yes over 72 90 TRUE yes over 81 75 FALSE yes over 81 75 FALSE yes over 83 86 FALSE yes over 83 86 FALSE yes sunny 69 70 FALSE yes sunny 69 70 FALSE yes rainy 65 70 TRUE no rainy 65 70 TRUE no sunny 75 70 TRUE yes sunny 75 70 TRUE yes rainy 75 80 FALSE yes rainy 75 80 FALSE yes rainy 68 80 FALSE yes rainy 68 80 FALSE yes sunny 85 85 FALSE no sunny 85 85 FALSE no sunny 80 90 TRUE no sunny 80 90 TRUE no rainy 71 91 TRUE no rainy 71 91 TRUE no sunny 72 95 FALSE no sunny 72 95 FALSE no rainy 70 96 FALSE yes rainy 70 96 FALSE yes Output. Note that max dom is seen for lowest humidity: %outlook $temp humid wind !play dom over 64 65 TRUE yes 0.93 over 64 65 TRUE yes 0.91 over 72 90 TRUE yes 0.23 over 72 90 TRUE yes 0.17 over 81 75 FALSE yes 0.67 over 81 75 FALSE yes 0.69 over 83 86 FALSE yes 0.3 over 83 86 FALSE yes 0.36 sunny 69 70 FALSE yes 0.65 sunny 69 70 FALSE yes 0.72 rainy 65 70 TRUE no 0.75 rainy 65 70 TRUE no 0.73 sunny 75 70 TRUE yes 0.74 sunny 75 70 TRUE yes 0.83 rainy 75 80 FALSE yes 0.53 rainy 75 80 FALSE yes 0.56 rainy 68 80 FALSE yes 0.41 rainy 68 80 FALSE yes 0.48 sunny 85 85 FALSE no 0.39 sunny 85 85 FALSE no 0.43 sunny 80 90 TRUE no 0.23 sunny 80 90 TRUE no 0.1 rainy 71 91 TRUE no 0.12 rainy 71 91 TRUE no 0.1 sunny 72 95 FALSE no 0.04 sunny 72 95 FALSE no 0.07 rainy 70 96 FALSE yes 0 rainy 70 96 FALSE yes 0 Test2 Input: auto Output: Sort by the last column (the dom score) and show first and last 10 lines. The output should look something like the line Here's the same data, with dom score added. Shown here are the 5 best and worst rows. in the domination lecture lecture. If you do Test2 correctly, then highest dom scores should be assocaited wiht rows with least weight, most acceleration and most mpg (and the lowest dom scores are associated with the reverse). Part2: Unsupervised discretization (Tricky) Port the code code unsuper.lua to Python and test it on weatherLong . This code find all numeric independent columns then splits them to minmize the execpted value of the standard deviation of those columns, after the splits. For example, input: %outlook, $temp, humid, wind, !play over, 64, 65, TRUE, yes over, 64, 65, TRUE, yes over, 72, 90, TRUE, yes over, 72, 90, TRUE, yes over, 81, 75, FALSE, yes over, 81, 75, FALSE, yes over, 83, 86, FALSE, yes over, 83, 86, FALSE, yes sunny, 69, 70, FALSE, yes sunny, 69, 70, FALSE, yes rainy, 65, 70, TRUE, no rainy, 65, 70, TRUE, no sunny, 75, 70, TRUE, yes sunny, 75, 70, TRUE, yes rainy, 75, 80, FALSE, yes rainy, 75, 80, FALSE, yes rainy, 68, 80, FALSE, yes rainy, 68, 80, FALSE, yes sunny, 85, 85, FALSE, no sunny, 85, 85, FALSE, no sunny, 80, 90, TRUE, no sunny, 80, 90, TRUE, no rainy, 71, 91, TRUE, no rainy, 71, 91, TRUE, no sunny, 72, 95, FALSE, no sunny, 72, 95, FALSE, no rainy, 70, 96, FALSE, yes rainy, 70, 96, FALSE, yes Output (where x..y means \"x to y\" and ..x means up to x\" and x.. means \"x and above\") %outlook temp humid wind !play over ..69 65 TRUE yes over ..69 65 TRUE yes rainy ..69 70 TRUE no rainy ..69 70 TRUE no rainy ..69 80 FALSE yes rainy ..69 80 FALSE yes sunny ..69 70 FALSE yes sunny ..69 70 FALSE yes rainy 70..72 96 FALSE yes rainy 70..72 96 FALSE yes rainy 70..72 91 TRUE no rainy 70..72 91 TRUE no over 70..72 90 TRUE yes sunny 70..72 95 FALSE no sunny 72..75 95 FALSE no over 72..75 90 TRUE yes rainy 72..75 80 FALSE yes rainy 72..75 80 FALSE yes sunny 72..75 70 TRUE yes sunny 72..75 70 TRUE yes sunny 80.. 90 TRUE no sunny 80.. 90 TRUE no over 80.. 75 FALSE yes over 80.. 75 FALSE yes over 80.. 86 FALSE yes over 80.. 86 FALSE yes sunny 80.. 85 FALSE no sunny 80.. 85 FALSE no Note: your results may differ somewhat from mine due to your different engineering decisions. That's cool.","title":"Five &#10004;"},{"location":"proj/w5/#homework5","text":"","title":"Homework5"},{"location":"proj/w5/#what-to-hand-in","text":"The usual drill. A new sub-directory. Python source coe. and Example out .txt file.","title":"What to hand in"},{"location":"proj/w5/#part-1-domination-easy","text":"Port the code dom.lua to Python. Using that code, add the dom score to each row of a data file. Test that code on 2 files: weatherLong and auto .","title":"Part 1: Domination (Easy)"},{"location":"proj/w5/#test1","text":"Input, from weatherLong : %outlook $temp humid wind !play over 64 65 TRUE yes over 64 65 TRUE yes over 72 90 TRUE yes over 72 90 TRUE yes over 81 75 FALSE yes over 81 75 FALSE yes over 83 86 FALSE yes over 83 86 FALSE yes sunny 69 70 FALSE yes sunny 69 70 FALSE yes rainy 65 70 TRUE no rainy 65 70 TRUE no sunny 75 70 TRUE yes sunny 75 70 TRUE yes rainy 75 80 FALSE yes rainy 75 80 FALSE yes rainy 68 80 FALSE yes rainy 68 80 FALSE yes sunny 85 85 FALSE no sunny 85 85 FALSE no sunny 80 90 TRUE no sunny 80 90 TRUE no rainy 71 91 TRUE no rainy 71 91 TRUE no sunny 72 95 FALSE no sunny 72 95 FALSE no rainy 70 96 FALSE yes rainy 70 96 FALSE yes Output. Note that max dom is seen for lowest humidity: %outlook $temp humid wind !play dom over 64 65 TRUE yes 0.93 over 64 65 TRUE yes 0.91 over 72 90 TRUE yes 0.23 over 72 90 TRUE yes 0.17 over 81 75 FALSE yes 0.67 over 81 75 FALSE yes 0.69 over 83 86 FALSE yes 0.3 over 83 86 FALSE yes 0.36 sunny 69 70 FALSE yes 0.65 sunny 69 70 FALSE yes 0.72 rainy 65 70 TRUE no 0.75 rainy 65 70 TRUE no 0.73 sunny 75 70 TRUE yes 0.74 sunny 75 70 TRUE yes 0.83 rainy 75 80 FALSE yes 0.53 rainy 75 80 FALSE yes 0.56 rainy 68 80 FALSE yes 0.41 rainy 68 80 FALSE yes 0.48 sunny 85 85 FALSE no 0.39 sunny 85 85 FALSE no 0.43 sunny 80 90 TRUE no 0.23 sunny 80 90 TRUE no 0.1 rainy 71 91 TRUE no 0.12 rainy 71 91 TRUE no 0.1 sunny 72 95 FALSE no 0.04 sunny 72 95 FALSE no 0.07 rainy 70 96 FALSE yes 0 rainy 70 96 FALSE yes 0","title":"Test1"},{"location":"proj/w5/#test2","text":"Input: auto Output: Sort by the last column (the dom score) and show first and last 10 lines. The output should look something like the line Here's the same data, with dom score added. Shown here are the 5 best and worst rows. in the domination lecture lecture. If you do Test2 correctly, then highest dom scores should be assocaited wiht rows with least weight, most acceleration and most mpg (and the lowest dom scores are associated with the reverse).","title":"Test2"},{"location":"proj/w5/#part2-unsupervised-discretization-tricky","text":"Port the code code unsuper.lua to Python and test it on weatherLong . This code find all numeric independent columns then splits them to minmize the execpted value of the standard deviation of those columns, after the splits. For example, input: %outlook, $temp, humid, wind, !play over, 64, 65, TRUE, yes over, 64, 65, TRUE, yes over, 72, 90, TRUE, yes over, 72, 90, TRUE, yes over, 81, 75, FALSE, yes over, 81, 75, FALSE, yes over, 83, 86, FALSE, yes over, 83, 86, FALSE, yes sunny, 69, 70, FALSE, yes sunny, 69, 70, FALSE, yes rainy, 65, 70, TRUE, no rainy, 65, 70, TRUE, no sunny, 75, 70, TRUE, yes sunny, 75, 70, TRUE, yes rainy, 75, 80, FALSE, yes rainy, 75, 80, FALSE, yes rainy, 68, 80, FALSE, yes rainy, 68, 80, FALSE, yes sunny, 85, 85, FALSE, no sunny, 85, 85, FALSE, no sunny, 80, 90, TRUE, no sunny, 80, 90, TRUE, no rainy, 71, 91, TRUE, no rainy, 71, 91, TRUE, no sunny, 72, 95, FALSE, no sunny, 72, 95, FALSE, no rainy, 70, 96, FALSE, yes rainy, 70, 96, FALSE, yes Output (where x..y means \"x to y\" and ..x means up to x\" and x.. means \"x and above\") %outlook temp humid wind !play over ..69 65 TRUE yes over ..69 65 TRUE yes rainy ..69 70 TRUE no rainy ..69 70 TRUE no rainy ..69 80 FALSE yes rainy ..69 80 FALSE yes sunny ..69 70 FALSE yes sunny ..69 70 FALSE yes rainy 70..72 96 FALSE yes rainy 70..72 96 FALSE yes rainy 70..72 91 TRUE no rainy 70..72 91 TRUE no over 70..72 90 TRUE yes sunny 70..72 95 FALSE no sunny 72..75 95 FALSE no over 72..75 90 TRUE yes rainy 72..75 80 FALSE yes rainy 72..75 80 FALSE yes sunny 72..75 70 TRUE yes sunny 72..75 70 TRUE yes sunny 80.. 90 TRUE no sunny 80.. 90 TRUE no over 80.. 75 FALSE yes over 80.. 75 FALSE yes over 80.. 86 FALSE yes over 80.. 86 FALSE yes sunny 80.. 85 FALSE no sunny 80.. 85 FALSE no Note: your results may differ somewhat from mine due to your different engineering decisions. That's cool.","title":"Part2: Unsupervised discretization (Tricky)"},{"location":"proj/w6/","text":"Homework6 What to hand in The usual drill. A new sub-directory. Python source coe. and Example out .txt file. Task1: Supervised Domination Add the dom score to a data set, then supervise discretize the independent numeric attribites Test that code on 2 files: weatherLong and auto . Code See super.lua See config.lua Test1 Output, from weatherLong , affter it is get a dom score (don't worry if you don't get exactly this): -- $temp ---------- |.. 64..85 |.. |.. 64..69 = 69 |.. |.. 70..85 |.. |.. |.. 70..72 = 9 |.. |.. |.. 75..85 = 48 %outlook temp humid wind !play dom over ..69 65 TRUE yes 0.93 over ..69 65 TRUE yes 0.91 rainy ..69 70 TRUE no 0.75 rainy ..69 70 TRUE no 0.73 rainy ..69 80 FALSE yes 0.41 rainy ..69 80 FALSE yes 0.48 sunny ..69 70 FALSE yes 0.65 sunny ..69 70 FALSE yes 0.72 rainy 70..72 96 FALSE yes 0 rainy 70..72 96 FALSE yes 0 rainy 70..72 91 TRUE no 0.1 rainy 70..72 91 TRUE no 0.12 over 70..72 90 TRUE yes 0.23 sunny 70..72 95 FALSE no 0.04 sunny 70..72 95 FALSE no 0.07 over 70..72 90 TRUE yes 0.17 rainy 75.. 80 FALSE yes 0.53 rainy 75.. 80 FALSE yes 0.56 sunny 75.. 70 TRUE yes 0.74 sunny 75.. 70 TRUE yes 0.83 sunny 75.. 90 TRUE no 0.23 sunny 75.. 90 TRUE no 0.1 over 75.. 75 FALSE yes 0.67 over 75.. 75 FALSE yes 0.69 over 75.. 86 FALSE yes 0.3 over 75.. 86 FALSE yes 0.36 sunny 75.. 85 FALSE no 0.43 sunny 75.. 85 FALSE no 0.39 Test2 Output, from auto , affter it is get a dom score (don't worry if you don't get exactly this). What I am showing here is just the ranges found by the recursive descent: - $displacement ---------- |.. 68..455 |.. |.. 68..156 a |.. |.. |.. 68..105 = 83 b |.. |.. |.. 105..156 = 60 |.. |.. 156..455 c |.. |.. |.. 156..262 = 38 |.. |.. |.. 262..455 d |.. |.. |.. |.. 262..351 = 16 e |.. |.. |.. |.. 351..455 = 7 -- $horsepower ---------- |.. 46..230 |.. |.. 46..96 |.. |.. |.. 46..70 a |.. |.. |.. |.. 46..60 = 95 b |.. |.. |.. |.. 61..70 = 88 |.. |.. |.. 70..96 c |.. |.. |.. |.. 70..82 = 71 d |.. |.. |.. |.. 83..96 = 58 |.. |.. 96..230 e |.. |.. |.. 96..125 = 37 |.. |.. |.. 125..230 f |.. |.. |.. |.. 125..145 = 20 |.. |.. |.. |.. 145..230 g |.. |.. |.. |.. |.. 145..170 = 12 h |.. |.. |.. |.. |.. 170..230 = 7 -- $model ---------- |.. 70..82 a |.. |.. 70..76 = 39 |.. |.. 76..82 b |.. |.. |.. 76..79 = 50 c |.. |.. |.. 79..82 = 74 Task2: Find the Splitter The splitter is the attribute which: if we split on that attribute, the expected value of the standard deviation after the split is less. As an example of expected value, in the above for $model , there are splits a,b,c of sizes 39,50,74. Suppose the variance of the dom score in those splits is Va, Vb, Vc 30/n * Va + 50/n * Vb + 74/n * Vc is the expected value of $model after the split is (where n = 39+50+74 ).","title":"Six &#10004;"},{"location":"proj/w6/#homework6","text":"","title":"Homework6"},{"location":"proj/w6/#what-to-hand-in","text":"The usual drill. A new sub-directory. Python source coe. and Example out .txt file.","title":"What to hand in"},{"location":"proj/w6/#task1-supervised-domination","text":"Add the dom score to a data set, then supervise discretize the independent numeric attribites Test that code on 2 files: weatherLong and auto .","title":"Task1:  Supervised Domination"},{"location":"proj/w6/#code","text":"See super.lua See config.lua","title":"Code"},{"location":"proj/w6/#test1","text":"Output, from weatherLong , affter it is get a dom score (don't worry if you don't get exactly this): -- $temp ---------- |.. 64..85 |.. |.. 64..69 = 69 |.. |.. 70..85 |.. |.. |.. 70..72 = 9 |.. |.. |.. 75..85 = 48 %outlook temp humid wind !play dom over ..69 65 TRUE yes 0.93 over ..69 65 TRUE yes 0.91 rainy ..69 70 TRUE no 0.75 rainy ..69 70 TRUE no 0.73 rainy ..69 80 FALSE yes 0.41 rainy ..69 80 FALSE yes 0.48 sunny ..69 70 FALSE yes 0.65 sunny ..69 70 FALSE yes 0.72 rainy 70..72 96 FALSE yes 0 rainy 70..72 96 FALSE yes 0 rainy 70..72 91 TRUE no 0.1 rainy 70..72 91 TRUE no 0.12 over 70..72 90 TRUE yes 0.23 sunny 70..72 95 FALSE no 0.04 sunny 70..72 95 FALSE no 0.07 over 70..72 90 TRUE yes 0.17 rainy 75.. 80 FALSE yes 0.53 rainy 75.. 80 FALSE yes 0.56 sunny 75.. 70 TRUE yes 0.74 sunny 75.. 70 TRUE yes 0.83 sunny 75.. 90 TRUE no 0.23 sunny 75.. 90 TRUE no 0.1 over 75.. 75 FALSE yes 0.67 over 75.. 75 FALSE yes 0.69 over 75.. 86 FALSE yes 0.3 over 75.. 86 FALSE yes 0.36 sunny 75.. 85 FALSE no 0.43 sunny 75.. 85 FALSE no 0.39","title":"Test1"},{"location":"proj/w6/#test2","text":"Output, from auto , affter it is get a dom score (don't worry if you don't get exactly this). What I am showing here is just the ranges found by the recursive descent: - $displacement ---------- |.. 68..455 |.. |.. 68..156 a |.. |.. |.. 68..105 = 83 b |.. |.. |.. 105..156 = 60 |.. |.. 156..455 c |.. |.. |.. 156..262 = 38 |.. |.. |.. 262..455 d |.. |.. |.. |.. 262..351 = 16 e |.. |.. |.. |.. 351..455 = 7 -- $horsepower ---------- |.. 46..230 |.. |.. 46..96 |.. |.. |.. 46..70 a |.. |.. |.. |.. 46..60 = 95 b |.. |.. |.. |.. 61..70 = 88 |.. |.. |.. 70..96 c |.. |.. |.. |.. 70..82 = 71 d |.. |.. |.. |.. 83..96 = 58 |.. |.. 96..230 e |.. |.. |.. 96..125 = 37 |.. |.. |.. 125..230 f |.. |.. |.. |.. 125..145 = 20 |.. |.. |.. |.. 145..230 g |.. |.. |.. |.. |.. 145..170 = 12 h |.. |.. |.. |.. |.. 170..230 = 7 -- $model ---------- |.. 70..82 a |.. |.. 70..76 = 39 |.. |.. 76..82 b |.. |.. |.. 76..79 = 50 c |.. |.. |.. 79..82 = 74","title":"Test2"},{"location":"proj/w6/#task2-find-the-splitter","text":"The splitter is the attribute which: if we split on that attribute, the expected value of the standard deviation after the split is less. As an example of expected value, in the above for $model , there are splits a,b,c of sizes 39,50,74. Suppose the variance of the dom score in those splits is Va, Vb, Vc 30/n * Va + 50/n * Vb + 74/n * Vc is the expected value of $model after the split is (where n = 39+50+74 ).","title":"Task2: Find the Splitter"},{"location":"review/one/","text":"Review1 What is NFL? What are its implications for selecting the \"best\" learner List 5 baseline criteria for an AI tools, make argument: Why it is important to meet that criteria? Why, pragmatically, it might be necessary to ignore (or, at least relax) that criteria Distinguish supervised (S) from unsupervised (U) learning When would do S or U? Is FFT an S or U? Why? (Hint: your answer should say something about FFT). Is the Fastmap Cluster an S or U? Why? (Hint: your answer should say something about the Fastmap clusterer). Instance models refer to specific values (e.g. X=6,Y=10,Z=100) while other models refer to ranges of some attributes (e.g. X 6 and Y 5 ). Which of these models reference points or volumes ? Which of these offer more generalizations of the past? Which of these are shorter to share? How to generalize from point models to models that cover volumes? FFTs: What are the attributes in an node of an FFT tree? What is the structure of an FFT tree (hint: use the node attributes to make that description). Fastmap Clustering Given 2 points Y,Z at distance c , if a new point X is a=dist(X,Y) and b=dist(X,Z) , derive an expression for the distance x that X falls along the line between Y,Z . What are the attributes in an node of an Fastmap cluster? (hint: parts of it are recursive) What is the structure of an Fastmap clustering tree (hint: use the node attributes to make that description). How to use a Fastmap cluster node for classification (hint: there are many ways) anomaly detection, sharing, privacy ( hint: see III.D.4 incremental model updates over an infinite stream of data? ( hint How to use any clustering method for optimization, anomaly detection, classification, sharing, privacy. How to use a Fastmap cluster free for: - very fast optimization ( hint: see Algorithm1 ) What are diversity measures for numeric and symbolic values? Offer a formula for each. What is the diversity of \"y,n,y,y,y,n,y,y,n\"? If you don't have a calculator, show all working and stop before the final calculation. What is the diversity of \"10,89,32,11,9,90,30,31,91\"? If you don't have a calculator, show all working and stop before the final calculation. Consider the following data. Just considering the age column, where to divide it such that age diversity is minimized? Assume a minimum bin size of 3. Now consider the age,alive relationship. Where to divide age in order to reduce the diversity of alive ? Assume a minimum bin size of 3. Data: age,alive 10, y 89, n 32, y 11, y 9, y 90, n 30, y 31, y 91, n Iterative dichmotization (ID) algorithms divide attributes into ranges, then recurse on each range. How do ID algorithms decide what attribute with which to split the data","title":"Review1"},{"location":"review/one/#review1","text":"What is NFL? What are its implications for selecting the \"best\" learner List 5 baseline criteria for an AI tools, make argument: Why it is important to meet that criteria? Why, pragmatically, it might be necessary to ignore (or, at least relax) that criteria Distinguish supervised (S) from unsupervised (U) learning When would do S or U? Is FFT an S or U? Why? (Hint: your answer should say something about FFT). Is the Fastmap Cluster an S or U? Why? (Hint: your answer should say something about the Fastmap clusterer). Instance models refer to specific values (e.g. X=6,Y=10,Z=100) while other models refer to ranges of some attributes (e.g. X 6 and Y 5 ). Which of these models reference points or volumes ? Which of these offer more generalizations of the past? Which of these are shorter to share? How to generalize from point models to models that cover volumes? FFTs: What are the attributes in an node of an FFT tree? What is the structure of an FFT tree (hint: use the node attributes to make that description). Fastmap Clustering Given 2 points Y,Z at distance c , if a new point X is a=dist(X,Y) and b=dist(X,Z) , derive an expression for the distance x that X falls along the line between Y,Z . What are the attributes in an node of an Fastmap cluster? (hint: parts of it are recursive) What is the structure of an Fastmap clustering tree (hint: use the node attributes to make that description). How to use a Fastmap cluster node for classification (hint: there are many ways) anomaly detection, sharing, privacy ( hint: see III.D.4 incremental model updates over an infinite stream of data? ( hint How to use any clustering method for optimization, anomaly detection, classification, sharing, privacy. How to use a Fastmap cluster free for: - very fast optimization ( hint: see Algorithm1 ) What are diversity measures for numeric and symbolic values? Offer a formula for each. What is the diversity of \"y,n,y,y,y,n,y,y,n\"? If you don't have a calculator, show all working and stop before the final calculation. What is the diversity of \"10,89,32,11,9,90,30,31,91\"? If you don't have a calculator, show all working and stop before the final calculation. Consider the following data. Just considering the age column, where to divide it such that age diversity is minimized? Assume a minimum bin size of 3. Now consider the age,alive relationship. Where to divide age in order to reduce the diversity of alive ? Assume a minimum bin size of 3. Data: age,alive 10, y 89, n 32, y 11, y 9, y 90, n 30, y 31, y 91, n Iterative dichmotization (ID) algorithms divide attributes into ranges, then recurse on each range. How do ID algorithms decide what attribute with which to split the data","title":"Review1"},{"location":"review/three/","text":"","title":"Three"},{"location":"review/two/","text":"Review 2 General Questions What constitutes a good baseline? (Hint: See lecture notes on baselines) Enumerate the categories and provide a one line description. What is a pareto frontier? True/False -- There exists a pareto frontier for 1-objective problem True/False -- An n-objective optimizer has an n-dimensional pareto frontier How does Naive Bayes work? What are the merits and demerits What family of data mining methods would you use for these cases? No. classes | Symbolic | Numerics | ------------|------------|----------|-- 0 | | | ------------|------------|----------|-- 1 | | | ------------|------------|----------|-- = 1 | | | ------------|------------|----------|-- Discuss some strategies to obtain a class value while performing kNN for A symbolic class (True or False) A numeric class (where values falls between 0 to 1) No class variable @relation weather @attribute outlook {sunny, overcast, rainy} @attribute temperature real @attribute humidity real @attribute windy {TRUE, FALSE} @attribute play {yes, no} @data sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,72,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Using the above example, answer the following questions: On variable types: What is symbolic variables? Please give 2 examples. What is numeric variables? Please give 2 examples. Name one operation that can be performed on both numeric and symbolic variables. Name one operation that can be performed on numeric but not symbolic variables. How do you calculate the distance between symbolic variables? Please give 1 example. How do you calculate the distance between numeric variables? Please give 1 example. What is normalization? Why might it be useful? In the above data, normalize temperature=65, humidity=70 On Independent variables: What is independent variable? Given an example What's the difference between decision tree and random forest? On Impurity: What is the connection between variance and standard deviation? What is the same about variance and entropy? What is difference between variane and entropy? How do we calculate entropy? What is the entropy of play? Show your working. How do we calculate standard deviation? What is the standard deviation of temperature ? Show your working. Note: The Gini Index is used to compute the impurity of a data partition. It is computed as 1 - (\u2211 (p.i)^2). For example, assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be: Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2) Is Gini defined for symbolic or number variables? On Dependent variables: Define a dependent variable. Given an example. What the difference between supervised and unsupervised learning? Please give 2 example tasks for each of them. In classification taskes, is the dependent variable numeric or symbolic? What about regressions? If there are more than one numeric class variable, what kind of problem do we have? On Decision Tree learning: Decision tree is often called \"iterative dichomization\". Why? In the following decision tree, outlook is the top split and temperature never appears in the tree. Why could that be? For the golf dataset above, do the following, show all your calculations and explainations: calculate the entropy of the class variables. calculate the expected value of the entropy of the class symbol after splitting on outlook . calculate the expected value of the entropy of the class symbol after splitting on temperature . compare the three values. What can you say? outlook = sunny | humidity = high: no | humidity = normal: yes outlook = overcast: yes outlook = rainy | windy = TRUE: no | windy = FALSE: yes Python Stuff What is the airspeed velocity of an unladen swallow? What does yield do? How is it different from a return ? What is the purpose of a context manager? Given the following code, explain what happens with respect to context manager- with open(f, 'w') as fname: do some thing do some more things Match the following regex terms to their tasks fill-up-this Explain the following regex operation. Specfically, what happens to the string variable a_text_string in the following snippet- re.sub(r'([ \\n\\r\\t]|#.* )', , a_text_string) Give an example of a ternary operator in python. What does the enumerate operation do in python? Explain what's going on here- [a[i] for i, user in enumerate(users) if user==True]","title":"Review 2"},{"location":"review/two/#review-2","text":"","title":"Review 2"},{"location":"review/two/#general-questions","text":"What constitutes a good baseline? (Hint: See lecture notes on baselines) Enumerate the categories and provide a one line description. What is a pareto frontier? True/False -- There exists a pareto frontier for 1-objective problem True/False -- An n-objective optimizer has an n-dimensional pareto frontier How does Naive Bayes work? What are the merits and demerits What family of data mining methods would you use for these cases? No. classes | Symbolic | Numerics | ------------|------------|----------|-- 0 | | | ------------|------------|----------|-- 1 | | | ------------|------------|----------|-- = 1 | | | ------------|------------|----------|-- Discuss some strategies to obtain a class value while performing kNN for A symbolic class (True or False) A numeric class (where values falls between 0 to 1) No class variable @relation weather @attribute outlook {sunny, overcast, rainy} @attribute temperature real @attribute humidity real @attribute windy {TRUE, FALSE} @attribute play {yes, no} @data sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,72,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Using the above example, answer the following questions: On variable types: What is symbolic variables? Please give 2 examples. What is numeric variables? Please give 2 examples. Name one operation that can be performed on both numeric and symbolic variables. Name one operation that can be performed on numeric but not symbolic variables. How do you calculate the distance between symbolic variables? Please give 1 example. How do you calculate the distance between numeric variables? Please give 1 example. What is normalization? Why might it be useful? In the above data, normalize temperature=65, humidity=70 On Independent variables: What is independent variable? Given an example What's the difference between decision tree and random forest? On Impurity: What is the connection between variance and standard deviation? What is the same about variance and entropy? What is difference between variane and entropy? How do we calculate entropy? What is the entropy of play? Show your working. How do we calculate standard deviation? What is the standard deviation of temperature ? Show your working. Note: The Gini Index is used to compute the impurity of a data partition. It is computed as 1 - (\u2211 (p.i)^2). For example, assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be: Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2) Is Gini defined for symbolic or number variables? On Dependent variables: Define a dependent variable. Given an example. What the difference between supervised and unsupervised learning? Please give 2 example tasks for each of them. In classification taskes, is the dependent variable numeric or symbolic? What about regressions? If there are more than one numeric class variable, what kind of problem do we have? On Decision Tree learning: Decision tree is often called \"iterative dichomization\". Why? In the following decision tree, outlook is the top split and temperature never appears in the tree. Why could that be? For the golf dataset above, do the following, show all your calculations and explainations: calculate the entropy of the class variables. calculate the expected value of the entropy of the class symbol after splitting on outlook . calculate the expected value of the entropy of the class symbol after splitting on temperature . compare the three values. What can you say? outlook = sunny | humidity = high: no | humidity = normal: yes outlook = overcast: yes outlook = rainy | windy = TRUE: no | windy = FALSE: yes","title":"General Questions"},{"location":"review/two/#python-stuff","text":"What is the airspeed velocity of an unladen swallow? What does yield do? How is it different from a return ? What is the purpose of a context manager? Given the following code, explain what happens with respect to context manager- with open(f, 'w') as fname: do some thing do some more things Match the following regex terms to their tasks fill-up-this Explain the following regex operation. Specfically, what happens to the string variable a_text_string in the following snippet- re.sub(r'([ \\n\\r\\t]|#.* )', , a_text_string) Give an example of a ternary operator in python. What does the enumerate operation do in python? Explain what's going on here- [a[i] for i, user in enumerate(users) if user==True]","title":"Python Stuff"}]}